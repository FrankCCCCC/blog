[{"content":"","permalink":"https://frankccccc.github.io/blog/posts/nngp_ntk/","summary":"","title":"Toward NNGP and NTK"},{"content":"The main different between 3 kinds of estimation is What do we assume for the prior? The Maximum Likelihood Estimation(MLE) doesn\u0026rsquo;t use any prior but only maiximize the probability according to the samples. On the other hand, MAP and Bayesian both use priors to estimate the probability. The Maximum A Posteriori(MAP) only use the probability of single event while Bayesian Estimation see a distribution as the prior.\nTo be continue\u0026hellip;\nReference  最大似然估計vs最大後驗概率  ","permalink":"https://frankccccc.github.io/blog/posts/mle_map_bayes/","summary":"The main different between 3 kinds of estimation is What do we assume for the prior? The Maximum Likelihood Estimation(MLE) doesn\u0026rsquo;t use any prior but only maiximize the probability according to the samples. On the other hand, MAP and Bayesian both use priors to estimate the probability. The Maximum A Posteriori(MAP) only use the probability of single event while Bayesian Estimation see a distribution as the prior.\nTo be continue\u0026hellip;","title":"An Insight Into MLE, MAP, and Bayesian Estimation"},{"content":"","permalink":"https://frankccccc.github.io/blog/posts/alphago/","summary":"","title":"Part I - From Alphago to Muzero"},{"content":"","permalink":"https://frankccccc.github.io/blog/posts/distributional_rl/","summary":"","title":"A Glimpse of Distributional RL"},{"content":"Multi-Armed Bandit Problem Imagine you are in a casionoand face multiple slot machines. Each machine is configured with an unknown probability of how likely you would get a reward at one play. The question is What\u0026rsquo;s the strategy to get the highest long-term reward?\nAn illustration of multi-armed bandit problem, refer to Lil\u0026rsquo;Log The Multi-Armed Bandit Problem and Its Solutions\nDefinition Upper Confidence Bounds(UCB) The UCB algorithm give a realtion between upper bound and probability confidence. That is to say, the UCB gives How likely is the real value of a random variable below the upper bound? To achieve this goal, we need to understand Hoeffding’s Inequality first.\nHoeffding’s Inequality Let $X_1,…,X_t$ be i.i.d. (independent and identically distributed) random variables and they are all bounded by the interval $[0, 1]$. The sample mean is\n$\\overline{X}_t = \\frac{1}{t} \\sum_{\\tau=1}^t X_\\tau$ Then for $u \u0026gt; 0$, we have:\n$$ \\mathbb{P} [ \\mathbb{E}[X] \u0026gt; \\overline{X}_t + u] \\leq e^{-2tu^2} $$\nThe inequation gives an upper bound in probability. Once the probability is small enough, we can say the upper bound is correct almost surely.\nCombine the Hoeffding’s Inequality and our goal. We can dervie\n$$ \\mathbb{P} [ Q(a) \u0026gt; \\hat{Q}_t(a) + U_t(a)] \\leq e^{-2t{U_t(a)}^2} $$\nOnce we get the bound, we can specify a target confidnce and always choose the action having highest upper bound.\n$$ a^{UCB}t = argmax{a \\in \\mathcal{A}} \\hat{Q}_t(a) + \\hat{U}_t(a) $$\nUCB1 Algorithm Since we want to measure the confidence of the upper bound, we can derive the confidence with the times of acting.\n$$ U_t(a) = \\sqrt{\\frac{2 \\log t}{N_t(a)}} \\text{ and } a^{UCB1}t = \\arg\\max{a \\in \\mathcal{A}} Q(a) + \\sqrt{\\frac{2 \\log t}{N_t(a)}} $$\n","permalink":"https://frankccccc.github.io/blog/posts/bandit/","summary":"Multi-Armed Bandit Problem Imagine you are in a casionoand face multiple slot machines. Each machine is configured with an unknown probability of how likely you would get a reward at one play. The question is What\u0026rsquo;s the strategy to get the highest long-term reward?\nAn illustration of multi-armed bandit problem, refer to Lil\u0026rsquo;Log The Multi-Armed Bandit Problem and Its Solutions\nDefinition Upper Confidence Bounds(UCB) The UCB algorithm give a realtion between upper bound and probability confidence.","title":"An Introduction to Multi-Armed Bandit Problem and Solutions"},{"content":"Gaussian Process Big Picture and Background Intuitively, Gaussian distribution define the state space, while Gaussian Process define the function space\nBefore we introduce Gaussian process, we should understand Gaussian distriution at first. For a RV(random variable) $X$ that follow Gaussian Distribution $\\mathcal{N}(0, 1)$ should be following image:\nThe P.D.F should be\n$$x \\sim \\mathcal{N}(\\mu, \\sigma) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} e^{- \\frac{1}{2} (\\frac{- \\mu}{\\sigma})^2}$$\nAs for Multivariate Gaussian Distribution, given 2 RV $x$, $y$ both 2 RV follow Gaussian Distribution $\\mathcal{N}(0, 1)$ we can illustrate it as\nThe P.D.F should be\nFor a set of random variables $X = (x_1, \u0026hellip;, x_k)$ that follow Gaussian distribution\n$$(x_1, \u0026hellip;, x_k) \\sim \\mathcal{N}(\\mu, \\Sigma) = \\frac{1}{\\sqrt{(2 \\pi)^k |\\Sigma|}} e^{- \\frac{1}{2} (X- \\mu)^{\\top} \\Sigma^{-1} (X- \\mu)}$$\nwhere $\\mu$ is the mean and $\\Sigma$ is the covariance matrix.\nThe Gaussian process can be regarded as a function space, for example, given a function $f(x)$ The Gaussian process can be illustrated as following:\nThe blue solid line represent the mean of the Gaussian process and the shaded blue area represent the standard deviation(which means the uncertainty of the RV) for the corresponding RV. For example, while $x=-4$, the function $f(4) = \\mathcal{N}(0, 2)$. That means the Gaussian process gives a Gaussian distribution $\\mathcal{N}(0, 2)$ to describe the possible value of $f(-4)$. The most likely value of $f(-4)$ is 0 (which is the mean of the distribution). As the figure shows, the Gaussian process is quite simple that the mean function is a constant 0 and the standard deviation is 2.\nThe dotted line are the functions sampled from the Gaussian process. Each line gives a mapping function from $x$ to $f(x)$.\nNote that the explaination above is from the point of view of function approximation. From the perspective of random process, the Gaussian process can be regarded as a time-variant system that the distribution is changing along the time.\nAfter understanding the priori, we can see the posteriori of Gaussian Process.\nIn the above figure, we estimate 5 points and re-draw the plot. The uncertainty of the estimated points become very small since we now have actual values of them via estimations.\nIn the view of 3D of the same posteriori, the Z axis means the probability. It illustrates the Gaussian process as a series of varying Gaussian distributions along the different values of $x$.\nThe above figure can be generated by this notebook. Specically thanks Martin Krasser. My notebook code is based on it.\nDefinition A Gaussian process is a time continuous stochastic process ${x_t; t \\in T}$ is Gaussian if and only if for every finite set of indices $t_1, \u0026hellip;, t_k$ in the index set $T$, $x_{t1},\u0026hellip;x_{tk} = (x_{t1}, \u0026hellip;, x_{tk})$ is a multivariate Gaussian random variable.\nFor example, any point $x_1, \u0026hellip; x_N \\in X, X \\in \\mathbb{R}^d$(Real Number with dimension $d$) is assigned a random variable $f(x)$ and where the joint distribution of a finite number of these variables $p(f(x_1),…,f(x_N))$ is itself Gaussian:\n$$p(f|X) = \\mathcal{N}(f|\\mu, K)$$\nwhere $\\mu$ is a vector which consists of mean function and $K$ is a covariance matrix which consists of covariance function or kernel function $\\kappa$. The set of mean function $\\mu = (m(x_1),…,m(x_N))$ give the mean value over set $X$. The set of kernel function is $K={K_{ij} = \\kappa(x_i,x_j)) where x_i, x_j \\in X}$ which define the correlaton between 2 values $x_i$ and $x_j$.\nNote that a data point $x_i$ or $x_j$ might be multi-dimensions. The kernel functions may defined on the vectors as well.\nKernel To understand the kernel function intuitively, the kernel function can be regarded as a kind of distance metric which give the distance in another space. For example, the kernel $k(x_i, x_j) = {x_i}^2 + {x_j}^2$ map the Cartesian coordinate to polar coordinate and convert the Euclidean distance into radius.\nSome common kernels are:\n  Constant Kernel:\n$K_C(x_i, x_j) = C$\n  RBF Kernel:\n$K_{RBF}(x_i, x_j) = e^{-\\frac{|| x_i - x_j ||^2}{2 \\sigma^2}}$\n  Periodic Kernel\nSuitable for periodic relation\n$K_{P}(x_i, x_j) = e^{-\\frac{2 \\sin^2 (\\frac{d}{2})}{\\ell^2}}$\n  Polynomial Kernel\n$K_{Poly}(x_i, x_j) = ( x_i^{\\top}x_j+ c)^d$\n  Neural Network Kernel\nModel the neural network as GP, aka neural network Gaussian Process(NNGP) For more detail, please refer to this slide.\n  For more detail, please refer to A Visual Exploration of Gaussian Processes. You can play with interactive widgets on the website.\nInference Given a training dataset with noise-free function values $f$ at inputs $X$, a GP prior can be converted into a GP posterior $p(f^{\\ast}|X^{\\ast},X,f)$ which can then be used to make predictions $f^{\\ast}$ at new inputs $X^{\\ast}$ By definition of a GP, the joint distribution of observed values $f$ and predictions $f^{\\ast}$ is again a Gaussian which can be partitioned into\n$$ \\begin{pmatrix} f\\newline f^{\\ast} \\end{pmatrix} \\sim \\mathcal{N} \\begin{pmatrix} 0, \\begin{pmatrix} K \u0026amp; K^{\\ast}\\newline K^{\\ast \\top} \u0026amp; K^{\\ast \\ast} \\end{pmatrix} \\end{pmatrix}$$\nwhere $K^{\\ast} = \\kappa(X,X^{\\ast})$ and $K^{\\ast \\ast} = \\kappa(X^{\\ast},X^{\\ast})$. With $N$ training data and $N^{\\ast}$ new input data $K$ is a $N×N$ matrix , $K^{\\ast}$ a $N×N^{\\ast}$ matrix and $K^{\\ast \\ast}$ a $N^{\\ast}×N^{\\ast}$ matrix.\nWe\u0026rsquo;ve known conditional distribution rules.\nAs a result, the predictive Gaussian Distribution is\n$$p(f^{\\ast} | X^{\\ast},X,f) = \\mathcal{N}(f^{\\ast} | \\mu^{\\ast}, \\Sigma^{\\ast})$$\n$$\\mu^{\\ast} = K^{\\ast \\top} K^{-1} f$$\n$$\\Sigma^{\\ast} = K^{\\ast \\ast} - K^{\\ast \\top} K^{-1} K^{\\ast}$$\nHowever, the above equations don\u0026rsquo;t consider the effect of noise. Suppose we need to evalutate a noisy model $y=f+\\epsilon$ where $\\epsilon \\sim \\mathcal{N}(0, \\sigma_y^2 I)$ is the noise. The noise follows normal distribution and has a covariance matrix $\\sigma_y^2 I$. Thus, the predictive distribution is\n$$p(f^{\\ast} | X^{\\ast},X,y) = \\mathcal{N}(f^{\\ast} | \\mu^{\\ast}, \\Sigma^{\\ast})$$\n$$\\mu^{\\ast} = K^{\\ast \\top} K_{y}^{-1} y$$\n$$\\Sigma^{\\ast} = K^{\\ast \\ast} - K^{\\ast \\top} K_{y}^{-1} K^{\\ast}$$\nwhere $K_{y}^{-1} = K + \\sigma_y^2 I$(linearilty of Gaussian distribution). Finally, we also want to replace the noise-free prediction $f^{\\ast}$ with noisy prediction $y^{\\ast}$. We can derive\n$$p(y^{\\ast} | X^{\\ast},X,y) = \\mathcal{N}(y^{\\ast} | \\mu^{\\ast}, \\Sigma^{\\ast} + \\sigma_y^2 I)$$\nFinally, we get the probability of noisy prediction $y^{\\ast}$ which conditions on noisy training dataset $X,y$ and test dataset $X^{\\ast}$.\n Bayesian Optimization In many machine learning or optimization problem, we need to optimize an unkown object function $f$. One of the solutions to optimize function $f$ is Bayesian Optimization. Bayesian Optimization assume the object function $f$ follows a distribution or prior model. This prior model is called surrogate model. We sample from the object function $f$ and approximate the function $f$ with surrogate model. The extra information like uncertainty provided from surrogate model contribute to the sample-efficiency of Bayesian optimization. In the mean time, we also use the acquisition function to choose the next sampling point.\nDefinition Formaly, suppose we have a block-box function $f : X \\to R$ that we with to minimize on some domain $x \\subseteq X$ . That is, the Bayesian optimization wish to \u001bfind\n$$\\begin{equation} x^{\\ast} = \\mathop{\\arg\\min}_{x \\in X} \\ \\ f(x) \\end{equation}$$\nIf we use Gaussian Process as prior(Surrogate Model), we can get\n$$p(f) = GP(f; \\mu, K)$$\nGiven observations $D = (x,f)$, we can condition our distribution on $D$ as usual\n$$p(f | D) = GP(f; \\mu_{f|D}, K_{f|D})$$\nSurrogate Model A popular model is Gaussian Process. Gaussian process defines a prior over functions and provides a flexiable, powerful and, smooth model which is especially suitable for dynamic models.\nAlgorithm The Bayesian optimization procedure is as follows.\n For index $t=1,2,…$ and an acquisition function $a(x|D)$\nrepeat:\n Find the next sampling point $x_t$ by optimizing the acquisition function over the surrogate model: $x_t = \\mathop{\\arg\\max}_{x \\in X} \\ a(x|D_{1:t−1})$ Or\n$x_t = \\mathop{\\arg\\min}_{x \\in X} \\ a(x|D_{1:t−1})$ Depends on the definition of acquisition function $a(x|D_{1:t−1})$\n Obtain a possibly noisy sample $y_t=f(x_t)+\\epsilon_t$ from the objective function $f$. Add the sample to previous samples $D_{1:t}=D_{1:t−1},(x_t,y_t)$ and update the surrogate model.   Probability improvement(PI) Method A naive idea is always evaluating the points with lowest value and then we can obtain a better result in every iteration. The PI method do the same thing exactly. However, the Gaussian Process gives a distribution of $f(x)$ on point $x$. As a result, in practice, we give an threshold $f'$, integrate the probability of $x \u0026lt; f'$ and, pick the point with highest probability. That is, PI always choose the point having largest probability of $x \u0026lt; f'$.\nFormaly, we can define an utility function $u(x)$\n$$u(x) = \\begin{cases} 0, \\ \\ f(x) \u0026gt; f'\\newline 1, \\ \\ f(x) \\leq f' \\end{cases}$$\nThen, integrate the probability of $f(x) \\leq f'$\n$$ a_{PI}(x|D) = \\mathbb{E}[u(x) | x, D] = \\int_{-\\infty}^{f'} \\mathcal{N}(f; \\mu(x), \\kappa(x, x)) \\ df =\\phi(f'; \\mu(x), \\kappa(x, x)) $$\nFinally, we choose next evaluating point $x_t$ with highest probability $a_{PI}(x|D_{1:t−1})$\n$\\begin{equation} x_t = \\mathop{\\arg\\max}_{x \\in X} \\ \\ a_{PI}(x|D_{1:t−1}) \\end{equation}$ Expected improvement(EI) Method PI algorithm is easy to understand. However, PI might get stuck in local optimal and underexplore globally. A better way is that we evaluates $f$ at the point that, in expectation, improves upon $f'$ the most. That is the idea of EI algorithm.\nFormaly, we suppose that $f'$ is the minimal value of $f$ observed so far. We can define an utility function as following:\n$$u(x) = \\mathop{\\max} (0, f' − f(x))$$\nThe ultility function can be regarded as the advanrage $f' - f(x)$ versus the current optimal $f'$. Then, we can derive the expectation via integration\n$$ a_{EI}(x|D) = \\mathbb{E}[u(x) | x, D] = \\int_{-\\infty}^{f'} (f' - f) \\mathcal{N}(f; \\mu(x), \\kappa(x, x)) \\ df $$\n$$ =(f' - \\mu(x))\\phi(f'; \\mu(x), \\kappa(x, x)) + \\kappa(x, x) \\mathcal{N}(f'; \\mu(x), \\kappa(x, x)) $$\nWe always choose the next evaluating point $x_t$ which has highest $a_{EI}(x|D_{1:t−1})$, thus\n$\\begin{equation} x_t = \\mathop{\\arg\\max}_{x \\in X} \\ \\ a_{EI}(x|D_{1:t−1}) \\end{equation}$ Intuitively, the term $(f' - \\mu(x))\\phi(f'; \\mu(x), \\kappa(x, x))$ can be seen as exploitation(it encourage to evaluate the point with higher advantage, lower $\\mu(x)$), since it means how much advantage does point $x$ has. The term $\\kappa(x, x) \\mathcal{N}(f'; \\mu(x), \\kappa(x, x))$ represents how much uncertainty does point $x$ has, so it can be viewed as exploration(it encourage to evaluate the point with higher uncertainty, higher $\\kappa(x, x)$). EI algorithm can trade off the exploration and exploitation automatically and also the most popular algorithm of Bayesian Optimization.\nBayesian Upper Confident Bound(Bayesian UCB, aka GP-UCB) Method Before diving to Bayesian UCB method, please understand the multi-armed bandit problem and UCB first.\nBayesian UCB inherents UCB. They both give a relation between upper bound and probability confidence. The different thing is UCB finds the relation with Hoeffding\u0026rsquo;s Inequality while Bayesian UCB find the relation with Gaussian distribution itself.\nFor example, it is common that we know if we sample values from Gaussian distribution, 95% of them are between the mean plus 2 standard deviation and mean subtract 2 standard deviation.\nFormally, we define the acquisition function as following\n$$ a_{UCB}(x|D; \\beta) = \\mu(x) - \\beta \\sigma(x) $$\nwhere $\\beta \u0026gt; 0$ is a tradeo\u001dff parameter and $\\sigma(x) = \\sqrt{\\kappa(x, x)}$ is the marginal standard deviation of $f(x)$.\nAccording the acquisition function, we always choose the next best evaluating point $x_t$\n$\\begin{equation} x_t = \\mathop{\\arg\\min}_{x \\in X} \\ \\ a_{UCB}(x|D_{1:t−1}; \\beta) \\end{equation}$ Again, the GP-UCB acquisition function contains explicit exploitation ($\\mu(x)$) and exploration ($\\sigma(x)$) terms. Nonetheless, strong theoretical results are known for GP-UCB, namely, that under certain conditions, the iterative application of this acquisition function will converge to the true global minimum of $f$.\nEntropy Search Since we\u0026rsquo;ve known the goal of the optimization problem is\n$$\\begin{equation} x^{\\ast} = \\mathop{\\arg\\min}_{x \\in X} \\ \\ f(x) \\end{equation}$$\nwhere $x^{\\ast}$ is the global optimal over the black-box function $f$. The entropy search aims to minimizing the entropy of $p(x^{\\ast} | D)$. That is, minimize the uncerntainty of $x^{\\ast}$ over the known data set $D$. We can define an utility function as following\n$$u(x) = H[x^{\\ast} | D] - H[x^{\\ast} | D, x, f(x)]$$\nwhere $H$ represent the Shannon entropy of the corresponding data point. The $u(x)$ means how much uncertinty of $p(x^{\\ast} | D)$ does the evaluation reduce after the data point $x$ is evaluated. In most of the time, the entropy(uncertainty) of $p(x^{\\ast} | D)$ should decrease after we evaluate more data points, so $u(x)$ should be positive in general. Thus, the acquisition function should be\n$$a_{ES}(x|D) = \\mathbb{E}[u(x) | x, D]$$\nHowever, we know $u(x)$ cannot be computed directly since it doesn\u0026rsquo;t have close form. There are a series of approximations trying to do it but I will ignore thme in this blog. Finally, we can choose the next evaluation point $x_t$ as following\n$\\begin{equation} x_t = \\mathop{\\arg\\max}_{x \\in X} \\ \\ a_{ES}(x|D_{1:t−1}; \\beta) \\end{equation}$ Reference This article is mainly based on\nGaussian Process\n  Gaussian processes by Martin Krasser\nProvide Python code to inference data with GP.\n  (ML 19.1) Gaussian processes - definition and first examples\nMade by mathematicalmonk. Give an intuitive explaination of Gaussian processes.\n  A Visual Exploration of Gaussian Processes\nProvide a lot of interactive widgets to play around with Gausssian Process but fewer math.\n  Gaussian Processes - Part 1\nAnother video providing a detailed explaination of Gaussian Process.\n  ML Tutorial: Gaussian Processes (Richard Turner)\nAnother video providing a detailed explaination of Gaussian Process.\n  UBC CS - Machine learning - Introduction to Gaussian processes\nUBC CS - Machine learning - Gaussian processes\nAnother video providing a detailed explaination of Gaussian Process.\n  Conditional Gaussian Distribution\n (PP 6.9) Conditional distributions of a Gaussian StackExchange - Deriving the conditional distributions of a multivariate normal distribution  Bayesian Optimization\n Bayesian optimization by Martin Krasser UAI 2018 2. Bayesian Optimization Washington University CSE515 - Bayesian Optimization  Kalman Filter\n  Bzarg: How a Kalman filter works, in pictures\nIntuitively explain Kalman Filter with picture \u0026amp; examples.\n  图说卡尔曼滤波，一份通俗易懂的教程\nIntuitively explain Kalman Filter with picture \u0026amp; examples. The article is translated from Bzarg: How a Kalman filter works.\n  ","permalink":"https://frankccccc.github.io/blog/posts/intro_gp_bayes_opt/","summary":"Gaussian Process Big Picture and Background Intuitively, Gaussian distribution define the state space, while Gaussian Process define the function space\nBefore we introduce Gaussian process, we should understand Gaussian distriution at first. For a RV(random variable) $X$ that follow Gaussian Distribution $\\mathcal{N}(0, 1)$ should be following image:\nThe P.D.F should be\n$$x \\sim \\mathcal{N}(\\mu, \\sigma) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} e^{- \\frac{1}{2} (\\frac{- \\mu}{\\sigma})^2}$$\nAs for Multivariate Gaussian Distribution, given 2 RV $x$, $y$ both 2 RV follow Gaussian Distribution $\\mathcal{N}(0, 1)$ we can illustrate it as","title":"A Very Brief Introduction to Gaussian Process and Bayesian Optimization"},{"content":"因為寫DL筆記時會用到大量數學符號，就索性把原先在Github上的DL_DB_Quick_Notes搬過來了，配合LATEX寫筆記順手很多，原先的Repo應該只會剩下收集Paper用。而最近生活上有些轉折，也許也會順便放些隨筆雜記，但就依心情而定。\n目前用的主題是PaperMod，整體設計算令人滿意，只不過在Deploy HUGO遇到蠻多麻煩，這邊簡單記錄一下\n設定Github Page Action Latex 設定 參考這篇\nStep 1 首先在安裝好的主題裡面layouts/partials/mathjax_support.html新增.html檔\n\u0026lt;script\u0026gt; MathJax = { tex: { inlineMath: [['$', '$'], ['\\\\(', '\\\\)']], displayMath: [['$$','$$'], ['\\\\[', '\\\\]']], processEscapes: true, processEnvironments: true }, options: { skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre'] } }; window.addEventListener('load', (event) =\u0026gt; { document.querySelectorAll(\u0026quot;mjx-container\u0026quot;).forEach(function(x){ x.parentElement.classList += 'has-jax'}) }); \u0026lt;/script\u0026gt; \u0026lt;script src=\u0026quot;https://polyfill.io/v3/polyfill.min.js?features=es6\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script type=\u0026quot;text/javascript\u0026quot; id=\u0026quot;MathJax-script\u0026quot; async src=\u0026quot;https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; Step 2 在layouts/partials/header.html的\u0026lt;/head\u0026gt; tag裡面再新增這段code\n{{ partial \u0026quot;mathjax_support.html\u0026quot; . }} Step 3 最後在assets/css/header.css檔裡面再加上這段code，如果沒有這個檔案，就把code加到所有頁面都會用到的CSS檔\ncode.has-jax { -webkit-font-smoothing: antialiased; background: inherit !important; border: none !important; font-size: 100%; } 以上，完工。給個範例\n$$a_{PI}(x|D) = \\mathbb{E}[u(x) | x, D] = \\int_{-\\infty}^{f'} \\mathcal{N}(f; \\mu(x), \\kappa(x, x)) \\ df =\\phi(f'; \\mu(x), \\kappa(x, x))$$ 顯示很完美\n$$a_{PI}(x|D) = \\mathbb{E}[u(x) | x, D] = \\int_{-\\infty}^{f'} \\mathcal{N}(f; \\mu(x), \\kappa(x, x)) \\ df =\\phi(f'; \\mu(x), \\kappa(x, x))$$\n只不過會Mathjax在parse底線的時，有時候會有一點問題，如\n$\\begin{equation} x_t = \\mathop{\\arg\\max}_{x \\in X} \\ \\ a_{PI}(x|D_{1:t−1}) \\end{equation}$ 顯示會出現\n$\\begin{equation} x_t = \\mathop{\\arg\\max}{x \\in X} \\ \\ a{PI}(x|D_{1:t−1}) \\end{equation}$\n會壞掉，解決辦法就是前後都加個 ` 符號，變成\n`$\\begin{equation} x_t = \\mathop{\\arg\\max}_{x \\in X} \\ \\ a_{PI}(x|D_{1:t−1}) \\end{equation}$` $\\begin{equation} x_t = \\mathop{\\arg\\max}_{x \\in X} \\ \\ a_{PI}(x|D_{1:t−1}) \\end{equation}$\n顯示就會正常了，但是會以Inline Code的方式顯示，就會變的小一點。這種現象的主要原因是在Step 1我們是把LATEX Code和Markdown的code一起parse，但Markdown語法本身就會用到底線，這會導致重複定義同一個符號，所以就需要而外把LATEX抓出來塞到Inline Code裡面處理，就不會重複定義。但基本上很少遇到有問題的情況，若遇到顯示有問題再加 ` 就好。\nMD 圖片路徑設定 由於HUGO在parse圖片連結時並不會對圖片連結進行轉換，也就是說遇到/img/figure.jpg會預設用絕對路徑解析，會抓到到baseurl/img/figure.jpg的圖片，遇到img/figure.jpg則會預設用相對路徑解析，就會抓到current_url/img/figure.jpg的圖片。\n但麻煩的是Deploy到Github Page上後，預設網址為https://{user_account}.github.io/{repository_name}/，網站的絕對路徑前綴就會變成https://{user_account}.github.io，而非HUGO config裡面設定的baseurl，會導致圖片完全無法顯示。\n解決辦法就是把所有MD的圖片路徑都轉成完整的網址，即在config.yml加上下面這行code就解決了\ncanonifyURLs: true Reference  Render LaTeX math expressions in Hugo with MathJax 3 深入但不淺出，如何用 github actions 自動發佈 gh-pages 使用Hugo+Github Pages建置Blog git submodule 教學 HUGO Theme: adityatelange/hugo-PaperMod  ","permalink":"https://frankccccc.github.io/blog/posts/move_blog/","summary":"因為寫DL筆記時會用到大量數學符號，就索性把原先在Github上的DL_DB_Quick_Notes搬過來了，配合LATEX寫筆記順手很多，原先的Repo應該只會剩下收集Paper用。而最近生活上有些轉折，也許也會順便放些隨筆雜記，但就依心情而定。\n目前用的主題是PaperMod，整體設計算令人滿意，只不過在Deploy HUGO遇到蠻多麻煩，這邊簡單記錄一下\n設定Github Page Action Latex 設定 參考這篇\nStep 1 首先在安裝好的主題裡面layouts/partials/mathjax_support.html新增.html檔\n\u0026lt;script\u0026gt; MathJax = { tex: { inlineMath: [['$', '$'], ['\\\\(', '\\\\)']], displayMath: [['$$','$$'], ['\\\\[', '\\\\]']], processEscapes: true, processEnvironments: true }, options: { skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre'] } }; window.addEventListener('load', (event) =\u0026gt; { document.querySelectorAll(\u0026quot;mjx-container\u0026quot;).forEach(function(x){ x.parentElement.classList += 'has-jax'}) }); \u0026lt;/script\u0026gt; \u0026lt;script src=\u0026quot;https://polyfill.io/v3/polyfill.min.js?features=es6\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script type=\u0026quot;text/javascript\u0026quot; id=\u0026quot;MathJax-script\u0026quot; async src=\u0026quot;https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; Step 2 在layouts/partials/header.html的\u0026lt;/head\u0026gt; tag裡面再新增這段code\n{{ partial \u0026quot;mathjax_support.html\u0026quot; . }} Step 3 最後在assets/css/header.","title":"部落格搬家記"}]