[{"content":"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model It is just the paper proposing MuZero. MuZero is quite famous when I write this note(Jan 2021). Lots of people tried to reproduce the incredible performance of the paper. Some of well-known implementations like muzero-general give a clear and modular implementation of MuZero. If you are interested in MuZero, you can play with it. Well, let\u0026rsquo;s diving into the paper.\nIntrodution The main idea of MuZero is to predict the future that are directly relevant for planning. The model receives the observation (e.g. an image of the Go board or the Atari screen) as an input and transforms it into a hidden state. The hidden state is then updated iteratively by a recurrent process that receives the previous hidden state and a hypothetical next action. At every one of these steps the model predicts the policy (e.g. the move to play), value function (e.g. the predicted winner), and immediate reward (e.g. the points scored by playing a move).\nAlgorithm The MuZero consist of 3 components: dynamic function, prediction function, representation function:\n  Dynamic Function $g(s^t, a^{t+1})=r^{t+1}, s^{t+1}$\n  Prediction Function $f(s^t)=p^t, v^t$\n  Representation Function $h(o^0)=s^0$\n  We denote $o^0$ as the initial observation, $s^0$ as the initial hidden state, $p^t$ as the policy function, $v^t$ as value function, and $r^t$ as reward function at time step $t$. These 3 components compose the deterministic latent dynamic model for MuZero. (The paper says stochastic transitions is left for future works)\nThe MuZero plan like Figure1 part A. Given a previous hidden state $s^{k−1}$ and a candidate action $a^k$, the dynamics function $g$ produces an immediate reward $r^k$ and a new hidden state $s^k$. The policy $p^k$ and value function $v^k$ are computed from the hidden state $s^k$ by a prediction function $f$.\nThe MuZero act in the environment like part B of Figure1. A Monte-Carlo Tree Search is performed at each timestep $t$, as described in A. An action $a_{t+1}$ is sampled from the search policy $\\pi_t$. The environment receives the action and generates a new observation $o_{t+1}$ and reward $u_{t+1}$. At the end of the episode the trajectory data is stored into a replay buffer.\nThe MuZero model $\\mu_{\\theta}$ with parameters $\\theta$, conditioned on past observations $o_1, \u0026hellip;, o_t$ and future actions $a_{t+1}, \u0026hellip;, a_{t+k}$.\nThe model always predicts the policy $p_t^k$, value function $v_t^k$, and immdiate reward $r_t^k$ after $k$ time steps at timestep $t$. For more detailed, the three future quantities are as following:\n  Policy\n$$ p_t^k \\approx \\pi (a_{t+k+1} | o_{1}, \u0026hellip;, o_t, a_{t+1}, \u0026hellip;, a_{t+k}) $$\nwhere $\\pi$ is the policy used to select real actions\n  Value Function\n$$ v_t^k \\approx E[u_{t+k+1} + \\gamma \\ u_{t+k+2} + \u0026hellip; | o_1 , \u0026hellip;, o_t, a_{t+1}, \u0026hellip;, a_{t+k}] $$\nwhere $u$ is the true, observation reward. $\\gamma$ is the discount factor of the environment.\n  Immediate Reward\n$$ r_t^k \\approx u_{t + k} $$\n  The MuZero train in the environment like part C of Figure1.\nAll parameters of the model are trained jointly to accurately match the policy, value, and reward, for every hypothetical step $k$, to corresponding target values observed after $k$ actual time-steps t have elapsed.(That is predict the policy, value, and reward after $k$ steps from current time-step $t$.) The training objective is to minimise the error between predicted policy $p_t^k$ and MCTS search policy $\\pi_{t+k}$.\nFor trade-off between accuracy and stability, we allow for long episodes with discounting and intermediate rewards by bootstrapping $n$ steps into the future from the search value. Final outcomes {lose, draw, win} in board games are treated as rewards $u_t \\in \\{ −1, 0, +1 \\}$ occurring at the final step of the episode.\nThus, MuZero define $z_t$ as folowing:\n$$ z_t = u_{t+1} + \\gamma u_{t+2} + \u0026hellip; + \\gamma^{n-1} u_{t+n} + \\gamma^n u_{t+n} $$\nThen, the loss function of MuZero is\n$$ l_t(\\theta) = \\sum_{k=0}^K l^{p}(\\pi_{t+k}, p_t^k) + \\sum_{k=0}^K l^v(z_{t+k}, v_t^k) + \\sum_{k=1}^K l^r(u_{t+k}, r_t^k) + c ||\\theta||^2 $$\nwhere $l^p$, $l^v$ and $l^r$ are loss functions for policy, value and reward, respectively. $c$ is a L2 regularization constant.\nExperiments \u0026amp; Results MuZero performs quite well both on board game and Atari 57.\nBoard Game\nAtari 57\n","permalink":"https://frankccccc.github.io/blog/posts/part_iii-from_alphago_to_muzero/","summary":"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model It is just the paper proposing MuZero. MuZero is quite famous when I write this note(Jan 2021). Lots of people tried to reproduce the incredible performance of the paper. Some of well-known implementations like muzero-general give a clear and modular implementation of MuZero. If you are interested in MuZero, you can play with it. Well, let\u0026rsquo;s diving into the paper.","title":"Part III - From AlphaGo to MuZero"},{"content":"Mastering the game of Go without human knowledge The paper propose AlphaGo Zero which is known as self-playing without human knowledge.\nReinforcement learning in AlphaGo Zero $$ (p, v) = f_{\\theta} $$\n$$ l = (z - v)^2 - \\pi^T log(p) + c||\\theta||^2 $$\nMastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm The paper propose AlphaZero which is known as self-playing to compete any kinds of board game.\n","permalink":"https://frankccccc.github.io/blog/posts/part_ii-from_alphago_to_muzero/","summary":"Mastering the game of Go without human knowledge The paper propose AlphaGo Zero which is known as self-playing without human knowledge.\nReinforcement learning in AlphaGo Zero $$ (p, v) = f_{\\theta} $$\n$$ l = (z - v)^2 - \\pi^T log(p) + c||\\theta||^2 $$\nMastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm The paper propose AlphaZero which is known as self-playing to compete any kinds of board game.","title":"Part II - From AlphaGo to MuZero"},{"content":"Value-Decomposition Network(VDN) QMIX Problem Setup And Assumption Constraint The QMIX imporve the VDN algorithm via give a more general form of the contraint. It defines the contraint like\n$$\\frac{\\partial Q_{tot}}{\\partial Q_{a}} \\geq 0, \\forall a$$\nwhere $Q_{tot}$ is the joint value function and $Q_{a}$ is the value function for each agent.\nAn intuitive eplaination is that we want the weights of any individual value function $Q_{a}$ are positive. If the weights of individual value function $Q_{a}$ are negative, it will discourage the agent to cooperate, since the higher $Q_{a}$, the lower joint value $Q_{tot}$. Trivially, the contraint of VDN is just a special case that $\\frac{\\partial Q_{tot}}{\\partial Q_{a}} = 1$.\nNetwork Architecture The architecture of QMIX is like the following figure\nFor each agent a and time step $t$, there is one agent network that represents its individual value function $Q_a(τ^a, u_t^a)$. We represent agent networks as DRQNs that receive the current individual observation $o_t^a$and the last action $u_{t−1}^a$ as input at each time step.\nThe mixing network is a feed-forward neural network that takes the agent network outputs $Q_a(τ^a, u_t^a)$ as input and mixes them monotonically, producing the values of $Q_{tot}$. To enforce the monotonicity constraint, the weights (but not the biases) of the mixing network are restricted to be non-negative and is produced by hypernetwork.\nEach hypernetwork consists of a single linear layer, followed by ReLU function to ensure that the mixing network weights are non-negative. Since we\u0026rsquo;ve assume that the multi-agent probllem can be solve by a joint value function. The hypernetworks take the current state $s_t$ as input.\nLoss Function QMIX can be trained end-by-end, the loss function is defined as\n$$L(\\theta) = \\sum_{i = 1}^{b}[(y_i^{tot} - Q_{tot}(\\tau, u, s; \\theta))^2]$$\nwhere $b$ is the batch size of transitions sampled from the replay buffer, and $y_{tot} = r + \\gamma \\ max_{u'} \\ Q_{tot}(τ', u', s'; θ^−)$, and $θ^-$ are the parameters of a target network asin DQN\nWeighted QMIX Papers IQL\nStabilising Experience Replay for Deep Multi-Agent Reinforcement Learning\nVDN\nValue-Decomposition Networks For Cooperative Multi-Agent Learning\nQMIX\nQMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning\nWeighted QMIX\nWeighted QMIX: Expanding Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning\nReference  多智能体强化学习算法 QMIX：Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning BAIR Blog - Scaling Multi-Agent Reinforcement Learning  ","permalink":"https://frankccccc.github.io/blog/posts/simple_guide_of_vdn_and_qmix/","summary":"Value-Decomposition Network(VDN) QMIX Problem Setup And Assumption Constraint The QMIX imporve the VDN algorithm via give a more general form of the contraint. It defines the contraint like\n$$\\frac{\\partial Q_{tot}}{\\partial Q_{a}} \\geq 0, \\forall a$$\nwhere $Q_{tot}$ is the joint value function and $Q_{a}$ is the value function for each agent.\nAn intuitive eplaination is that we want the weights of any individual value function $Q_{a}$ are positive. If the weights of individual value function $Q_{a}$ are negative, it will discourage the agent to cooperate, since the higher $Q_{a}$, the lower joint value $Q_{tot}$.","title":"Simple Guide Of VDN And QMIX"},{"content":"Problem Setup It is also knowd as Evidence Lower Bound(ELBO) or VLB. We can assume that $X$ are observations (data) and $Z$ are hidden/latent variables. In general, we can also imagine $Z$ as a parameter and the relationship between $Z$ and $X$ are represented as the following\nIn the mean time, by the definition of Bayes' Theorem and conditional probability, we can get\n$$p(Z | X) = \\frac{p(X | Z) p(Z)}{p(X)} = \\frac{p(X | Z) p(Z)}{\\int_{Z} p(X, Z)}$$\nJensen\u0026rsquo;s Inequality It states that for the convex transformation $f$, the mean $f(w \\cdot x + (1 - w) y)$ of $x$, $y$ on convex transform $f$ is less than or equal to the mean applied after convex transformation $w \\cdot f(x) + (1 - w) f(y)$.\nFormaly, corresponding to the notation of the above figure, the Jensen\u0026rsquo;s inequality can be defined as\n$$f(t x_1 + (1 - t) x_2) \\leq t f(x_1) + (1 - t) f(x_2)$$\nIn probability theory, for a random variable $X$ and a convex function $\\varphi$, we can state the inequality as\n$$\\varphi \\ (E[X]) \\leq E[\\varphi(X)]$$\n","permalink":"https://frankccccc.github.io/blog/posts/a_guide_of_variational_lower_bound/","summary":"Problem Setup It is also knowd as Evidence Lower Bound(ELBO) or VLB. We can assume that $X$ are observations (data) and $Z$ are hidden/latent variables. In general, we can also imagine $Z$ as a parameter and the relationship between $Z$ and $X$ are represented as the following\nIn the mean time, by the definition of Bayes' Theorem and conditional probability, we can get\n$$p(Z | X) = \\frac{p(X | Z) p(Z)}{p(X)} = \\frac{p(X | Z) p(Z)}{\\int_{Z} p(X, Z)}$$","title":"A Guide Of Variational Lower Bound"},{"content":"Shannon Entropy For discrete random variable $X$ with events $\\{ x_1, \u0026hellip;, x_n \\}$ and probability mass function $P(X)$, we defien the Shannon Entropy $H(X)$ as\n$$H(X) = E[-log_b \\ P(X)] = - \\sum_{i = 1}^{i = n} \\ P(x_i) log_b \\ P(x_i)$$\nwhere $b$ is the base of the logarithm. The unit of Shannon entropy is bit for $b = 2$ while nat for $b = e$\nThe Perspective of Venn Diagram We can illustrate the relation between joint entropy, conditional entropy, and mutual entropy as the following figure\nwhere $H(X), \\ H(Y)$ are Shannon entropy of RV $X, \\ Y$ respectively. $H(X, Y)$ is the joint entopy. $I(X; Y)$ is the mutual entropy. $H(X|Y), \\ H(Y|X)$ are conditional entropy that given $Y$ and $X$ respectively.\nJoint Entropy The joint distribution is $P(X,Y)$ for two discrete random variables $X$ and $Y$. Thus the joint entropy is defined as\n$$H(X, Y) = E[-log \\ P(X, Y)] = - \\sum_{i = 1}^{i = n} \\sum_{j = 1}^{j = n} \\ P(x_i, y_j) log \\ P(x_i, y_j)$$\nConditional Entropy The conditional entropy of $Y$ given $X$ is defined as\n$$H(Y | X) = H(X, Y) - H(Y)$$\n$$= - \\sum_{i = 1}^{i = n} \\sum_{j = 1}^{j = n} \\ P(x_i, y_j) log \\ P(x_i, y_j) - (- \\sum_{j = 1}^{j = n} \\ P(y_j) log \\ P(y_j))$$\n$$= \\sum_{j = 1}^{j = n} \\ (\\sum_{i = 1}^{i = n} P(x_i, y_j)) log \\ (\\sum_{i = 1}^{i = n} P(x_i, y_j)) - \\sum_{i = 1}^{i = n} \\sum_{j = 1}^{j = n} \\ P(x_i, y_j) log \\ P(x_i, y_j)$$\n$$ = \\sum_{i = 1}^{i = n} \\sum_{j = 1}^{j = n} \\ P(x_i, y_j) (log \\ (\\sum_{i = 1}^{i = n} P(x_i, y_j)) - log \\ P(x_i, y_j)) $$\n$$ = \\sum_{i = 1}^{i = n} \\sum_{j = 1}^{j = n} \\ P(x_i, y_j) (- log \\ \\frac{P(x_i, y_j)}{P(y_j)}) $$\n$$ = - \\sum_{i = 1}^{i = n} \\sum_{j = 1}^{j = n} \\ P(x_i, y_j) log \\ \\frac{P(x_i, y_j)}{P(y_j)} $$\nMutual Information(MI) In general, MI is the intersection or say the common information for both random variables $X$ and $Y$.\nLet $(X,Y)$ be a pair of random variables with values over the space $X \\times Y$. If their joint distribution is $P(X,Y)$ and the marginal distributions are $P_X(X)$ and $P_Y(Y)$, the mutual information is defined as\n$$I(X; Y) = H(X) + H(Y) - H(X, Y)$$\n$$ = - \\sum_{i = 1}^{i = n} \\ P_X(x_i) log \\ P_X(x_i) - \\sum_{j = 1}^{j = n} \\ P_Y(y_j) log \\ P_Y(y_j) + \\sum_{i = 1}^{i = n} \\sum_{j = 1}^{j = n} \\ P(x_i, y_j) log \\ P(x_i, y_j) $$\n$$ = - \\sum_{i = 1}^{i = n} \\ (\\sum_{j = 1}^{j = n} P(x_i, y_j)) \\ log (\\sum_{j = 1}^{j = n} P(x_i, y_j)) - \\sum_{j = 1}^{j = n} \\ (\\sum_{i = 1}^{i = n} P(x_i, y_j)) \\ log (\\sum_{i = 1}^{i = n} P(x_i, y_j)) $$\n$$ +\\sum_{i = 1}^{i = n} \\sum_{j = 1}^{j = n} \\ P(x_i, y_j) log \\ P(x_i, y_j) $$\n$$ = \\sum_{i = 1}^{i = n} \\sum_{j = 1}^{j = n} \\ P(x_i, y_j) log \\ \\frac{P(x_i, y_j)}{P_X(X) \\ P_Y(Y)} $$\nIn the view of set, MI can also be defined as\n$$I(X; Y) = H(X) + H(Y) - H(X, Y)$$\n$$ = H(X) - H(X | Y) = H(Y) - H(Y | X) $$\n$$ = H(X, Y) - H(X | Y) - H(Y | X) $$\n","permalink":"https://frankccccc.github.io/blog/posts/a_set_of_shannon_entropy/","summary":"Shannon Entropy For discrete random variable $X$ with events $\\{ x_1, \u0026hellip;, x_n \\}$ and probability mass function $P(X)$, we defien the Shannon Entropy $H(X)$ as\n$$H(X) = E[-log_b \\ P(X)] = - \\sum_{i = 1}^{i = n} \\ P(x_i) log_b \\ P(x_i)$$\nwhere $b$ is the base of the logarithm. The unit of Shannon entropy is bit for $b = 2$ while nat for $b = e$\nThe Perspective of Venn Diagram We can illustrate the relation between joint entropy, conditional entropy, and mutual entropy as the following figure","title":"A Set of Shannon Entropy"},{"content":"Neural Network Gaussian Process(NNGP) Model the neural network as GP, aka neural network Gaussian Process(NNGP). Intuitively, the kernel of NNGP compute the distance between the output vectors of 2 input data points.\nWe define the following functions as neural networks with fully-conntected layers:\n$$z_{i}^{1}(x) = b_i^{1} + \\sum_{j=1}^{N_1} \\ W_{ij}^{1}x_j^1(x), \\ \\ x_{j}^{1}(x) = \\phi(b_i^{0} + \\sum_{k=1}^{d_{in}} \\ W_{ik}^{0}x_k(x))$$\nwhere $b_i^{1}$ is the $i$th-bias of the second layer(the same as first hidden layer), $W_{ij}^{1}$ is the $i$th-weights of the first layer(the same as input layer) , function $\\phi$ is the activation function, and $x$ is the input data of the neural network. As a result,\nThus, the kernel of $l$-th layer is\n$$K_{NN}^l(x, x') = \\sigma_b^2 + \\sigma_w^2 E_{z_i^{l-1} \\sim GP(0, K^{l-1})}[\\phi(z_i^{l-1}(x)) \\phi(z_i^{l-1}(x'))]$$\nNeural Tangent Kernel(NTK) \n\u0026ldquo;In short, NTK represent the changes of the weights before and after the gradient descent update\u0026rdquo; Let\u0026rsquo;s start the journey of revealing the black-box neural networks.\nSetup a Neural Network First of all, we need to define a simple neural network with 2 hidden layers\n$$ y(x, w)$$\nwhere $y$ is the neural network with weights $w \\in \\mathbb{R}^m$ and, ${ x, \\bar{y} }_N$ is the dataset which is a set of the input data and the output data with $N$ data points. Since we focus on analyze the weight $w$, we simplify the notation $y(x, w)$ as $y(w)$\nSuppose we have a regression task on the network $y(w)$, define\n$$L(w) = \\frac{1}{N} \\frac{1}{2} \\Vert y(w) - \\bar{y} \\Vert^2_2 $$\nwhere $L(w)$ is our object loss which we want to minimize. Since the term $\\frac{1}{N}$ is regardless to our goal, just ignore it and we get a simpler loss function\n$$L(w) = \\frac{1}{2} \\Vert y(w) - \\bar{y} \\Vert^2_2 $$\nTo The Limit of Infinite Width In order to measure the difference of the weights during training the neural network, we define a normalized metric as following\n$$\\frac{\\Vert w_n - w_0 \\Vert_2}{\\Vert w_0 \\Vert_2}$$\nwhere $w_n$ and $w_0$ are the weights at n-th training iteration and the initial weights. $\\Vert w_n - w_0 \\Vert_2$ means the quantity of the differnce between parameters $w_n$ and $w_0$ and it is normalized by the 2-norm $\\Vert w_0 \\Vert_2$\nAs we can see, the difference of the weights during training decrease as the width of network grows. As a result, the trained weights should be very close to the inital weights $w_0$ as the width of network goes to infinity.\nApply Taylor Expansion We\u0026rsquo;ve known the Taylor expansion is\n$$f(x) = \\sum_{n=0}^{\\infty} \\ \\frac{f^{n}(a)}{n!} (x - a)^{n}$$\nA function $f(w)$ expanded on the $w_0$ with first order approximation is\n$$f(w) \\approx f(w_0) + \\frac{df(w_0)}{dw} (w - w_0)$$\nIt is trivial that if $w$ is a vector, we need to replace the derivative $\\frac{df(w_0)}{dw}$ with gradient $\\nabla_{w} f(w_0)^{\\top}$\n$$f(w) \\approx f(w_0) + \\nabla_{w} f(w_0)^{\\top} \\ (w - w_0)$$\nApply to the network $y(w)$\n$$y(w) \\approx y(w_0) + \\nabla_{w} y(w_0)^{\\top} \\ (w - w_0)$$\nwhere $\\nabla_{w} y(w_0)$ and $y(w_0)$ are constants.\nThus, the Taylor expansion of $y(w)$ is just a linear model. Though the expansion around $w_0$ is regardless to the proof of NTK, it is still a useful tool to analyze the accuracy of the linear approximation with infinite-wide network.\nHowever, the most difficult thing is how can we guarantee the approximation is accurate enough? It is so complex that I wouldn\u0026rsquo;t put it in this article but I will provide an intuitive explaination of what does NTK mean? in the following article. Please keep reading it if you are interested in it.\nAn Simpler Explaination Without Flow Simply, we only consider a 1-dimension network $f(x, w), \\ w, x, \\bar{y} \\in \\mathbb{R}$ for a dataset $x \\in X, \\ \\bar{y} \\in \\bar{Y}$ which are input data points and output data points respectively.\nFirst of all, let\u0026rsquo;s define the loss function of a neural network\n$$L_{1}(x, w) = \\frac{1}{2} \\Vert f(x, w) - \\bar{y} \\Vert^2_2$$\nThe gradient descent is\n$$ w_{t+1} = w_0 + \\eta \\ \\frac{dL_1(x, w)}{dw} $$ $$ = w_0 + \\eta (f(x, w) - \\bar{y}) \\frac{df(x, w)}{dw} $$\nwhere $\\eta$ is the learning rate.\nNTK represent the changes of the weights before and after the gradient descent update. Thus, the changes of weights can be defined as\n$$lim_{\\eta \\to 0} \\frac{f(x, w + \\eta \\ \\frac{dL_1(x', w)}{dw}) - f(x, w)}{\\eta}$$\n$$= lim_{\\eta \\to 0} \\frac{f(x, w + \\eta \\ (f(x', w) - \\bar{y}) \\frac{df(x', w)}{dw}) - f(x, w)}{\\eta}$$\nTo simplify the notation, let $\\eta \\ (f(x', w) - \\bar{y}) \\frac{df(x', w)}{dw}) = \\Delta w$.\nWe can derive\n$$ lim_{\\eta \\to 0} \\frac{f(x, w + \\eta \\ (f(x', w) - \\bar{y}) \\frac{df(x', w)}{dw}) - f(x, w)}{\\eta} $$\n$$ = lim_{\\eta \\to 0} \\frac{f(x, w + \\Delta w) - f(x, w)}{\\eta} $$\nSuppose the learning rate $\\eta$ is small enough and thus, $w \\approx w + \\Delta w$. We can expand around $w + \\Delta w$ with Taylor expansion\n$$f(x, w) \\approx f(x, w + \\Delta w) + \\frac{df(x, w + \\Delta w)}{dw} (w - (w + \\Delta w))$$\n$$ = f(x, w + \\Delta w) - \\frac{df(x, w + \\Delta w)}{dw}\\Delta w$$\nWe can get\n$$ lim_{\\eta \\to 0} \\frac{f(x, w + \\Delta w) - f(x, w)}{\\eta} $$\n$$ = lim_{\\eta \\to 0} \\frac{f(x, w + \\Delta w) - (f(x, w + \\Delta w) - \\frac{df(x, w + \\Delta w)}{dw} \\Delta w)}{\\eta} $$\n$$ = lim_{\\eta \\to 0} \\ \\frac{1}{\\eta} \\frac{df(x, w + \\Delta w)}{dw} \\Delta w $$\n$$ = lim_{\\eta \\to 0} \\ \\frac{df(x, w + \\eta \\ (f(x', w) - \\bar{y}) \\frac{df(x', w)}{dw})}{dw} \\ (f(x', w) - \\bar{y}) \\frac{df(x', w)}{dw} $$\n$$ = \\frac{df(x, w)}{dw} \\ (f(x', w) - \\bar{y}) \\frac{df(x', w)}{dw} $$\nSince the weight almost not change, let $w = w_0$ and NTK is defined as\n$$k_{1}^{NTK}(x, x') = \\frac{df(x,w)}{dw} \\frac{df(x', w)}{dw} = \\frac{df(x,w_0)}{dw} \\frac{df(x', w_0)}{dw}$$\nSince $f(x', w) - \\bar{y}$ would be very close to 0 while MSE is close to 0, we can simply ignore it. It is trivial that NTK represent the changes of weights before and after gradient descent. It measure the difference of weights quantitatively and thus we can approximate the process of gradient descent with Gaussian process.\nFlow And Vector Field So far, we\u0026rsquo;ve shown the neural tangent kernel on 1-width network. To move forward to the infinite-wide network, we need 2 tools to help us analyzing the process of gradient descent in high-dimensionalal. As a result, before diving into NTK more deeply, we need to understand what is Gradient Flow and Vector Field.\nVector Field Define a space $\\chi \\in \\mathbb{R}^d$ with d dimensions and a point of the space $x \\in \\mathbb{R}^d$. A hyperplane $f(x) \\ f: \\chi \\to \\mathbb{R}$. As we want to find the global minimal point $x^*$\n$$x^* = \\mathop{\\arg\\min}_{x \\in X} \\ f(x)$$\nThe gradient of the hyperplane $\\nabla_x \\ f: \\chi \\to \\mathbb{R}^d$ represent the gradients of each point on the hyperplane $f$.\nThen, we define a vector field $F: \\chi \\to \\mathbb{R}^d$ assigning the velocity vector to each points of the space. Mathmatically, the vector field $F$ has the same function space as the gradient $\\nabla_x f$. As a result, we can also see the gradient $\\nabla_x f$ as a vector field $- \\nabla_x f = F(x)$ which assigns the velocity vector $v \\in \\mathbb{R}$ to each point $x \\in \\chi$.\n$$F(x) = - \\nabla_x f(x) = v$$\nA hyperplane and the gradients can be illustrated as the following figure. The orange surface represents the hyperplane $f$ and the corresponding gradient $\\nabla_x f$ of each points $x \\in \\chi$ on the hyperplane $f$ is the blue arrows in the bottom. Note that the gradients $\\nabla_x f(x)$ here are ascent while gradients of our optimization problem are descent $- \\nabla_x f(x)$. They have oppsite direction. Intuitively, the gradients represent the direction and steepness of the points on the hyperplane while the vector field is the velocity vector of the points. Mathmatically, the gradients and the vector field have the same function space, so we let them be equal but not due to the physical perspective.\nThen we introduce another variable time. Let $c(t)$ for $c: \\mathbb{R} \\to \\mathbb{R}^d$ represent the dynamics of along the time $t$. The function $c(t)$ gives the position in the space $\\chi \\in \\mathbb{R}^d$ along time $t$.\nAs a result, we know\n$$c(t + \\delta) = c(t) + \\delta F(c(t)) = c(t) - \\delta \\nabla_x f(c(t))$$\nwhere $\\delta$ represent the time-step of 2 positions. $\\delta F(c(t)) = - \\delta \\nabla_x f(c(t))$ means time products velocity vector and then get the movment vector during the time $\\delta$.\nGradient Flow The gradient flow is defined as\n$$\\dot{X}(t) = F(c(t)) = - \\nabla_x f(c(t)) = - \\nabla_x f(c(t))\\dot{c}(t) = - \\nabla_x f(c(t)) \\frac{dc(t)}{dt}$$\nThe gradient flow describe changing gradients along time.\nCombined With Gradient Flow We\u0026rsquo;ve know the update of the gradient descent is\n$$w_{t+1} = w_t - \\eta \\nabla_{w} L(w_t)$$\nLet the function $w(t) = w_t$ and define the gradient flow over weights is $\\dot{w}(t)$\n$$\\dot{w}(t) = - \\nabla_{w} L(w(t))$$\nActually, the meaning of the gradient flow $\\dot{w}(t) = \\frac{dw(t)}{dt}$ is likey the changing direction of gradient descent along time.\nWe expand the gradient of the loss function with chain rule\n$$ \\dot{w}(t) = - \\nabla_{w} L(w(t)) $$\n$$ = - \\nabla_{w} \\frac{1}{2} \\Vert y(w(t)) - \\bar{y} \\Vert^2_2 $$\n$$ = - \\frac{1}{2} \\cdot 2 \\nabla_{w} y(w(t)) (y(w(t)) - \\bar{y}) $$\n$$ = - \\nabla_{w} y(w(t)) (y(w(t)) - \\bar{y}) $$\nNow we can derive the flow of the network $\\dot{y}(w(t))$\n$$ \\dot{y}(w(t)) = \\nabla_{w} y(w(t))^{\\top} \\dot{w}(t) $$\n$$ = -\\nabla_{w} y(w(t))^{\\top} \\nabla_{w} y(w) (y(w(t)) - \\bar{y}) $$\n$$ = - \\nabla_{w} y(w(t))^{\\top} \\nabla_{w} y(w(t))(y(w(t)) - \\bar{y}) $$\nTo simplify the notation, we replace the dynamics $w(t)$ with $w_t$.\n$$w(t) = w_t$$\nThus, we get\n$$\\dot{y}(w_t) = - \\nabla_{w} y(w_t)^{\\top} \\nabla_{w} y(w_t)(y(w_t) - \\bar{y})$$\nHowever, we\u0026rsquo;ve known the mathmatical form of the flow $\\dot{y}(w_t)$, but what\u0026rsquo;s the meaning of the flow $\\dot{y}(w_t)$? Well, we can see the updated weights $w_t$ during the gradient descent as a trajectory in a high-dimensional space. Since the learning rate $\\eta$ is quite small, the the difference of weights $w_t$ between before and after the gradient descent is very small. As a result, we can see the discrete porgress of the graient descent as a continuous trjectory like the following figure. The flow over the neural network $\\dot{y}(w_t)$ is actually the tangent line of $w_t$. The flow $\\dot{y}(w_t)$ describe the velocity vector of the point $w_t$ and can predicts close-enough next point $w_{t+1}$.\nSince $y(w_t) - \\bar{y}$ would be very close to 0, too while MSE is close to 0, we can simply ignore it.\nActually, we are now very close to the neural tangent kernel(NTK). The NTK is a kernel matrix defined as\n$$\\boldsymbol{K_{NTK}(x, x')} = \\nabla_{w} y(x, w_t)^{\\top} \\nabla_{w} y(x', w_t)$$ Since the weights of the infinite-wide network doesn\u0026rsquo;t change during the training.\n$$y(w_t) \\approx y(w_0)$$\nWe get\n$$ -\\nabla_{w} y(w_t)^{\\top} \\nabla_{w} y(w_t) \\approx -\\nabla_{w} y(w_0)^{\\top} \\nabla_{w} y(w_0) = -\\nabla_{w} y(x, w_0)^{\\top} \\nabla_{w} y(x', w_0) $$\n$$= \\boldsymbol{K_{NTK}(x, x')}$$ Again, $\\boldsymbol{K_{NTK}(x, x')}$ is the Neural Tangent Kernel, NTK.\nThe way here to measure the distance between 2 tangents is the Cosine Similarity with inner product. The cosine value of 2 identical vector is 1 and 2 orthorgonal vectors is 0 which are totally different. With an additional minus sign, we can regard the negative similarity as a kind of distance.\nTo summary, the weights of an infinite-wide network almost don\u0026rsquo;t change during training. As a result, the kernel always stay almost the same. We can use NTK to analyze many properties of neural network and the neural networks are no longer black boxes.\nPapers NNGP\n Deep Neural Networks as Gaussian Processes  NTK\n Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent  Reference Thank for the following posts / people sincerely.\nGaussian Distribution\n StackExchange - Product of two multivariate normal distribution  NNGP\n Deep Gaussian Processes Gonzalo Mateo García - Deep Neural Networks as Gaussian Processes Deep Gaussian Processes for Machine Learning  NTK\n Understanding the Neural Tangent Kernel By Rajat\u0026rsquo;s Blog  Code for the blog rajatvd/NTK   Ultra-Wide Deep Nets and the Neural Tangent Kernel (NTK) CMU ML Blog: Ultra-Wide Deep Nets and the Neural Tangent Kernel (NTK) Some Intuition on the Neural Tangent Kernel 直观理解Neural Tangent Kernel  Flow\n  Let it flow - Gradient flow and gradient descent\n  Max Planck Science - Gradient Flow I\n  StackExchange - gradient flow and what is, for example, L2 gradient?\n  Taylor Expansion\n Taylor series expansion  ","permalink":"https://frankccccc.github.io/blog/posts/toward_nngp_and_ntk/","summary":"Neural Network Gaussian Process(NNGP) Model the neural network as GP, aka neural network Gaussian Process(NNGP). Intuitively, the kernel of NNGP compute the distance between the output vectors of 2 input data points.\nWe define the following functions as neural networks with fully-conntected layers:\n$$z_{i}^{1}(x) = b_i^{1} + \\sum_{j=1}^{N_1} \\ W_{ij}^{1}x_j^1(x), \\ \\ x_{j}^{1}(x) = \\phi(b_i^{0} + \\sum_{k=1}^{d_{in}} \\ W_{ik}^{0}x_k(x))$$\nwhere $b_i^{1}$ is the $i$th-bias of the second layer(the same as first hidden layer), $W_{ij}^{1}$ is the $i$th-weights of the first layer(the same as input layer) , function $\\phi$ is the activation function, and $x$ is the input data of the neural network.","title":"Toward NNGP and NTK"},{"content":"The main different between 3 kinds of estimation is What do we assume for the prior? The Maximum Likelihood Estimation(MLE) doesn\u0026rsquo;t use any prior but only maiximize the probability according to the samples. On the other hand, MAP and Bayesian both use priors to estimate the probability. The Maximum A Posteriori(MAP) only use the probability of single event while Bayesian Estimation see a distribution as the prior.\nTo be continue\u0026hellip;\nReference  最大似然估計vs最大後驗概率 A Gentle Introduction to Maximum Likelihood Estimation and Maximum A Posteriori Estimation MLE, MAP and Bayesian Inference  ","permalink":"https://frankccccc.github.io/blog/posts/some_intuition_of_mle_map_and_bayesian_estimation/","summary":"The main different between 3 kinds of estimation is What do we assume for the prior? The Maximum Likelihood Estimation(MLE) doesn\u0026rsquo;t use any prior but only maiximize the probability according to the samples. On the other hand, MAP and Bayesian both use priors to estimate the probability. The Maximum A Posteriori(MAP) only use the probability of single event while Bayesian Estimation see a distribution as the prior.\nTo be continue\u0026hellip;","title":"Some Intuition Of MLE, MAP, and Bayesian Estimation"},{"content":"AlphaGo is quite famous when I was a freshman of college. It somehow is the reason that I was addicted to Reinforcement Learning. Thus Our journey of model-based RL will start here. Although it is not the first one that propose model-based RL, I still believe it will give a big picture of model-based RL.\nMastering the game of Go with deep neural networks and tree search Introduction AlphaGo combines 2 kinds of model, including policy network and value network. The policy network takes the board position as input and output the probability of next action of each position. The value network also take the board position as input and output the winner of the game.\nWe pass in the board position as a 19×19 image and use convolutional layers to construct a representation of the position. We use these neural networks to reduce the effective depth and breadth of the search tree: evaluating positions using a value network, and sampling actions using a policy network.\nWe train the model in 2 stage. In the first stage, we use supervised learning with KGS dataset to train the policy network to predict the next action of humans. Then, in the second stage, we use reinforment learning and self-play to train the model by themself.\nStage1: Supervised Learning of Policy Network A fast rollout policy $p_{\\pi}$ and supervised learning (SL) policy network $p_{\\sigma}$ are trained to predict human expert moves in a data set of positions. The fast rollout policy $p_{\\pi}$ is trained only with some important features such as Stone colour to reduce the complexity of the model(faster but less accurate)while the SL policy network $p_{\\sigma}$ is trained with whole position of Go.\nWe trained a 13-layer policy network, which we call the SL policy network, from 30 million positions from the KGS Go Server. Then we update the policy network with the following function to maximize the probability of predicting the action of human experts:\n$$ \\triangle \\sigma \\propto \\frac{\\partial log \\ p_{\\sigma}(a|s)}{\\partial \\sigma} $$\nStage2: Reinforcement learning of policy networks We use policy gradient reinforcement learning (RL) to update the network. The RL policy network $p_{\\rho}$ is **identical in structure to the SL policy network**, its weights $\\rho$ are **initialized to the same values**, $\\rho = \\sigma$. We play games between the current policy network $p_{\\rho}$ and a **randomly selected previous iteration of the policy network to prevent overfit and stablize training**. To update the RL policy network, we use **policy gradient** to maximize the expected outcome:\n$$ \\triangle \\rho \\propto \\frac{\\partial log \\ p_{\\rho}(a_t|s_t)}{\\partial \\rho} z_t $$\nHere we use a reward function $r(s)$ that t is 0 for all non-terminal time steps $t\u0026lt;T$. The outcome $z_t = \\pm r(s_T)$ is the terminal reward at the end of the game if the current player wins, $r(s_T) = +1$, loses $r(s_T) = -1$.\nStage2: Reinforcement learning of value networks Estimating a value function $v^p(s)$ that predicts the outcome from position $s$ of games played by using policy $p$ for both players\n$$ v^p(s) = E[z_t | s_t = s, a_{t \u0026hellip; T} \\sim p] $$\nWe approximate the value function using a value network $v_{\\theta}(s)$ with weights $\\theta$, $v_{\\theta}(s) \\approx v^{p_{\\rho}}(s) \\approx v^*(s)$.\nWe define the loss function of the value network with mean squared error(MSE):\n$$ \\triangle \\theta \\propto \\frac{\\partial v_{\\theta}(s)}{\\partial \\theta} (z - v_{\\theta}(s)) $$\nBut how do we search the optimal value through policy network? There are 5 steps as Figure3:\n  Step 1: Selection\nEach simulation traverses the tree by selecting the edge with maximum action value $Q$, plus a bonus $u(P)$ that depends on a stored prior probability $P(s, a)$ for that edge.\n  Step 2: Expansion\nThe leaf node may be expanded. The new node is processed once by the policy network $p_{\\sigma}$ with output $P(s, a)=p_{\\sigma}(a|s)$.\nEach edge $(s, a)$ of the search tree stores an action value $Q(s, a)$, visit count $N(s, a)$, and prior probability $P(s, a)$.\nThe $u(s, a)$ is a kind of bonus that is proportional to the prior probability but decays with repeated visits to encourage exploration.\nAt each time step $t$ of each simulation, an action $a_t$ is selected from state $s_t$\n$$ a_t = \\mathop{\\arg\\max}_a (Q(s_t, a) + u(s_t, a)) $$\nso as to maximize action value plus a bonus\n$$ u(s, a) \\propto \\frac{P(s, a)}{1 + N(s, a)} $$\n  Step 3: Evaluation\nThe leaf node is evaluated in two very different ways: first, by the value network $v_{\\theta}(s_L)$; and second, by the outcome $z_L$ of a random rollout played out until terminal step $T$ using the fast rollout policy $p_{\\pi}$; these evaluations are combined, using a mixing parameter $\\lambda$, into a leaf evaluation $V(s_L)$.\n$$ V(s_L) = (1 - \\lambda) v_{\\theta}(s_L) + \\lambda z_L $$\n  Step 4: Backup\nAt the end of simulation, the action values and visit counts of all traversed edges are updated. Each edge accumulates the visit count and mean evaluation of all simulations passing through that edge as following:\n$$ N(s, a) = \\sum_{i = 1}^{n} 1(s, a, i) $$\n$$ Q(s, a) = \\frac{1}{N(s, a)} \\sum_{i = 1}^{n} 1(s, a, i) V(s_L^i) $$\nwhere $s_L^i$ is the leaf node from the ith simulation and $1(s, a, i)$ indicates whether an edge $(s, a)$ was traversed during the ith simulation.\n  ","permalink":"https://frankccccc.github.io/blog/posts/part_i-from_alphago_to_muzero/","summary":"AlphaGo is quite famous when I was a freshman of college. It somehow is the reason that I was addicted to Reinforcement Learning. Thus Our journey of model-based RL will start here. Although it is not the first one that propose model-based RL, I still believe it will give a big picture of model-based RL.\nMastering the game of Go with deep neural networks and tree search Introduction AlphaGo combines 2 kinds of model, including policy network and value network.","title":"Part I - From AlphaGo to MuZero"},{"content":"","permalink":"https://frankccccc.github.io/blog/posts/a_glimpse_of_distributional_rl/","summary":"","title":"A Glimpse of Distributional RL"},{"content":"Multi-Armed Bandit Problem Imagine you are in a casionoand face multiple slot machines. Each machine is configured with an unknown probability of how likely you would get a reward at one play. The question is What\u0026rsquo;s the strategy to get the highest long-term reward?\nAn illustration of multi-armed bandit problem, refer to Lil\u0026rsquo;Log The Multi-Armed Bandit Problem and Its Solutions\nDefinition Upper Confidence Bounds(UCB) The UCB algorithm give a realtion between upper bound and probability confidence. That is to say, the UCB gives How likely is the real value of a random variable below the upper bound? To achieve this goal, we need to understand Hoeffding’s Inequality first.\nHoeffding’s Inequality Let $X_1,…,X_t$ be i.i.d. (independent and identically distributed) random variables and they are all bounded by the interval $[0, 1]$. The sample mean is\n$\\overline{X}_t = \\frac{1}{t} \\sum_{\\tau=1}^t X_\\tau$ Then for $u \u0026gt; 0$, we have:\n$$ P( E[X] \u0026gt; \\overline{X}_t + u) \\leq e^{-2tu^2} $$\nThe inequation gives an upper bound in probability. Once the probability is small enough, we can say the upper bound is correct almost surely.\nCombine the Hoeffding’s Inequality and our goal. We can dervie\n$$ P( Q(a) \u0026gt; \\hat{Q}_t(a) + U_t(a)) \\leq e^{-2t{U_t(a)}^2} $$\nOnce we get the bound, we can specify a target confidnce and always choose the action having highest upper bound.\n$$ a^{UCB}t = argmax{a \\in \\mathcal{A}} \\hat{Q}_t(a) + \\hat{U}_t(a) $$\nUCB1 Algorithm Since we want to measure the confidence of the upper bound, we can derive the confidence with the times of acting.\n$$ U_t(a) = \\sqrt{\\frac{2 \\log t}{N_t(a)}} \\text{ and } a^{UCB1}t = \\arg\\max{a \\in \\mathcal{A}} Q(a) + \\sqrt{\\frac{2 \\log t}{N_t(a)}} $$\nEpsilon Greedy Thompson Sampling Reference  Lil\u0026rsquo;Log - The Multi-Armed Bandit Problem and Its Solutions  ","permalink":"https://frankccccc.github.io/blog/posts/bandit/","summary":"Multi-Armed Bandit Problem Imagine you are in a casionoand face multiple slot machines. Each machine is configured with an unknown probability of how likely you would get a reward at one play. The question is What\u0026rsquo;s the strategy to get the highest long-term reward?\nAn illustration of multi-armed bandit problem, refer to Lil\u0026rsquo;Log The Multi-Armed Bandit Problem and Its Solutions\nDefinition Upper Confidence Bounds(UCB) The UCB algorithm give a realtion between upper bound and probability confidence.","title":"An Introduction to Multi-Armed Bandit Problem"},{"content":"Gaussian Process Big Picture and Background Intuitively, Gaussian distribution define the state space, while Gaussian Process define the function space\nBefore we introduce Gaussian process, we should understand Gaussian distriution at first. For a RV(random variable) $X$ that follow Gaussian Distribution $\\mathcal{N}(0, 1)$ should be following image:\nThe P.D.F should be\n$$x \\sim \\mathcal{N}(\\mu, \\sigma) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} e^{- \\frac{1}{2} (\\frac{- \\mu}{\\sigma})^2}$$\nAs for Multivariate Gaussian Distribution, given 2 RV $x$, $y$ both 2 RV follow Gaussian Distribution $\\mathcal{N}(0, 1)$ we can illustrate it as\nThe P.D.F should be\nFor a set of random variables $X = (x_1, \u0026hellip;, x_k)$ that follow Gaussian distribution\n$$(x_1, \u0026hellip;, x_k) \\sim \\mathcal{N}(\\mu, \\Sigma) = \\frac{1}{\\sqrt{(2 \\pi)^k |\\Sigma|}} e^{- \\frac{1}{2} (X- \\mu)^{\\top} \\Sigma^{-1} (X- \\mu)}$$\nwhere $\\mu$ is the mean and $\\Sigma$ is the covariance matrix.\nThe Gaussian process can be regarded as a function space, for example, given a function $f(x)$ The Gaussian process can be illustrated as following:\nThe blue solid line represent the mean of the Gaussian process and the shaded blue area represent the standard deviation(which means the uncertainty of the RV) for the corresponding RV. For example, while $x=-4$, the function $f(4) = \\mathcal{N}(0, 2)$. That means the Gaussian process gives a Gaussian distribution $\\mathcal{N}(0, 2)$ to describe the possible value of $f(-4)$. The most likely value of $f(-4)$ is 0 (which is the mean of the distribution). As the figure shows, the Gaussian process is quite simple that the mean function is a constant 0 and the standard deviation is 2.\nThe dotted line are the functions sampled from the Gaussian process. Each line gives a mapping function from $x$ to $f(x)$.\nNote that the explaination above is from the point of view of function approximation. From the perspective of random process, the Gaussian process can be regarded as a time-variant system that the distribution is changing along the time.\nAfter understanding the priori, we can see the posteriori of Gaussian Process.\nIn the above figure, we estimate 5 points and re-draw the plot. The uncertainty of the estimated points become very small since we now have actual values of them via estimations.\nIn the view of 3D of the same posteriori, the Z axis means the probability. It illustrates the Gaussian process as a series of varying Gaussian distributions along the different values of $x$.\nThe above figure can be generated by this notebook. Specically thanks Martin Krasser. My notebook code is based on it.\nDefinition A Gaussian process is a time continuous stochastic process ${x_t; t \\in T}$ is Gaussian if and only if for every finite set of indices $t_1, \u0026hellip;, t_k$ in the index set $T$, $x_{t1},\u0026hellip;x_{tk} = (x_{t1}, \u0026hellip;, x_{tk})$ is a multivariate Gaussian random variable.\nFor example, any point $x_1, \u0026hellip; x_N \\in X, X \\in \\mathbb{R}^d$(Real Number with dimension $d$) is assigned a random variable $f(x)$ and where the joint distribution of a finite number of these variables $p(f(x_1),…,f(x_N))$ is itself Gaussian:\n$$p(f|X) = \\mathcal{N}(f|\\mu, K)$$\nwhere $\\mu$ is a vector which consists of mean function and $K$ is a covariance matrix which consists of covariance function or kernel function $\\kappa$. The set of mean function $\\mu = (m(x_1),…,m(x_N))$ give the mean value over set $X$. The set of kernel function is $K={K_{ij} = \\kappa(x_i,x_j)) where x_i, x_j \\in X}$ which define the correlaton between 2 values $x_i$ and $x_j$.\nNote that a data point $x_i$ or $x_j$ might be multi-dimensions. The kernel functions may defined on the vectors as well.\nKernel To understand the kernel function intuitively, the kernel function can be regarded as a kind of distance metric which give the distance in another space. For example, the kernel $k(x_i, x_j) = {x_i}^2 + {x_j}^2$ map the Cartesian coordinate to polar coordinate and convert the Euclidean distance into radius.\nSome common kernels are:\n  Constant Kernel:\n$K_C(x_i, x_j) = C$\n  RBF Kernel:\n$K_{RBF}(x_i, x_j) = e^{-\\frac{|| x_i - x_j ||^2}{2 \\sigma^2}}$\n  Periodic Kernel\nSuitable for periodic relation\n$K_{P}(x_i, x_j) = e^{-\\frac{2 \\sin^2 (\\frac{d}{2})}{\\ell^2}}$\n  Polynomial Kernel\n$K_{Poly}(x_i, x_j) = ( x_i^{\\top}x_j+ c)^d$\n  Neural Network Kernel\nModel the neural network as GP, aka neural network Gaussian Process(NNGP). Intuitively, the kernel of NNGP compute the distance between the output vectors of 2 input data points.\nWe define the following functions as neural networks with fully-conntected layers:\n$$z_{i}^{1}(x) = b_i^{1} + \\sum_{j=1}^{N_1} \\ W_{ij}^{1}x_j^1(x), \\ \\ x_{j}^{1}(x) = \\phi(b_i^{0} + \\sum_{k=1}^{d_{in}} \\ W_{ik}^{0}x_k(x))$$\nwhere $b_i^{1}$ is the $i$th-bias of the second layer(the same as first hidden layer), $W_{ij}^{1}$ is the $i$th-weights of the first layer(the same as input layer) , function $\\phi$ is the activation function, and $x$ is the input data of the neural network. As a result,\nThus, the kernel of $l$-th layer is\n$$K_{NN}^l(x, x') = \\sigma_b^2 + \\sigma_w^2 E_{z_i^{l-1} \\sim GP(0, K^{l-1})}[\\phi(z_i^{l-1}(x)) \\phi(z_i^{l-1}(x'))]$$\nFor more detail, please refer to this slide and the paper Deep Neural Networks as Gaussian Processes.\n  For more detail, please refer to A Visual Exploration of Gaussian Processes. You can play with interactive widgets on the website.\nInference Given a training dataset with noise-free function values $f$ at inputs $X$, a GP prior can be converted into a GP posterior $p(f^{\\ast}|X^{\\ast},X,f)$ which can then be used to make predictions $f^{\\ast}$ at new inputs $X^{\\ast}$ By definition of a GP, the joint distribution of observed values $f$ and predictions $f^{\\ast}$ is again a Gaussian which can be partitioned into\n$$ \\begin{pmatrix} f\\newline f^{\\ast} \\end{pmatrix} \\sim \\mathcal{N} \\begin{pmatrix} 0, \\begin{pmatrix} K \u0026amp; K^{\\ast}\\newline K^{\\ast \\top} \u0026amp; K^{\\ast \\ast} \\end{pmatrix} \\end{pmatrix}$$\nwhere $K^{\\ast} = \\kappa(X,X^{\\ast})$ and $K^{\\ast \\ast} = \\kappa(X^{\\ast},X^{\\ast})$. With $N$ training data and $N^{\\ast}$ new input data $K$ is a $N×N$ matrix , $K^{\\ast}$ a $N×N^{\\ast}$ matrix and $K^{\\ast \\ast}$ a $N^{\\ast}×N^{\\ast}$ matrix.\nWe\u0026rsquo;ve known conditional distribution rules.\nAs a result, the predictive Gaussian Distribution is\n$$p(f^{\\ast} | X^{\\ast},X,f) = \\mathcal{N}(f^{\\ast} | \\mu^{\\ast}, \\Sigma^{\\ast})$$\n$$\\mu^{\\ast} = K^{\\ast \\top} K^{-1} f$$\n$$\\Sigma^{\\ast} = K^{\\ast \\ast} - K^{\\ast \\top} K^{-1} K^{\\ast}$$\nHowever, the above equations don\u0026rsquo;t consider the effect of noise. Suppose we need to evalutate a noisy model $y=f+\\epsilon$ where $\\epsilon \\sim \\mathcal{N}(0, \\sigma_y^2 I)$ is the noise. The noise follows normal distribution and has a covariance matrix $\\sigma_y^2 I$. Thus, the predictive distribution is\n$$p(f^{\\ast} | X^{\\ast},X,y) = \\mathcal{N}(f^{\\ast} | \\mu^{\\ast}, \\Sigma^{\\ast})$$\n$$\\mu^{\\ast} = K^{\\ast \\top} K_{y}^{-1} y$$\n$$\\Sigma^{\\ast} = K^{\\ast \\ast} - K^{\\ast \\top} K_{y}^{-1} K^{\\ast}$$\nwhere $K_{y}^{-1} = K + \\sigma_y^2 I$(linearilty of Gaussian distribution). Finally, we also want to replace the noise-free prediction $f^{\\ast}$ with noisy prediction $y^{\\ast}$. We can derive\n$$p(y^{\\ast} | X^{\\ast},X,y) = \\mathcal{N}(y^{\\ast} | \\mu^{\\ast}, \\Sigma^{\\ast} + \\sigma_y^2 I)$$\nFinally, we get the probability of noisy prediction $y^{\\ast}$ which conditions on noisy training dataset $X,y$ and test dataset $X^{\\ast}$.\n Bayesian Optimization In many machine learning or optimization problem, we need to optimize an unkown object function $f$. One of the solutions to optimize function $f$ is Bayesian Optimization. Bayesian Optimization assume the object function $f$ follows a distribution or prior model. This prior model is called surrogate model. We sample from the object function $f$ and approximate the function $f$ with surrogate model. The extra information like uncertainty provided from surrogate model contribute to the sample-efficiency of Bayesian optimization. In the mean time, we also use the acquisition function to choose the next sampling point.\nDefinition Formaly, suppose we have a block-box function $f : X \\to R$ that we with to minimize on some domain $x \\subseteq X$ . That is, the Bayesian optimization wish to \u001bfind\n$$\\begin{equation} x^{\\ast} = \\mathop{\\arg\\min}_{x \\in X} \\ \\ f(x) \\end{equation}$$\nIf we use Gaussian Process as prior(Surrogate Model), we can get\n$$p(f) = GP(f; \\mu, K)$$\nGiven observations $D = (x,f)$, we can condition our distribution on $D$ as usual\n$$p(f | D) = GP(f; \\mu_{f|D}, K_{f|D})$$\nSurrogate Model A popular model is Gaussian Process. Gaussian process defines a prior over functions and provides a flexiable, powerful and, smooth model which is especially suitable for dynamic models.\nAlgorithm The Bayesian optimization procedure is as follows.\n For index $t=1,2,…$ and an acquisition function $a(x|D)$\nrepeat:\n Find the next sampling point $x_t$ by optimizing the acquisition function over the surrogate model: $x_t = \\mathop{\\arg\\max}_{x \\in X} \\ a(x|D_{1:t−1})$ Or\n$x_t = \\mathop{\\arg\\min}_{x \\in X} \\ a(x|D_{1:t−1})$ Depends on the definition of acquisition function $a(x|D_{1:t−1})$\n Obtain a possibly noisy sample $y_t=f(x_t)+\\epsilon_t$ from the objective function $f$. Add the sample to previous samples $D_{1:t}=D_{1:t−1},(x_t,y_t)$ and update the surrogate model.   Probability improvement(PI) Method A naive idea is always evaluating the points with lowest value and then we can obtain a better result in every iteration. The PI method do the same thing exactly. However, the Gaussian Process gives a distribution of $f(x)$ on point $x$. As a result, in practice, we give an threshold $f'$, integrate the probability of $x \u0026lt; f'$ and, pick the point with highest probability. That is, PI always choose the point having largest probability of $x \u0026lt; f'$.\nFormaly, we can define an utility function $u(x)$\n$$u(x) = \\begin{cases} 0, \\ \\ f(x) \u0026gt; f'\\newline 1, \\ \\ f(x) \\leq f' \\end{cases}$$\nThen, integrate the probability of $f(x) \\leq f'$\n$$ a_{PI}(x|D) = E[u(x) | x, D] = \\int_{-\\infty}^{f'} \\mathcal{N}(f; \\mu(x), \\kappa(x, x)) \\ df =\\phi(f'; \\mu(x), \\kappa(x, x)) $$\nFinally, we choose next evaluating point $x_t$ with highest probability $a_{PI}(x|D_{1:t−1})$\n$\\begin{equation} x_t = \\mathop{\\arg\\max}_{x \\in X} \\ \\ a_{PI}(x|D_{1:t−1}) \\end{equation}$ Expected improvement(EI) Method PI algorithm is easy to understand. However, PI might get stuck in local optimal and underexplore globally. A better way is that we evaluates $f$ at the point that, in expectation, improves upon $f'$ the most. That is the idea of EI algorithm.\nFormaly, we suppose that $f'$ is the minimal value of $f$ observed so far. We can define an utility function as following:\n$$u(x) = \\mathop{\\max} (0, f' − f(x))$$\nThe ultility function can be regarded as the advanrage $f' - f(x)$ versus the current optimal $f'$. Then, we can derive the expectation via integration\n$$ a_{EI}(x|D) = E[u(x) | x, D] = \\int_{-\\infty}^{f'} (f' - f) \\mathcal{N}(f; \\mu(x), \\kappa(x, x)) \\ df $$\n$$ =(f' - \\mu(x))\\phi(f'; \\mu(x), \\kappa(x, x)) + \\kappa(x, x) \\mathcal{N}(f'; \\mu(x), \\kappa(x, x)) $$\nWe always choose the next evaluating point $x_t$ which has highest $a_{EI}(x|D_{1:t−1})$, thus\n$\\begin{equation} x_t = \\mathop{\\arg\\max}_{x \\in X} \\ \\ a_{EI}(x|D_{1:t−1}) \\end{equation}$ Intuitively, the term $(f' - \\mu(x))\\phi(f'; \\mu(x), \\kappa(x, x))$ can be seen as exploitation(it encourage to evaluate the point with higher advantage, lower $\\mu(x)$), since it means how much advantage does point $x$ has. The term $\\kappa(x, x) \\mathcal{N}(f'; \\mu(x), \\kappa(x, x))$ represents how much uncertainty does point $x$ has, so it can be viewed as exploration(it encourage to evaluate the point with higher uncertainty, higher $\\kappa(x, x)$). EI algorithm can trade off the exploration and exploitation automatically and also the most popular algorithm of Bayesian Optimization.\nBayesian Upper Confident Bound(Bayesian UCB, aka GP-UCB) Method Before diving to Bayesian UCB method, please understand the multi-armed bandit problem and UCB first.\nBayesian UCB inherents UCB. They both give a relation between upper bound and probability confidence. The different thing is UCB finds the relation with Hoeffding\u0026rsquo;s Inequality while Bayesian UCB find the relation with Gaussian distribution itself.\nFor example, it is common that we know if we sample values from Gaussian distribution, 95% of them are between the mean plus 2 standard deviation and mean subtract 2 standard deviation.\nFormally, we define the acquisition function as following\n$$ a_{UCB}(x|D; \\beta) = \\mu(x) - \\beta \\sigma(x) $$\nwhere $\\beta \u0026gt; 0$ is a tradeo\u001dff parameter and $\\sigma(x) = \\sqrt{\\kappa(x, x)}$ is the marginal standard deviation of $f(x)$.\nAccording the acquisition function, we always choose the next best evaluating point $x_t$\n$\\begin{equation} x_t = \\mathop{\\arg\\min}_{x \\in X} \\ \\ a_{UCB}(x|D_{1:t−1}; \\beta) \\end{equation}$ Again, the GP-UCB acquisition function contains explicit exploitation ($\\mu(x)$) and exploration ($\\sigma(x)$) terms. Nonetheless, strong theoretical results are known for GP-UCB, namely, that under certain conditions, the iterative application of this acquisition function will converge to the true global minimum of $f$.\nEntropy Search Since we\u0026rsquo;ve known the goal of the optimization problem is\n$$\\begin{equation} x^{\\ast} = \\mathop{\\arg\\min}_{x \\in X} \\ \\ f(x) \\end{equation}$$\nwhere $x^{\\ast}$ is the global optimal over the black-box function $f$. The entropy search aims to minimizing the entropy of $p(x^{\\ast} | D)$. That is, minimize the uncerntainty of $x^{\\ast}$ over the known data set $D$. We can define an utility function as following\n$$u(x) = H[x^{\\ast} | D] - H[x^{\\ast} | D, x, f(x)]$$\nwhere $H$ represent the Shannon entropy of the corresponding data point. The $u(x)$ means how much uncertinty of $p(x^{\\ast} | D)$ does the evaluation reduce after the data point $x$ is evaluated. In most of the time, the entropy(uncertainty) of $p(x^{\\ast} | D)$ should decrease after we evaluate more data points, so $u(x)$ should be positive in general. Thus, the acquisition function should be\n$$a_{ES}(x|D) = E[u(x) | x, D]$$\nHowever, we know $u(x)$ cannot be computed directly since it doesn\u0026rsquo;t have close form. There are a series of approximations trying to do it but I will ignore thme in this blog. Finally, we can choose the next evaluation point $x_t$ as following\n$\\begin{equation} x_t = \\mathop{\\arg\\max}_{x \\in X} \\ \\ a_{ES}(x|D_{1:t−1}; \\beta) \\end{equation}$ Reference This article is mainly based on\nGaussian Process\n  Gaussian processes by Martin Krasser\nProvide Python code to inference data with GP.\n  (ML 19.1) Gaussian processes - definition and first examples\nMade by mathematicalmonk. Give an intuitive explaination of Gaussian processes.\n  A Visual Exploration of Gaussian Processes\nProvide a lot of interactive widgets to play around with Gausssian Process but fewer math.\n  Gaussian Processes - Part 1\nAnother video providing a detailed explaination of Gaussian Process.\n  ML Tutorial: Gaussian Processes (Richard Turner)\nAnother video providing a detailed explaination of Gaussian Process.\n  UBC CS - Machine learning - Introduction to Gaussian processes\nUBC CS - Machine learning - Gaussian processes\nAnother video providing a detailed explaination of Gaussian Process.\n  Conditional Gaussian Distribution\n (PP 6.9) Conditional distributions of a Gaussian StackExchange - Deriving the conditional distributions of a multivariate normal distribution  Bayesian Optimization\n Bayesian optimization by Martin Krasser UAI 2018 2. Bayesian Optimization Washington University CSE515 - Bayesian Optimization  Kalman Filter\n  Bzarg: How a Kalman filter works, in pictures\nIntuitively explain Kalman Filter with picture \u0026amp; examples.\n  图说卡尔曼滤波，一份通俗易懂的教程\nIntuitively explain Kalman Filter with picture \u0026amp; examples. The article is translated from Bzarg: How a Kalman filter works.\n  ","permalink":"https://frankccccc.github.io/blog/posts/intro_gp_bayes_opt/","summary":"Gaussian Process Big Picture and Background Intuitively, Gaussian distribution define the state space, while Gaussian Process define the function space\nBefore we introduce Gaussian process, we should understand Gaussian distriution at first. For a RV(random variable) $X$ that follow Gaussian Distribution $\\mathcal{N}(0, 1)$ should be following image:\nThe P.D.F should be\n$$x \\sim \\mathcal{N}(\\mu, \\sigma) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} e^{- \\frac{1}{2} (\\frac{- \\mu}{\\sigma})^2}$$\nAs for Multivariate Gaussian Distribution, given 2 RV $x$, $y$ both 2 RV follow Gaussian Distribution $\\mathcal{N}(0, 1)$ we can illustrate it as","title":"A Very Brief Introduction to Gaussian Process and Bayesian Optimization"},{"content":"因為寫DL筆記時會用到大量數學符號，就索性把原先在Github上的DL_DB_Quick_Notes搬過來了，配合LATEX寫筆記順手很多，原先的Repo應該只會剩下收集Paper用。而最近生活上有些轉折，也許也會順便放些隨筆雜記，但就依心情而定。\n目前用的主題是PaperMod，整體設計算令人滿意，只不過在Deploy Hugo遇到蠻多麻煩，這邊簡單記錄一下\n設定Github Page Action 參考PaperMod ExampleSite的gh-pages.yml設定，自己再作一些修改，大致如下\nname: Build GH-Pages on: push: paths-ignore: - 'images/**' - 'LICENSE' - 'README.md' branches: - master workflow_dispatch: # manual run jobs: deploy: runs-on: ubuntu-latest steps: - name: Git checkout uses: actions/checkout@v2 with: ref: master - name: Get Theme run: git submodule update --init --recursive - name: Update theme to Latest commit run: git submodule update --remote --merge - name: Setup hugo uses: peaceiris/actions-hugo@v2 with: hugo-version: 'latest' - name: Build run: hugo --buildDrafts --gc --verbose --minify - name: Deploy uses: peaceiris/actions-gh-pages@v3 with: github_token: ${{ secrets.YOUR_TOKEN }} publish_dir: ./public name隨意填上該Action的名稱，on則是event trigger，指定在甚麼時候trigger jobs，並在branch內設定要被build的分支，\nstep的話就是填寫Build到Deploy的步驟，可以大致看出依序是先git checkout到要被build的分支，然後下載更新themes，然後設定Hugo，再來用指令hugo --buildDrafts --gc --verbose --minify build static website，其中--buildDrafts代表把草稿也build好公開，--minify則是盡量壓縮build好的網站大小。\n最後Deploy步驟則是要把build好的網頁推到gh-pages branch上，注意分支gh-pages需要手動先創好推上Github才能讓Github Action自動Build，否則會報錯。同時Github會需要全限修改分支內容，要去Settings-\u0026gt;Developer settings-\u0026gt;Personal access tokens裡面新增token，並給予workflow、admin:repo_hook權限(我有給這些權限，但是不確定那些真的會需要)，按確定後把Token複製下來。\n感謝Milk Midi整理\n接下來再到your_blog_repo-\u0026gt;Settings-\u0026gt;Secrets新增Actions secrets，把Token貼上，再把Actions secrets 的名字貼到YOUR_TOKEN就好。\n感謝Milk Midi整理\n比較詳細的圖解說明可以參考這篇，太懶得紀錄這種瑣碎操作。\nLatex 設定 參考這篇\nStep 1 首先在安裝好的主題裡面layouts/partials/mathjax_support.html新增.html檔\n\u0026lt;script\u0026gt; MathJax = { tex: { inlineMath: [['$', '$'], ['\\\\(', '\\\\)']], displayMath: [['$$','$$'], ['\\\\[', '\\\\]']], processEscapes: true, processEnvironments: true }, options: { skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre'] } }; window.addEventListener('load', (event) =\u0026gt; { document.querySelectorAll(\u0026quot;mjx-container\u0026quot;).forEach(function(x){ x.parentElement.classList += 'has-jax'}) }); \u0026lt;/script\u0026gt; \u0026lt;script src=\u0026quot;https://polyfill.io/v3/polyfill.min.js?features=es6\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script type=\u0026quot;text/javascript\u0026quot; id=\u0026quot;MathJax-script\u0026quot; async src=\u0026quot;https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; Step 2 在layouts/partials/header.html的\u0026lt;/head\u0026gt; tag裡面再新增這段code\n{{ partial \u0026quot;mathjax_support.html\u0026quot; . }} Step 3 最後在assets/css/header.css檔裡面再加上這段code，如果沒有這個檔案，就把code加到所有頁面都會用到的CSS檔\ncode.has-jax { -webkit-font-smoothing: antialiased; background: inherit !important; border: none !important; font-size: 100%; } 以上，完工。給個範例\n$$a_{PI}(x|D) = E[u(x) | x, D] = \\int_{-\\infty}^{f'} \\mathcal{N}(f; \\mu(x), \\kappa(x, x)) \\ df =\\phi(f'; \\mu(x), \\kappa(x, x))$$ 顯示很完美\n$$a_{PI}(x|D) = E[u(x) | x, D] = \\int_{-\\infty}^{f'} \\mathcal{N}(f; \\mu(x), \\kappa(x, x)) \\ df =\\phi(f'; \\mu(x), \\kappa(x, x))$$\n只不過會Mathjax在parse底線的時，有時候會有一點問題，如\n$\\begin{equation} x_t = \\mathop{\\arg\\max}_{x \\in X} \\ \\ a_{PI}(x|D_{1:t−1}) \\end{equation}$ 顯示會出現\n$\\begin{equation} x_t = \\mathop{\\arg\\max}{x \\in X} \\ \\ a{PI}(x|D_{1:t−1}) \\end{equation}$\n會壞掉，解決辦法就是前後都加個 ` 符號，變成\n`$\\begin{equation} x_t = \\mathop{\\arg\\max}_{x \\in X} \\ \\ a_{PI}(x|D_{1:t−1}) \\end{equation}$` $\\begin{equation} x_t = \\mathop{\\arg\\max}_{x \\in X} \\ \\ a_{PI}(x|D_{1:t−1}) \\end{equation}$\n顯示就會正常了，但是會以Inline Code的方式顯示，就會變的小一點。這種現象的主要原因是在Step 1我們是把LATEX Code和Markdown的code一起parse，但Markdown語法本身就會用到底線，這會導致重複定義同一個符號，所以就需要而外把LATEX抓出來塞到Inline Code裡面處理，就不會重複定義。但基本上很少遇到有問題的情況，若遇到顯示有問題再加 ` 就好。\nMD 圖片路徑設定 由於Hugo在parse圖片連結時並不會對圖片連結進行轉換，也就是說遇到/blog/img/figure.jpg會預設用絕對路徑解析，會抓到到baseurl/blog/img/figure.jpg的圖片，遇到img/figure.jpg則會預設用相對路徑解析，就會抓到current_url/blog/img/figure.jpg的圖片。\n但麻煩的是Deploy到Github Page上後，預設網址為https://{user_account}.github.io/{repository_name}/，網站的絕對路徑前綴就會變成https://{user_account}.github.io，而非Hugo config裡面設定的baseurl，會導致圖片完全無法顯示。\n解決辦法就是把所有MD的圖片路徑都轉成完整的網址，即在config.yml加上下面這行code就解決了\ncanonifyURLs: true MD 圖片路徑設定 Vol.2 用了一段時間後，Post數量超過一頁能顯示的範圍，才發現Paginaiton有問題，原因就在於canonifyURLs: true會把Pagination的URL都弄成絕對路徑，把網址搞壞。解決方法就是把canonifyURL設回預設值canonifyURLs: false，但這樣就會把md圖片又搞壞。\n最後最後的解決辦法就是把md的圖片路徑前面加上/blog，因為部屬在Github Page上的Reop名是blog，所以Github Page的網址就是https://{Username}.github.io/blog，，而Hugo是靜態網頁產生器，Build後的圖片都會serve在/img下，所以就直接用專案路徑加Build好的圖片路徑來寫md(改來改去超煩的)。\n安裝Utterances留言板 Utterances Official\n相較於GitTalk, GitMent等基於Github Issue的留言插件，Utterances要求的權限最少，因此決定採用。設定安裝可參考此篇\n線上壓縮圖片 Google推出的Squoosh挺方便的。\nFinally 最後的成果就如這個Blog，Repo的連結則在這裡，有興趣的人應該可以參考一下code。雖然說這種文章應該多少對有需要的人有所幫助，而且很容易就可以獲得成就感(因為寫這種文章不需要動腦)，相對於人生，實在容易許多。\n最後工商一下舊部落格 DL DB Quick Note，但文章應該會慢慢整理搬過來，對RL, NTK, 費茲傑羅, 黃麗群和獨立音樂有興趣的也歡迎討論，只不過我可能只回重要事項，其他看心情回覆就是了。\nReference  Render LaTeX math expressions in Hugo with MathJax 3 深入但不淺出，如何用 github actions 自動發佈 gh-pages 使用Hugo+Github Pages建置Blog git submodule 教學 Hugo Theme: adityatelange/hugo-PaperMod List of LaTeX mathematical symbols  ","permalink":"https://frankccccc.github.io/blog/posts/move_blog/","summary":"因為寫DL筆記時會用到大量數學符號，就索性把原先在Github上的DL_DB_Quick_Notes搬過來了，配合LATEX寫筆記順手很多，原先的Repo應該只會剩下收集Paper用。而最近生活上有些轉折，也許也會順便放些隨筆雜記，但就依心情而定。\n目前用的主題是PaperMod，整體設計算令人滿意，只不過在Deploy Hugo遇到蠻多麻煩，這邊簡單記錄一下\n設定Github Page Action 參考PaperMod ExampleSite的gh-pages.yml設定，自己再作一些修改，大致如下\nname: Build GH-Pages on: push: paths-ignore: - 'images/**' - 'LICENSE' - 'README.md' branches: - master workflow_dispatch: # manual run jobs: deploy: runs-on: ubuntu-latest steps: - name: Git checkout uses: actions/checkout@v2 with: ref: master - name: Get Theme run: git submodule update --init --recursive - name: Update theme to Latest commit run: git submodule update --remote --merge - name: Setup hugo uses: peaceiris/actions-hugo@v2 with: hugo-version: 'latest' - name: Build run: hugo --buildDrafts --gc --verbose --minify - name: Deploy uses: peaceiris/actions-gh-pages@v3 with: github_token: ${{ secrets.","title":"部落格搬家記"}]