<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>statistics on Golden Hat</title><link>https://frankccccc.github.io/blog/categories/statistics/</link><description>Recent content in statistics on Golden Hat</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Fri, 09 Jul 2021 19:37:43 +0800</lastBuildDate><atom:link href="https://frankccccc.github.io/blog/categories/statistics/index.xml" rel="self" type="application/rss+xml"/><item><title>From EM To VBEM</title><link>https://frankccccc.github.io/blog/posts/from_em_to_vbem/</link><pubDate>Fri, 09 Jul 2021 18:27:01 +0800</pubDate><guid>https://frankccccc.github.io/blog/posts/from_em_to_vbem/</guid><description>1. Introduction When we use K-Means or GMM to solve clustering problem, the most important hyperparameter is the number of the cluster. It is quite hard to decide and cause the good/bad performance significantly. In the mean time, K-Means also cannot handle unbalanced dataset well. However, the variational Bayesian Gaussian mixture model(VB-GMM) can solve these. VB-GMM is a Bayesian model that contains priors over the parameters of GMM. Thus, VB-GMM can be optimized by variational Bayesian expectation maximization(VBEM) and find the optimal cluster number automatically.</description></item><item><title>Toward VB-GMM</title><link>https://frankccccc.github.io/blog/posts/toward_vbgmm/</link><pubDate>Fri, 09 Jul 2021 19:37:43 +0800</pubDate><guid>https://frankccccc.github.io/blog/posts/toward_vbgmm/</guid><description>3. Variational Bayesian Gaussian Mixture Model(VB-GMM) 3.1 Graphical Model Gaussian Mixture Model &amp;amp; Clustering
The variational Bayesian Gaussian mixture model(VB-GMM) can be represented as the above graphical model. We see each data point as a Gaussian mixture distribution with $K$ components. We also denote the number of data points as $N$. Each $x_n$ is a Gaussian mixture distribution with a weight $\pi_n$ corresponds to a data point. $z_n$ is an one-hot latent variable that indicates which cluster(component) does the data point belongs to.</description></item><item><title>A Guide Of Variational Lower Bound</title><link>https://frankccccc.github.io/blog/posts/a_guide_of_variational_lower_bound/</link><pubDate>Tue, 23 Feb 2021 12:39:16 +0800</pubDate><guid>https://frankccccc.github.io/blog/posts/a_guide_of_variational_lower_bound/</guid><description>Problem Setup The Variational Lower Bound is also knowd as Evidence Lower Bound(ELBO) or VLB. It is quite useful that we can derive a lower bound of a model containing a hidden variable. Futhermore, we can even maximize the bound to maximize the log probability. We can assume that $X$ are observations (data) and $Z$ are hidden/latent variables which is unobservable. In general, we can also imagine $Z$ as a parameter and the relationship between $Z$ and $X$ are represented as the following</description></item><item><title>Some Intuition Of MLE, MAP, and Bayesian Estimation</title><link>https://frankccccc.github.io/blog/posts/some_intuition_of_mle_map_and_bayesian_estimation/</link><pubDate>Fri, 19 Feb 2021 11:15:15 +0800</pubDate><guid>https://frankccccc.github.io/blog/posts/some_intuition_of_mle_map_and_bayesian_estimation/</guid><description>The main different between 3 kinds of estimation is What do we assume for the prior? The Maximum Likelihood Estimation(MLE) doesn&amp;rsquo;t use any prior but only maiximize the probability according to the samples. On the other hand, MAP and Bayesian both use priors to estimate the probability. The Maximum A Posteriori(MAP) only use the probability of single event while Bayesian Estimation see a distribution as the prior.
To be continue&amp;hellip;</description></item></channel></rss>