<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>machine learning on Golden Hat</title><link>https://frankccccc.github.io/blog/categories/machine-learning/</link><description>Recent content in machine learning on Golden Hat</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Fri, 19 Feb 2021 20:46:29 +0800</lastBuildDate><atom:link href="https://frankccccc.github.io/blog/categories/machine-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>Toward NNGP and NTK</title><link>https://frankccccc.github.io/blog/posts/toward_nngp_and_ntk/</link><pubDate>Fri, 19 Feb 2021 20:46:29 +0800</pubDate><guid>https://frankccccc.github.io/blog/posts/toward_nngp_and_ntk/</guid><description>Neural Network Gaussian Process(NNGP) Model the neural network as GP, aka neural network Gaussian Process(NNGP). Intuitively, the kernel of NNGP compute the distance between the output vectors of 2 input data points.
We define the following functions as neural networks with fully-conntected layers:
$$z_{i}^{1}(x) = b_i^{1} + \Sigma_{j=1}^{N_1} \ W_{ij}^{1}x_j^1(x), \ \ x_{j}^{1}(x) = \phi(b_i^{0} + \Sigma_{k=1}^{d_{in}} \ W_{ik}^{0}x_k(x))$$
where $b_i^{1}$ is the $i$th-bias of the second layer(the same as first hidden layer), $W_{ij}^{1}$ is the $i$th-weights of the first layer(the same as input layer) , function $\phi$ is the activation function, and $x$ is the input data of the neural network.</description></item><item><title>An Insight Into MLE, MAP, and Bayesian Estimation</title><link>https://frankccccc.github.io/blog/posts/mle_map_bayes/</link><pubDate>Fri, 19 Feb 2021 11:15:15 +0800</pubDate><guid>https://frankccccc.github.io/blog/posts/mle_map_bayes/</guid><description>The main different between 3 kinds of estimation is What do we assume for the prior? The Maximum Likelihood Estimation(MLE) doesn&amp;rsquo;t use any prior but only maiximize the probability according to the samples. On the other hand, MAP and Bayesian both use priors to estimate the probability. The Maximum A Posteriori(MAP) only use the probability of single event while Bayesian Estimation see a distribution as the prior.
To be continue&amp;hellip;</description></item><item><title>A Very Brief Introduction to Gaussian Process and Bayesian Optimization</title><link>https://frankccccc.github.io/blog/posts/intro_gp_bayes_opt/</link><pubDate>Tue, 16 Feb 2021 17:28:58 +0800</pubDate><guid>https://frankccccc.github.io/blog/posts/intro_gp_bayes_opt/</guid><description>Gaussian Process Big Picture and Background Intuitively, Gaussian distribution define the state space, while Gaussian Process define the function space
Before we introduce Gaussian process, we should understand Gaussian distriution at first. For a RV(random variable) $X$ that follow Gaussian Distribution $\mathcal{N}(0, 1)$ should be following image:
The P.D.F should be
$$x \sim \mathcal{N}(\mu, \sigma) = \frac{1}{\sigma \sqrt{2 \pi}} e^{- \frac{1}{2} (\frac{- \mu}{\sigma})^2}$$
As for Multivariate Gaussian Distribution, given 2 RV $x$, $y$ both 2 RV follow Gaussian Distribution $\mathcal{N}(0, 1)$ we can illustrate it as</description></item></channel></rss>