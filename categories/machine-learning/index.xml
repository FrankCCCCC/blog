<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>machine learning on Golden Hat</title><link>https://frankccccc.github.io/blog/categories/machine-learning/</link><description>Recent content in machine learning on Golden Hat</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Fri, 19 Feb 2021 20:46:29 +0800</lastBuildDate><atom:link href="https://frankccccc.github.io/blog/categories/machine-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>Toward NNGP and NTK</title><link>https://frankccccc.github.io/blog/posts/nngp_ntk/</link><pubDate>Fri, 19 Feb 2021 20:46:29 +0800</pubDate><guid>https://frankccccc.github.io/blog/posts/nngp_ntk/</guid><description>Neural Network Gaussian Process(NNGP) Neural Tangent Kernel(NTK) &amp;ldquo;In short, NTK represent the changes of the weights before and after the gradient descent update&amp;rdquo; Let&amp;rsquo;s start the journey of revealing the black-box neural networks.
Setup a Neural Network First of all, we need to define a simple neural network with 2 hidden layers
$$ y(x, w)$$
where $y$ is the neural network with weights $w \in \mathbb{R}^m$ and, $\{ x, \bar{y} \}$ is the dataset which is a set of the input data and the output data with $N$ data points.</description></item><item><title>An Insight Into MLE, MAP, and Bayesian Estimation</title><link>https://frankccccc.github.io/blog/posts/mle_map_bayes/</link><pubDate>Fri, 19 Feb 2021 11:15:15 +0800</pubDate><guid>https://frankccccc.github.io/blog/posts/mle_map_bayes/</guid><description>The main different between 3 kinds of estimation is What do we assume for the prior? The Maximum Likelihood Estimation(MLE) doesn&amp;rsquo;t use any prior but only maiximize the probability according to the samples. On the other hand, MAP and Bayesian both use priors to estimate the probability. The Maximum A Posteriori(MAP) only use the probability of single event while Bayesian Estimation see a distribution as the prior.
To be continue&amp;hellip;</description></item><item><title>A Very Brief Introduction to Gaussian Process and Bayesian Optimization</title><link>https://frankccccc.github.io/blog/posts/intro_gp_bayes_opt/</link><pubDate>Tue, 16 Feb 2021 17:28:58 +0800</pubDate><guid>https://frankccccc.github.io/blog/posts/intro_gp_bayes_opt/</guid><description>Gaussian Process Big Picture and Background Intuitively, Gaussian distribution define the state space, while Gaussian Process define the function space
Before we introduce Gaussian process, we should understand Gaussian distriution at first. For a RV(random variable) $X$ that follow Gaussian Distribution $\mathcal{N}(0, 1)$ should be following image:
The P.D.F should be
$$x \sim \mathcal{N}(\mu, \sigma) = \frac{1}{\sigma \sqrt{2 \pi}} e^{- \frac{1}{2} (\frac{- \mu}{\sigma})^2}$$
As for Multivariate Gaussian Distribution, given 2 RV $x$, $y$ both 2 RV follow Gaussian Distribution $\mathcal{N}(0, 1)$ we can illustrate it as</description></item></channel></rss>