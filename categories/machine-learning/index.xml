<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>machine learning on Golden Hat</title><link>https://frankccccc.github.io/blog/categories/machine-learning/</link><description>Recent content in machine learning on Golden Hat</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Thu, 08 Jul 2021 12:39:16 +0800</lastBuildDate><atom:link href="https://frankccccc.github.io/blog/categories/machine-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>A Review of SVM and SMO</title><link>https://frankccccc.github.io/blog/posts/smo/</link><pubDate>Thu, 08 Jul 2021 12:39:16 +0800</pubDate><guid>https://frankccccc.github.io/blog/posts/smo/</guid><description>Note: full code is on my github.
1. Abstract In this article, I will derive SMO algorithm and the Fourier kernel approximation which are well-known algorithm for kernel machine. SMO can solve optimization problem of SVM efficiently and the Fourier kernel approximation is a kind of kernel approximation that can speed up the computation of the kernel matrix. In the last section, I will conduct a evaluation of my manual SVM on the simulation dataset and &amp;ldquo;Women&amp;rsquo;s Clothing E-Commerce Review Dataset&amp;rdquo;.</description></item><item><title>Part II - Toward NNGP and NTK</title><link>https://frankccccc.github.io/blog/posts/toward_nngp_and_ntk/</link><pubDate>Fri, 19 Feb 2021 20:46:29 +0800</pubDate><guid>https://frankccccc.github.io/blog/posts/toward_nngp_and_ntk/</guid><description>Neural Tangent Kernel(NTK)
&amp;ldquo;In short, NTK represent the changes of the weights before and after the gradient descent update&amp;rdquo; Let&amp;rsquo;s start the journey of revealing the black-box neural networks.
Setup a Neural Network First of all, we need to define a simple neural network with 2 hidden layers
$$ y(x, w)$$
where $y$ is the neural network with weights $w \in \mathbb{R}^m$ and, ${ x, \bar{y} }_N$ is the dataset which is a set of the input data and the output data with $N$ data points.</description></item><item><title>A Very Brief Introduction to Gaussian Process and Bayesian Optimization</title><link>https://frankccccc.github.io/blog/posts/intro_gp_bayes_opt/</link><pubDate>Tue, 16 Feb 2021 17:28:58 +0800</pubDate><guid>https://frankccccc.github.io/blog/posts/intro_gp_bayes_opt/</guid><description>Gaussian Process Big Picture and Background Intuitively, Gaussian distribution define the state space, while Gaussian Process define the function space
Before we introduce Gaussian process, we should understand Gaussian distriution at first. For a RV(random variable) $X$ that follow Gaussian Distribution $\mathcal{N}(0, 1)$ should be following image:
The P.D.F should be
$$x \sim \mathcal{N}(\mu, \sigma) = \frac{1}{\sigma \sqrt{2 \pi}} e^{- \frac{1}{2} (\frac{- \mu}{\sigma})^2}$$
As for Multivariate Gaussian Distribution, given 2 RV $x$, $y$ both 2 RV follow Gaussian Distribution $\mathcal{N}(0, 1)$ we can illustrate it as</description></item><item><title>Part I - Toward NNGP and NTK</title><link>https://frankccccc.github.io/blog/posts/part_i_toward_nngp_and_ntk/</link><pubDate>Mon, 15 Mar 2021 23:34:57 +0800</pubDate><guid>https://frankccccc.github.io/blog/posts/part_i_toward_nngp_and_ntk/</guid><description>Neural Network Gaussian Process(NNGP) Model the neural network as GP, aka neural network Gaussian Process(NNGP). Intuitively, the kernel of NNGP compute the distance between the output vectors of 2 input data points.
We define the following functions as neural networks with fully-conntected layers:
$$z_{i}^{1}(x) = b_i^{1} + \sum_{j=1}^{N_1} \ W_{ij}^{1}x_j^1(x), \ \ x_{j}^{1}(x) = \phi(b_i^{0} + \sum_{k=1}^{d_{in}} \ W_{ik}^{0}x_k(x))$$
where $b_i^{1}$ is the $i$th-bias of the second layer(the same as first hidden layer), $W_{ij}^{1}$ is the $i$th-weights of the first layer(the same as input layer) , function $\phi$ is the activation function, and $x$ is the input data of the neural network.</description></item><item><title>Some Intuition Of MLE, MAP, and Bayesian Estimation</title><link>https://frankccccc.github.io/blog/posts/some_intuition_of_mle_map_and_bayesian_estimation/</link><pubDate>Fri, 19 Feb 2021 11:15:15 +0800</pubDate><guid>https://frankccccc.github.io/blog/posts/some_intuition_of_mle_map_and_bayesian_estimation/</guid><description>The main different between 3 kinds of estimation is What do we assume for the prior? The Maximum Likelihood Estimation(MLE) doesn&amp;rsquo;t use any prior but only maiximize the probability according to the samples. On the other hand, MAP and Bayesian both use priors to estimate the probability. The Maximum A Posteriori(MAP) only use the probability of single event while Bayesian Estimation see a distribution as the prior.
To be continue&amp;hellip;</description></item><item><title>An Introduction to Multi-Armed Bandit Problem</title><link>https://frankccccc.github.io/blog/posts/bandit/</link><pubDate>Tue, 16 Feb 2021 20:11:41 +0800</pubDate><guid>https://frankccccc.github.io/blog/posts/bandit/</guid><description>Multi-Armed Bandit Problem Imagine you are in a casionoand face multiple slot machines. Each machine is configured with an unknown probability of how likely you would get a reward at one play. The question is What&amp;rsquo;s the strategy to get the highest long-term reward?
An illustration of multi-armed bandit problem, refer to Lil&amp;rsquo;Log The Multi-Armed Bandit Problem and Its Solutions
Definition Upper Confidence Bounds(UCB) The UCB algorithm give a realtion between upper bound and probability confidence.</description></item></channel></rss>