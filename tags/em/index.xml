<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>EM on Golden Hat</title><link>https://frankccccc.github.io/blog/tags/em/</link><description>Recent content in EM on Golden Hat</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Fri, 09 Jul 2021 18:27:01 +0800</lastBuildDate><atom:link href="https://frankccccc.github.io/blog/tags/em/index.xml" rel="self" type="application/rss+xml"/><item><title>From EM To VBEM</title><link>https://frankccccc.github.io/blog/posts/from_em_to_vbem/</link><pubDate>Fri, 09 Jul 2021 18:27:01 +0800</pubDate><guid>https://frankccccc.github.io/blog/posts/from_em_to_vbem/</guid><description>1. Introduction When we use K-Means or GMM to solve clustering problem, the most important hyperparameter is the number of the cluster. It is quite hard to decide and cause the good/bad performance significantly. In the mean time, K-Means also cannot handle unbalanced dataset well. However, the variational Bayesian Gaussian mixture model(VB-GMM) can solve these. VB-GMM is a Bayesian model that contains priors over the parameters of GMM. Thus, VB-GMM can be optimized by variational Bayesian expectation maximization(VBEM) and find the optimal cluster number automatically.</description></item></channel></rss>