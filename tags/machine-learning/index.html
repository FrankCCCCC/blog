<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>machine learning | Golden Hat</title><meta name=keywords content><meta name=description content="Machine Learning, Programming and, Murmuring"><meta name=author content="SY Chou"><link rel=canonical href=https://frankccccc.github.io/blog/tags/machine-learning/><script async src="https://www.googletagmanager.com/gtag/js?id=G-S9MCZ2NDS7"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','G-S9MCZ2NDS7')</script><link href=/blog/assets/css/stylesheet.min.3768d020b7a9fd49f21ddb7d521ce9b187ce47c2929b883041396159d4d25553.css integrity="sha256-N2jQILep/UnyHdt9UhzpsYfOR8KSm4gwQTlhWdTSVVM=" rel="preload stylesheet" as=style><link rel=icon href=https://frankccccc.github.io/blog/img/just_imgs/gold_empty_circle.svg><link rel=icon type=image/png sizes=16x16 href=https://frankccccc.github.io/blog/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://frankccccc.github.io/blog/favicon-32x32.png><link rel=apple-touch-icon href=https://frankccccc.github.io/blog/apple-touch-icon.png><link rel=mask-icon href=https://frankccccc.github.io/blog/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><meta name=generator content="Hugo 0.85.0"><link rel=alternate type=application/rss+xml href=https://frankccccc.github.io/blog/tags/machine-learning/index.xml><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css integrity=sha384-Um5gpz1odJg5Z4HAmzPtgZKdTBHZdw8S29IecapCSB31ligYPhHQZMIlWLYQGVoc crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.js integrity=sha384-YNHdsYkH6gMx9y3mRkmcJ2mFUjTd0qNQQvY9VYZgQd7DcN7env35GzlmFaZ23JGp crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/contrib/auto-render.min.js integrity=sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:!0},{left:'$',right:'$',display:!1},{left:'\\(',right:'\\)',display:!1},{left:'\\[',right:'\\]',display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-S9MCZ2NDS7"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','G-S9MCZ2NDS7')</script><meta property="og:title" content="machine learning"><meta property="og:description" content="Machine Learning, Programming and, Murmuring"><meta property="og:type" content="website"><meta property="og:url" content="https://frankccccc.github.io/blog/tags/machine-learning/"><meta property="og:updated_time" content="2021-07-08T12:39:16+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="machine learning"><meta name=twitter:description content="Machine Learning, Programming and, Murmuring"></head><body class="list dark" id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove('dark')</script><noscript><style type=text/css>#theme-toggle,.top-link{display:none}</style></noscript><header class=header><nav class=nav><div class=logo><a href=https://frankccccc.github.io/blog/ accesskey=h title="Golden Hat (Alt + H)"><img src=/blog/img/just_imgs/gold_empty_circle.svg alt=logo aria-label=logo height=35>Golden Hat</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu onscroll=menu_on_scroll()><li><a href=https://frankccccc.github.io/blog/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://frankccccc.github.io/blog/series title=Series><span>Series</span></a></li><li><a href=https://frankccccc.github.io/blog/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://frankccccc.github.io/blog/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><header class=page-header><div class=breadcrumbs><a href=https://frankccccc.github.io/blog/>Home</a>&nbsp;»&nbsp;<a href=https://frankccccc.github.io/blog/tags/>Tags</a></div><h1>machine learning</h1></header><article class="post-entry tag-entry"><figure class=entry-cover><img src=https://frankccccc.github.io/blog/img/just_imgs/snow_forest.jpg alt></figure><header class=entry-header><h2>A Review of SVM and SMO</h2></header><section class=entry-content><p>Note: full code is on my github.
1. Abstract In this article, I will derive SMO algorithm and the Fourier kernel approximation which are well-known algorithm for kernel machine. SMO can solve optimization problem of SVM efficiently and the Fourier kernel approximation is a kind of kernel approximation that can speed up the computation of the kernel matrix. In the last section, I will conduct a evaluation of my manual SVM on the simulation dataset and “Women’s Clothing E-Commerce Review Dataset”....</p></section><footer class=entry-footer>July 8, 2021&nbsp;·&nbsp;15 min&nbsp;·&nbsp;SY Chou</footer><a class=entry-link aria-label="post link to A Review of SVM and SMO" href=https://frankccccc.github.io/blog/posts/smo/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img src=https://frankccccc.github.io/blog/img/just_imgs/wave_process.jpg alt></figure><header class=entry-header><h2>Part II - Toward NNGP and NTK</h2></header><section class=entry-content><p>Neural Tangent Kernel(NTK)
“In short, NTK represent the changes of the weights before and after the gradient descent update” Let’s start the journey of revealing the black-box neural networks.
Setup a Neural Network First of all, we need to define a simple neural network with 2 hidden layers
$$ y(x, w)$$
where $y$ is the neural network with weights $w \in \mathbb{R}^m$ and, ${ x, \bar{y} }_N$ is the dataset which is a set of the input data and the output data with $N$ data points....</p></section><footer class=entry-footer>February 19, 2021&nbsp;·&nbsp;10 min&nbsp;·&nbsp;SY Chou</footer><a class=entry-link aria-label="post link to Part II - Toward NNGP and NTK" href=https://frankccccc.github.io/blog/posts/toward_nngp_and_ntk/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img src=https://frankccccc.github.io/blog/img/just_imgs/ny_skyline.jpg alt></figure><header class=entry-header><h2>A Very Brief Introduction to Gaussian Process and Bayesian Optimization</h2></header><section class=entry-content><p>Gaussian Process Big Picture and Background Intuitively, Gaussian distribution define the state space, while Gaussian Process define the function space
Before we introduce Gaussian process, we should understand Gaussian distriution at first. For a RV(random variable) $X$ that follow Gaussian Distribution $\mathcal{N}(0, 1)$ should be following image:
The P.D.F should be
$$x \sim \mathcal{N}(\mu, \sigma) = \frac{1}{\sigma \sqrt{2 \pi}} e^{- \frac{1}{2} (\frac{- \mu}{\sigma})^2}$$
As for Multivariate Gaussian Distribution, given 2 RV $x$, $y$ both 2 RV follow Gaussian Distribution $\mathcal{N}(0, 1)$ we can illustrate it as...</p></section><footer class=entry-footer>February 16, 2021&nbsp;·&nbsp;12 min&nbsp;·&nbsp;SY Chou</footer><a class=entry-link aria-label="post link to A Very Brief Introduction to Gaussian Process and Bayesian Optimization" href=https://frankccccc.github.io/blog/posts/intro_gp_bayes_opt/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img src=https://frankccccc.github.io/blog/img/just_imgs/flow_sea.jpg alt></figure><header class=entry-header><h2>Part I - Toward NNGP and NTK<div class=entry-isdraft><sup>&nbsp;&nbsp;[draft]</sup></div></h2></header><section class=entry-content><p>Neural Network Gaussian Process(NNGP) Model the neural network as GP, aka neural network Gaussian Process(NNGP). Intuitively, the kernel of NNGP compute the distance between the output vectors of 2 input data points.
We define the following functions as neural networks with fully-conntected layers:
$$z_{i}^{1}(x) = b_i^{1} + \sum_{j=1}^{N_1} \ W_{ij}^{1}x_j^1(x), \ \ x_{j}^{1}(x) = \phi(b_i^{0} + \sum_{k=1}^{d_{in}} \ W_{ik}^{0}x_k(x))$$
where $b_i^{1}$ is the $i$th-bias of the second layer(the same as first hidden layer), $W_{ij}^{1}$ is the $i$th-weights of the first layer(the same as input layer) , function $\phi$ is the activation function, and $x$ is the input data of the neural network....</p></section><footer class=entry-footer>March 15, 2021&nbsp;·&nbsp;1 min&nbsp;·&nbsp;SY Chou</footer><a class=entry-link aria-label="post link to Part I - Toward NNGP and NTK" href=https://frankccccc.github.io/blog/posts/part_i_toward_nngp_and_ntk/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img src=https://frankccccc.github.io/blog/img/just_imgs/dice.jpg alt></figure><header class=entry-header><h2>Some Intuition Of MLE, MAP, and Bayesian Estimation<div class=entry-isdraft><sup>&nbsp;&nbsp;[draft]</sup></div></h2></header><section class=entry-content><p>The main different between 3 kinds of estimation is What do we assume for the prior? The Maximum Likelihood Estimation(MLE) doesn’t use any prior but only maiximize the probability according to the samples. On the other hand, MAP and Bayesian both use priors to estimate the probability. The Maximum A Posteriori(MAP) only use the probability of single event while Bayesian Estimation see a distribution as the prior.
To be continue…...</p></section><footer class=entry-footer>February 19, 2021&nbsp;·&nbsp;1 min&nbsp;·&nbsp;SY Chou</footer><a class=entry-link aria-label="post link to Some Intuition Of MLE, MAP, and Bayesian Estimation" href=https://frankccccc.github.io/blog/posts/some_intuition_of_mle_map_and_bayesian_estimation/></a></article><article class="post-entry tag-entry"><figure class=entry-cover><img src=https://frankccccc.github.io/blog/img/just_imgs/bandit.jpg alt></figure><header class=entry-header><h2>An Introduction to Multi-Armed Bandit Problem<div class=entry-isdraft><sup>&nbsp;&nbsp;[draft]</sup></div></h2></header><section class=entry-content><p>Multi-Armed Bandit Problem Imagine you are in a casionoand face multiple slot machines. Each machine is configured with an unknown probability of how likely you would get a reward at one play. The question is What’s the strategy to get the highest long-term reward?
An illustration of multi-armed bandit problem, refer to Lil’Log The Multi-Armed Bandit Problem and Its Solutions
Definition Upper Confidence Bounds(UCB) The UCB algorithm give a realtion between upper bound and probability confidence....</p></section><footer class=entry-footer>February 16, 2021&nbsp;·&nbsp;2 min&nbsp;·&nbsp;SY Chou</footer><a class=entry-link aria-label="post link to An Introduction to Multi-Armed Bandit Problem" href=https://frankccccc.github.io/blog/posts/bandit/></a></article></main><footer class=footer><span>&copy; 2021 <a href=https://frankccccc.github.io/blog/>Golden Hat</a></span>
<span>&#183;</span>
<span>Powered by <a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span>
<span>&#183;</span>
<span>Theme <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)"><button class=top-link id=top-link type=button accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></button></a>
<script>window.onload=function(){localStorage.getItem("menu-scroll-position")&&(document.getElementById('menu').scrollLeft=localStorage.getItem("menu-scroll-position"))},document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})});var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")};function menu_on_scroll(){localStorage.setItem("menu-scroll-position",document.getElementById('menu').scrollLeft)}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script></body></html>