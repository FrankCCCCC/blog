<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Golden Hat</title><meta name=keywords content><meta name=description content><meta name=author content="SY Chou"><link rel=canonical href=https://frankccccc.github.io/blog/><script async src="https://www.googletagmanager.com/gtag/js?id=G-S9MCZ2NDS7"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','G-S9MCZ2NDS7')</script><link href=/blog/assets/css/stylesheet.min.3768d020b7a9fd49f21ddb7d521ce9b187ce47c2929b883041396159d4d25553.css integrity="sha256-N2jQILep/UnyHdt9UhzpsYfOR8KSm4gwQTlhWdTSVVM=" rel="preload stylesheet" as=style><link rel=icon href=https://frankccccc.github.io/blog/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://frankccccc.github.io/blog/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://frankccccc.github.io/blog/favicon-32x32.png><link rel=apple-touch-icon href=https://frankccccc.github.io/blog/apple-touch-icon.png><link rel=mask-icon href=https://frankccccc.github.io/blog/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><meta name=generator content="Hugo 0.81.0"><link rel=alternate type=application/rss+xml href=https://frankccccc.github.io/blog/index.xml><link rel=alternate type=application/json href=https://frankccccc.github.io/blog/index.json><script async src="https://www.googletagmanager.com/gtag/js?id=G-S9MCZ2NDS7"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','G-S9MCZ2NDS7')</script><meta property="og:title" content="Golden Hat"><meta property="og:description" content><meta property="og:type" content="website"><meta property="og:url" content="https://frankccccc.github.io/blog/"><meta property="og:updated_time" content="2021-03-04T17:02:31+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Golden Hat"><meta name=twitter:description content><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","name":"Golden Hat","url":"https://frankccccc.github.io/blog/","description":"","thumbnailUrl":"https://frankccccc.github.io/blog/favicon.ico","sameAs":["mailto:quantumplanz@outlook.com","https://github.com/FrankCCCCC","https://www.linkedin.com/in/%E8%81%96%E8%AB%BA-%E5%91%A8-ba89b2184/"]}</script></head><body class="list dark" id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove('dark')</script><noscript><style type=text/css>#theme-toggle,.top-link{display:none}</style></noscript><header class=header><nav class=nav><div class=logo><a href=https://frankccccc.github.io/blog/ accesskey=h title="Golden Hat (Alt + H)"><img src=/blog/img/just_imgs/gold_empty_circle.svg alt=logo aria-label=logo height=35>Golden Hat</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu onscroll=menu_on_scroll()><li><a href=https://frankccccc.github.io/blog/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://frankccccc.github.io/blog/series title=Series><span>Series</span></a></li><li><a href=https://frankccccc.github.io/blog/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://frankccccc.github.io/blog/tags/ title=Tags><span>Tags</span></a></li></ul></nav><script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$'],['\\[','\\]']],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:['script','noscript','style','textarea','pre']}},window.addEventListener('load',a=>{document.querySelectorAll("mjx-container").forEach(function(a){a.parentElement.classList+='has-jax'})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></header><main class=main><article class="first-entry home-info"><header class=entry-header><h1>SY Chou&rsquo;s Blog</h1></header><section class=entry-content><p>Then wear the gold hat, if that will move her; If you can bounce high, bounce for her too, Till she cry &ldquo;Lover, gold-hatted, high-bouncing lover, I must have you!&rdquo; &#171;The Great Gatsby&#187;</p></section><footer class=entry-footer><div class=social-icons><a href=mailto:quantumplanz@outlook.com target=_blank rel="noopener noreferrer me" title=Email><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M4 4h16c1.1.0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1.0-2-.9-2-2V6c0-1.1.9-2 2-2z"/><polyline points="22,6 12,13 2,6"/></svg></a><a href=https://github.com/FrankCCCCC target=_blank rel="noopener noreferrer me" title=Github><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg></a><a href=https://www.linkedin.com/in/%E8%81%96%E8%AB%BA-%E5%91%A8-ba89b2184/ target=_blank rel="noopener noreferrer me" title=Linkedin><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M16 8a6 6 0 016 6v7h-4v-7a2 2 0 00-2-2 2 2 0 00-2 2v7h-4v-7a6 6 0 016-6z"/><rect x="2" y="9" width="4" height="12"/><circle cx="4" cy="4" r="2"/></svg></a></div></footer></article><article class=post-entry><figure class=entry-cover><img src=https://frankccccc.github.io/blog/img/just_imgs/muzero.webp alt></figure><header class=entry-header><h2>Part III - From AlphaGo to MuZero<div class=entry-isdraft><sup>&nbsp;&nbsp;[draft]</sup></div></h2></header><section class=entry-content><p>Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model It is just the paper proposing MuZero. MuZero is quite famous when I write this note(Jan 2021). Lots of people tried to reproduce the incredible performance of the paper. Some of well-known implementations like muzero-general give a clear and modular implementation of MuZero. If you are interested in MuZero, you can play with it. Well, let’s diving into the paper....</p></section><footer class=entry-footer>March 4, 2021&nbsp;·&nbsp;4 min&nbsp;·&nbsp;SY Chou</footer><a class=entry-link aria-label="post link to Part III - From AlphaGo to MuZero" href=https://frankccccc.github.io/blog/posts/part_iii-from_alphago_to_muzero/></a></article><article class=post-entry><figure class=entry-cover><img src=https://frankccccc.github.io/blog/img/just_imgs/alphago_zero.webp alt></figure><header class=entry-header><h2>Part II - From AlphaGo to MuZero<div class=entry-isdraft><sup>&nbsp;&nbsp;[draft]</sup></div></h2></header><section class=entry-content><p>Mastering the game of Go without human knowledge The paper propose AlphaGo Zero which is known as self-playing without human knowledge.
Reinforcement learning in AlphaGo Zero $$ (p, v) = f_{\theta} $$
$$ l = (z - v)^2 - \pi^T log(p) + c||\theta||^2 $$
Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm The paper propose AlphaZero which is known as self-playing to compete any kinds of board game....</p></section><footer class=entry-footer>March 4, 2021&nbsp;·&nbsp;1 min&nbsp;·&nbsp;SY Chou</footer><a class=entry-link aria-label="post link to Part II - From AlphaGo to MuZero" href=https://frankccccc.github.io/blog/posts/part_ii-from_alphago_to_muzero/></a></article><article class=post-entry><figure class=entry-cover><img src=https://frankccccc.github.io/blog/img/just_imgs/inside_glacier.jpg alt></figure><header class=entry-header><h2>Simple Guide Of VDN And QMIX<div class=entry-isdraft><sup>&nbsp;&nbsp;[draft]</sup></div></h2></header><section class=entry-content><p>Value-Decomposition Network(VDN) QMIX Problem Setup And Assumption Constraint The QMIX imporve the VDN algorithm via give a more general form of the contraint. It defines the contraint like
$$\frac{\partial Q_{tot}}{\partial Q_{a}} \geq 0, \forall a$$
where $Q_{tot}$ is the joint value function and $Q_{a}$ is the value function for each agent.
An intuitive eplaination is that we want the weights of any individual value function $Q_{a}$ are positive. If the weights of individual value function $Q_{a}$ are negative, it will discourage the agent to cooperate, since the higher $Q_{a}$, the lower joint value $Q_{tot}$....</p></section><footer class=entry-footer>February 26, 2021&nbsp;·&nbsp;2 min&nbsp;·&nbsp;SY Chou</footer><a class=entry-link aria-label="post link to Simple Guide Of VDN And QMIX" href=https://frankccccc.github.io/blog/posts/simple_guide_of_vdn_and_qmix/></a></article><article class=post-entry><figure class=entry-cover><img src=https://frankccccc.github.io/blog/img/just_imgs/whirl_flow.jpg alt></figure><header class=entry-header><h2>A Guide Of Variational Lower Bound<div class=entry-isdraft><sup>&nbsp;&nbsp;[draft]</sup></div></h2></header><section class=entry-content><p>Problem Setup The Variational Lower Bound is also knowd as Evidence Lower Bound(ELBO) or VLB. It is quite useful that we can derive a lower bound of a model containing a hidden variable. Futhermore, we can even maximize the bound to maximize the log probability. We can assume that $X$ are observations (data) and $Z$ are hidden/latent variables which is unobservable. In general, we can also imagine $Z$ as a parameter and the relationship between $Z$ and $X$ are represented as the following...</p></section><footer class=entry-footer>February 23, 2021&nbsp;·&nbsp;4 min&nbsp;·&nbsp;SY Chou</footer><a class=entry-link aria-label="post link to A Guide Of Variational Lower Bound" href=https://frankccccc.github.io/blog/posts/a_guide_of_variational_lower_bound/></a></article><article class=post-entry><figure class=entry-cover><img src=https://frankccccc.github.io/blog/img/just_imgs/star_trail.jpg alt></figure><header class=entry-header><h2>A Set of Shannon Entropy</h2></header><section class=entry-content><p>Shannon Entropy For discrete random variable $X$ with events $\{ x_1, …, x_n \}$ and probability mass function $P(X)$, we defien the Shannon Entropy $H(X)$ as
$$H(X) = E[-log_b \ P(X)] = - \sum_{i = 1}^{i = n} \ P(x_i) log_b \ P(x_i)$$
where $b$ is the base of the logarithm. The unit of Shannon entropy is bit for $b = 2$ while nat for $b = e$
The Perspective of Venn Diagram We can illustrate the relation between joint entropy, conditional entropy, and mutual entropy as the following figure...</p></section><footer class=entry-footer>February 23, 2021&nbsp;·&nbsp;3 min&nbsp;·&nbsp;SY Chou</footer><a class=entry-link aria-label="post link to A Set of Shannon Entropy" href=https://frankccccc.github.io/blog/posts/a_set_of_shannon_entropy/></a></article><article class=post-entry><figure class=entry-cover><img src=https://frankccccc.github.io/blog/img/just_imgs/wave_process.jpg alt></figure><header class=entry-header><h2>Toward NNGP and NTK<div class=entry-isdraft><sup>&nbsp;&nbsp;[draft]</sup></div></h2></header><section class=entry-content><p>Neural Network Gaussian Process(NNGP) Model the neural network as GP, aka neural network Gaussian Process(NNGP). Intuitively, the kernel of NNGP compute the distance between the output vectors of 2 input data points.
We define the following functions as neural networks with fully-conntected layers:
$$z_{i}^{1}(x) = b_i^{1} + \sum_{j=1}^{N_1} \ W_{ij}^{1}x_j^1(x), \ \ x_{j}^{1}(x) = \phi(b_i^{0} + \sum_{k=1}^{d_{in}} \ W_{ik}^{0}x_k(x))$$
where $b_i^{1}$ is the $i$th-bias of the second layer(the same as first hidden layer), $W_{ij}^{1}$ is the $i$th-weights of the first layer(the same as input layer) , function $\phi$ is the activation function, and $x$ is the input data of the neural network....</p></section><footer class=entry-footer>February 19, 2021&nbsp;·&nbsp;10 min&nbsp;·&nbsp;SY Chou</footer><a class=entry-link aria-label="post link to Toward NNGP and NTK" href=https://frankccccc.github.io/blog/posts/toward_nngp_and_ntk/></a></article><article class=post-entry><figure class=entry-cover><img src=https://frankccccc.github.io/blog/img/just_imgs/dice.jpg alt></figure><header class=entry-header><h2>Some Intuition Of MLE, MAP, and Bayesian Estimation<div class=entry-isdraft><sup>&nbsp;&nbsp;[draft]</sup></div></h2></header><section class=entry-content><p>The main different between 3 kinds of estimation is What do we assume for the prior? The Maximum Likelihood Estimation(MLE) doesn’t use any prior but only maiximize the probability according to the samples. On the other hand, MAP and Bayesian both use priors to estimate the probability. The Maximum A Posteriori(MAP) only use the probability of single event while Bayesian Estimation see a distribution as the prior.
To be continue…...</p></section><footer class=entry-footer>February 19, 2021&nbsp;·&nbsp;1 min&nbsp;·&nbsp;SY Chou</footer><a class=entry-link aria-label="post link to Some Intuition Of MLE, MAP, and Bayesian Estimation" href=https://frankccccc.github.io/blog/posts/some_intuition_of_mle_map_and_bayesian_estimation/></a></article><article class=post-entry><figure class=entry-cover><img src=https://frankccccc.github.io/blog/img/just_imgs/alphago.webp alt></figure><header class=entry-header><h2>Part I - From AlphaGo to MuZero<div class=entry-isdraft><sup>&nbsp;&nbsp;[draft]</sup></div></h2></header><section class=entry-content><p>AlphaGo is quite famous when I was a freshman of college. It somehow is the reason that I was addicted to Reinforcement Learning. Thus Our journey of model-based RL will start here. Although it is not the first one that propose model-based RL, I still believe it will give a big picture of model-based RL.
Mastering the game of Go with deep neural networks and tree search Introduction AlphaGo combines 2 kinds of model, including policy network and value network....</p></section><footer class=entry-footer>February 19, 2021&nbsp;·&nbsp;5 min&nbsp;·&nbsp;SY Chou</footer><a class=entry-link aria-label="post link to Part I - From AlphaGo to MuZero" href=https://frankccccc.github.io/blog/posts/part_i-from_alphago_to_muzero/></a></article><article class=post-entry><figure class=entry-cover><img src=https://frankccccc.github.io/blog/img/just_imgs/distributional_rl_prism.webp alt></figure><header class=entry-header><h2>A Glimpse of Distributional RL<div class=entry-isdraft><sup>&nbsp;&nbsp;[draft]</sup></div></h2></header><section class=entry-content><p>...</p></section><footer class=entry-footer>February 16, 2021&nbsp;·&nbsp;0 min&nbsp;·&nbsp;SY Chou</footer><a class=entry-link aria-label="post link to A Glimpse of Distributional RL" href=https://frankccccc.github.io/blog/posts/a_glimpse_of_distributional_rl/></a></article><article class=post-entry><figure class=entry-cover><img src=https://frankccccc.github.io/blog/img/just_imgs/bandit.jpg alt></figure><header class=entry-header><h2>An Introduction to Multi-Armed Bandit Problem<div class=entry-isdraft><sup>&nbsp;&nbsp;[draft]</sup></div></h2></header><section class=entry-content><p>Multi-Armed Bandit Problem Imagine you are in a casionoand face multiple slot machines. Each machine is configured with an unknown probability of how likely you would get a reward at one play. The question is What’s the strategy to get the highest long-term reward?
An illustration of multi-armed bandit problem, refer to Lil’Log The Multi-Armed Bandit Problem and Its Solutions
Definition Upper Confidence Bounds(UCB) The UCB algorithm give a realtion between upper bound and probability confidence....</p></section><footer class=entry-footer>February 16, 2021&nbsp;·&nbsp;2 min&nbsp;·&nbsp;SY Chou</footer><a class=entry-link aria-label="post link to An Introduction to Multi-Armed Bandit Problem" href=https://frankccccc.github.io/blog/posts/bandit/></a></article><footer class=page-footer><nav class=pagination><a class=next href=https://frankccccc.github.io/blog/page/2/>Next Page »</a></nav></footer></main><footer class=footer><span>&copy; 2021 <a href=https://frankccccc.github.io/blog/>Golden Hat</a></span>
<span>&#183;</span>
<span>Powered by <a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span>
<span>&#183;</span>
<span>Theme <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)"><button class=top-link id=top-link type=button accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></button></a>
<script>window.onload=function(){localStorage.getItem("menu-scroll-position")&&(document.getElementById('menu').scrollLeft=localStorage.getItem("menu-scroll-position"))},document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})});var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")};function menu_on_scroll(){localStorage.setItem("menu-scroll-position",document.getElementById('menu').scrollLeft)}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script></body></html>