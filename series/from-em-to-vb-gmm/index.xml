<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>From EM To VB-GMM on Golden Hat</title><link>https://frankccccc.github.io/blog/series/from-em-to-vb-gmm/</link><description>Recent content in From EM To VB-GMM on Golden Hat</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Fri, 09 Jul 2021 19:37:43 +0800</lastBuildDate><atom:link href="https://frankccccc.github.io/blog/series/from-em-to-vb-gmm/index.xml" rel="self" type="application/rss+xml"/><item><title>From EM To VBEM</title><link>https://frankccccc.github.io/blog/posts/from_em_to_vbem/</link><pubDate>Fri, 09 Jul 2021 18:27:01 +0800</pubDate><guid>https://frankccccc.github.io/blog/posts/from_em_to_vbem/</guid><description>1. Introduction When we use K-Means or GMM to solve clustering problem, the most important hyperparameter is the number of the cluster. It is quite hard to decide and cause the good/bad performance significantly. In the mean time, K-Means also cannot handle unbalanced dataset well. However, the variational Bayesian Gaussian mixture model(VB-GMM) can solve these. VB-GMM is a Bayesian model that contains priors over the parameters of GMM. Thus, VB-GMM can be optimized by variational Bayesian expectation maximization(VBEM) and find the optimal cluster number automatically.</description></item><item><title>Toward VB-GMM</title><link>https://frankccccc.github.io/blog/posts/toward_vbgmm/</link><pubDate>Fri, 09 Jul 2021 19:37:43 +0800</pubDate><guid>https://frankccccc.github.io/blog/posts/toward_vbgmm/</guid><description>3. Variational Bayesian Gaussian Mixture Model(VB-GMM) 3.1 Graphical Model Gaussian Mixture Model &amp;amp; Clustering
The variational Bayesian Gaussian mixture model(VB-GMM) can be represented as the above graphical model. We see each data point as a Gaussian mixture distribution with $K$ components. We also denote the number of data points as $N$. Each $x_n$ is a Gaussian mixture distribution with a weight $\pi_n$ corresponds to a data point. $z_n$ is an one-hot latent variable that indicates which cluster(component) does the data point belongs to.</description></item></channel></rss>