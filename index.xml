<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Golden Hat</title><link>https://frankccccc.github.io/blog/</link><description>Recent content on Golden Hat</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Thu, 04 Mar 2021 17:02:31 +0800</lastBuildDate><atom:link href="https://frankccccc.github.io/blog/index.xml" rel="self" type="application/rss+xml"/><item><title>Part III - From AlphaGo to MuZero</title><link>https://frankccccc.github.io/blog/posts/part_iii-from_alphago_to_muzero/</link><pubDate>Thu, 04 Mar 2021 17:02:31 +0800</pubDate><guid>https://frankccccc.github.io/blog/posts/part_iii-from_alphago_to_muzero/</guid><description>Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model That is just the paper proposing MuZero. It is quite famous when I write this note(Jan 2021). Lots of people tried to reproduce the incredible performance of this paper. Some of well-known implementations like muzero-general give a clear code and modular structure of MuZero.
Introdution The main idea is to predict the future that are directly relevant for planning.</description></item><item><title>Part II - From AlphaGo to MuZero</title><link>https://frankccccc.github.io/blog/posts/part_ii-from_alphago_to_muzero/</link><pubDate>Thu, 04 Mar 2021 16:54:39 +0800</pubDate><guid>https://frankccccc.github.io/blog/posts/part_ii-from_alphago_to_muzero/</guid><description/></item><item><title>Simple Guide Of VDN And QMIX</title><link>https://frankccccc.github.io/blog/posts/simple_guide_of_vdn_and_qmix/</link><pubDate>Fri, 26 Feb 2021 01:13:41 +0800</pubDate><guid>https://frankccccc.github.io/blog/posts/simple_guide_of_vdn_and_qmix/</guid><description>Value-Decomposition Network(VDN) QMIX Problem Setup And Assumption Constraint The QMIX imporve the VDN algorithm via give a more general form of the contraint. It defines the contraint like
$$\frac{\partial Q_{tot}}{\partial Q_{a}} \geq 0, \forall a$$
where $Q_{tot}$ is the joint value function and $Q_{a}$ is the value function for each agent.
An intuitive eplaination is that we want the weights of any individual value function $Q_{a}$ are positive. If the weights of individual value function $Q_{a}$ are negative, it will discourage the agent to cooperate, since the higher $Q_{a}$, the lower joint value $Q_{tot}$.</description></item><item><title>A Guide Of Variational Lower Bound</title><link>https://frankccccc.github.io/blog/posts/a_guide_of_variational_lower_bound/</link><pubDate>Tue, 23 Feb 2021 12:39:16 +0800</pubDate><guid>https://frankccccc.github.io/blog/posts/a_guide_of_variational_lower_bound/</guid><description>Problem Setup It is also knowd as Evidence Lower Bound(ELBO) or VLB. We can assume that $X$ are observations (data) and $Z$ are hidden/latent variables. In general, we can also imagine $Z$ as a parameter and the relationship between $Z$ and $X$ are represented as the following
In the mean time, by the definition of Bayes' Theorem and conditional probability, we can get
$$p(Z | X) = \frac{p(X | Z) p(Z)}{p(X)} = \frac{p(X | Z) p(Z)}{\int_{Z} p(X, Z)}$$</description></item><item><title>A Set of Shannon Entropy</title><link>https://frankccccc.github.io/blog/posts/a_set_of_shannon_entropy/</link><pubDate>Tue, 23 Feb 2021 01:03:19 +0800</pubDate><guid>https://frankccccc.github.io/blog/posts/a_set_of_shannon_entropy/</guid><description>Shannon Entropy For discrete random variable $X$ with events $\{ x_1, &amp;hellip;, x_n \}$ and probability mass function $P(X)$, we defien the Shannon Entropy $H(X)$ as
$$H(X) = E[-log_b \ P(X)] = - \sum_{i = 1}^{i = n} \ P(x_i) log_b \ P(x_i)$$
where $b$ is the base of the logarithm. The unit of Shannon entropy is bit for $b = 2$ while nat for $b = e$
The Perspective of Venn Diagram We can illustrate the relation between joint entropy, conditional entropy, and mutual entropy as the following figure</description></item><item><title>Toward NNGP and NTK</title><link>https://frankccccc.github.io/blog/posts/toward_nngp_and_ntk/</link><pubDate>Fri, 19 Feb 2021 20:46:29 +0800</pubDate><guid>https://frankccccc.github.io/blog/posts/toward_nngp_and_ntk/</guid><description>Neural Network Gaussian Process(NNGP) Model the neural network as GP, aka neural network Gaussian Process(NNGP). Intuitively, the kernel of NNGP compute the distance between the output vectors of 2 input data points.
We define the following functions as neural networks with fully-conntected layers:
$$z_{i}^{1}(x) = b_i^{1} + \sum_{j=1}^{N_1} \ W_{ij}^{1}x_j^1(x), \ \ x_{j}^{1}(x) = \phi(b_i^{0} + \sum_{k=1}^{d_{in}} \ W_{ik}^{0}x_k(x))$$
where $b_i^{1}$ is the $i$th-bias of the second layer(the same as first hidden layer), $W_{ij}^{1}$ is the $i$th-weights of the first layer(the same as input layer) , function $\phi$ is the activation function, and $x$ is the input data of the neural network.</description></item><item><title>Some Intuition Of MLE, MAP, and Bayesian Estimation</title><link>https://frankccccc.github.io/blog/posts/some_intuition_of_mle_map_and_bayesian_estimation/</link><pubDate>Fri, 19 Feb 2021 11:15:15 +0800</pubDate><guid>https://frankccccc.github.io/blog/posts/some_intuition_of_mle_map_and_bayesian_estimation/</guid><description>The main different between 3 kinds of estimation is What do we assume for the prior? The Maximum Likelihood Estimation(MLE) doesn&amp;rsquo;t use any prior but only maiximize the probability according to the samples. On the other hand, MAP and Bayesian both use priors to estimate the probability. The Maximum A Posteriori(MAP) only use the probability of single event while Bayesian Estimation see a distribution as the prior.
To be continue&amp;hellip;</description></item><item><title>Part I - From AlphaGo to MuZero</title><link>https://frankccccc.github.io/blog/posts/part_i-from_alphago_to_muzero/</link><pubDate>Fri, 19 Feb 2021 01:14:40 +0800</pubDate><guid>https://frankccccc.github.io/blog/posts/part_i-from_alphago_to_muzero/</guid><description>AlphaGo is quite famous when I was a freshman of college. It somehow is the reason that I was addicted to Reinforcement Learning. Thus Our journey of model-based RL will start here. Although it is not the first one that propose model-based RL, I still believe it will give a big picture of model-based RL.
Mastering the game of Go with deep neural networks and tree search Introduction AlphaGo combines 2 kinds of model, including policy network and value network.</description></item><item><title>A Glimpse of Distributional RL</title><link>https://frankccccc.github.io/blog/posts/a_glimpse_of_distributional_rl/</link><pubDate>Tue, 16 Feb 2021 20:36:18 +0800</pubDate><guid>https://frankccccc.github.io/blog/posts/a_glimpse_of_distributional_rl/</guid><description/></item><item><title>An Introduction to Multi-Armed Bandit Problem</title><link>https://frankccccc.github.io/blog/posts/bandit/</link><pubDate>Tue, 16 Feb 2021 20:11:41 +0800</pubDate><guid>https://frankccccc.github.io/blog/posts/bandit/</guid><description>Multi-Armed Bandit Problem Imagine you are in a casionoand face multiple slot machines. Each machine is configured with an unknown probability of how likely you would get a reward at one play. The question is What&amp;rsquo;s the strategy to get the highest long-term reward?
An illustration of multi-armed bandit problem, refer to Lil&amp;rsquo;Log The Multi-Armed Bandit Problem and Its Solutions
Definition Upper Confidence Bounds(UCB) The UCB algorithm give a realtion between upper bound and probability confidence.</description></item><item><title>A Very Brief Introduction to Gaussian Process and Bayesian Optimization</title><link>https://frankccccc.github.io/blog/posts/intro_gp_bayes_opt/</link><pubDate>Tue, 16 Feb 2021 17:28:58 +0800</pubDate><guid>https://frankccccc.github.io/blog/posts/intro_gp_bayes_opt/</guid><description>Gaussian Process Big Picture and Background Intuitively, Gaussian distribution define the state space, while Gaussian Process define the function space
Before we introduce Gaussian process, we should understand Gaussian distriution at first. For a RV(random variable) $X$ that follow Gaussian Distribution $\mathcal{N}(0, 1)$ should be following image:
The P.D.F should be
$$x \sim \mathcal{N}(\mu, \sigma) = \frac{1}{\sigma \sqrt{2 \pi}} e^{- \frac{1}{2} (\frac{- \mu}{\sigma})^2}$$
As for Multivariate Gaussian Distribution, given 2 RV $x$, $y$ both 2 RV follow Gaussian Distribution $\mathcal{N}(0, 1)$ we can illustrate it as</description></item><item><title>部落格搬家記</title><link>https://frankccccc.github.io/blog/posts/move_blog/</link><pubDate>Tue, 16 Feb 2021 17:28:58 +0800</pubDate><guid>https://frankccccc.github.io/blog/posts/move_blog/</guid><description>因為寫DL筆記時會用到大量數學符號，就索性把原先在Github上的DL_DB_Quick_Notes搬過來了，配合LATEX寫筆記順手很多，原先的Repo應該只會剩下收集Paper用。而最近生活上有些轉折，也許也會順便放些隨筆雜記，但就依心情而定。
目前用的主題是PaperMod，整體設計算令人滿意，只不過在Deploy Hugo遇到蠻多麻煩，這邊簡單記錄一下
設定Github Page Action 參考PaperMod ExampleSite的gh-pages.yml設定，自己再作一些修改，大致如下
name: Build GH-Pages on: push: paths-ignore: - 'images/**' - 'LICENSE' - 'README.md' branches: - master workflow_dispatch: # manual run jobs: deploy: runs-on: ubuntu-latest steps: - name: Git checkout uses: actions/checkout@v2 with: ref: master - name: Get Theme run: git submodule update --init --recursive - name: Update theme to Latest commit run: git submodule update --remote --merge - name: Setup hugo uses: peaceiris/actions-hugo@v2 with: hugo-version: 'latest' - name: Build run: hugo --buildDrafts --gc --verbose --minify - name: Deploy uses: peaceiris/actions-gh-pages@v3 with: github_token: ${{ secrets.</description></item><item><title>Search</title><link>https://frankccccc.github.io/blog/search/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://frankccccc.github.io/blog/search/</guid><description>search</description></item></channel></rss>