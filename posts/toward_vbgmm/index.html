<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Toward VB-GMM | Golden Hat</title><meta name=keywords content="EM,machine learning,statistics,bayes"><meta name=description content="Note: the code in R is on my Github
3. Variational Bayesian Gaussian Mixture Model(VB-GMM) 3.1 Graphical Model Gaussian Mixture Model & Clustering
The variational Bayesian Gaussian mixture model(VB-GMM) can be represented as the above graphical model. We see each data point as a Gaussian mixture distribution with $K$ components. We also denote the number of data points as $N$. Each $x_n$ is a Gaussian mixture distribution with a weight $\pi_n$ corresponds to a data point."><meta name=author content="SY Chou"><link rel=canonical href=https://frankccccc.github.io/blog/posts/toward_vbgmm/><script async src="https://www.googletagmanager.com/gtag/js?id=G-S9MCZ2NDS7"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-S9MCZ2NDS7")</script><link href=/blog/assets/css/stylesheet.min.3768d020b7a9fd49f21ddb7d521ce9b187ce47c2929b883041396159d4d25553.css integrity="sha256-N2jQILep/UnyHdt9UhzpsYfOR8KSm4gwQTlhWdTSVVM=" rel="preload stylesheet" as=style><link rel=icon href=https://frankccccc.github.io/blog/img/just_imgs/gold_empty_circle.svg><link rel=icon type=image/png sizes=16x16 href=https://frankccccc.github.io/blog/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://frankccccc.github.io/blog/favicon-32x32.png><link rel=apple-touch-icon href=https://frankccccc.github.io/blog/apple-touch-icon.png><link rel=mask-icon href=https://frankccccc.github.io/blog/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><meta name=generator content="Hugo 0.104.3"><script async src="https://www.googletagmanager.com/gtag/js?id=G-S9MCZ2NDS7"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-S9MCZ2NDS7")</script><meta property="og:title" content="Toward VB-GMM"><meta property="og:description" content="Note: the code in R is on my Github
3. Variational Bayesian Gaussian Mixture Model(VB-GMM) 3.1 Graphical Model Gaussian Mixture Model & Clustering
The variational Bayesian Gaussian mixture model(VB-GMM) can be represented as the above graphical model. We see each data point as a Gaussian mixture distribution with $K$ components. We also denote the number of data points as $N$. Each $x_n$ is a Gaussian mixture distribution with a weight $\pi_n$ corresponds to a data point."><meta property="og:type" content="article"><meta property="og:url" content="https://frankccccc.github.io/blog/posts/toward_vbgmm/"><meta property="og:image" content="https://frankccccc.github.io/blog/img/just_imgs/underwater_ice.jpg"><meta property="article:published_time" content="2021-07-09T19:37:43+08:00"><meta property="article:modified_time" content="2021-07-09T19:37:43+08:00"><meta property="og:see_also" content="https://frankccccc.github.io/blog/posts/from_em_to_vbem/"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://frankccccc.github.io/blog/img/just_imgs/underwater_ice.jpg"><meta name=twitter:title content="Toward VB-GMM"><meta name=twitter:description content="Note: the code in R is on my Github
3. Variational Bayesian Gaussian Mixture Model(VB-GMM) 3.1 Graphical Model Gaussian Mixture Model & Clustering
The variational Bayesian Gaussian mixture model(VB-GMM) can be represented as the above graphical model. We see each data point as a Gaussian mixture distribution with $K$ components. We also denote the number of data points as $N$. Each $x_n$ is a Gaussian mixture distribution with a weight $\pi_n$ corresponds to a data point."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://frankccccc.github.io/blog/posts/"},{"@type":"ListItem","position":2,"name":"Toward VB-GMM","item":"https://frankccccc.github.io/blog/posts/toward_vbgmm/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Toward VB-GMM","name":"Toward VB-GMM","description":"Note: the code in R is on my Github\n3. Variational Bayesian Gaussian Mixture Model(VB-GMM) 3.1 Graphical Model Gaussian Mixture Model \u0026amp;amp; Clustering\nThe variational Bayesian …","keywords":["EM","machine learning","statistics","bayes"],"articleBody":"Note: the code in R is on my Github\n3. Variational Bayesian Gaussian Mixture Model(VB-GMM) 3.1 Graphical Model Gaussian Mixture Model \u0026 Clustering\nThe variational Bayesian Gaussian mixture model(VB-GMM) can be represented as the above graphical model. We see each data point as a Gaussian mixture distribution with $K$ components. We also denote the number of data points as $N$. Each $x_n$ is a Gaussian mixture distribution with a weight $\\pi_n$ corresponds to a data point. $z_n$ is an one-hot latent variable that indicates which cluster(component) does the data point belongs to. Finally, A component $k$ follows the Gaussian distribution with mean $\\mu_k$ and covariance matrix $\\Lambda_k$. $\\Lambda = { \\Lambda_1, …, \\Lambda_K }$ and $\\mu = { \\mu_1, …, \\mu_K }$ are vectors denote the parameters of Gaussian mixture distribution.\nThus, the joint distribution of the VB-GMM is\n$$ p(X, Z, \\pi, \\mu, \\Lambda) = p(X | Z, \\pi, \\mu, \\Lambda) p(Z | \\pi) p(\\pi) p(\\mu | \\Lambda) p(\\Lambda) $$\n$p(X | Z, \\pi, \\mu, \\Lambda)$ denotes the Gaussian mixture model given on the latent variables and parameters. $p(Z | \\pi)$ denotes the latent variables. As for priors, $p(\\pi)$ denotes the prior distribution on the latent variables $Z$ and $p(\\mu | \\Lambda) p(\\Lambda)$ denotes the priors distribution on the Gaussian distribution $X$.\n3.2 Gaussian Mixture Model Suppose each data point $x_n \\in \\mathbb{R}^D$ has dimension $D$. We define the latent variables $Z = { z_1, …, z_N }, Z \\in \\mathbb{R}^{N \\times K}$, where $z_i ={z_{i1}, …, z_{iK} }, z_i \\in \\mathbb{R}^K, z_{ij} \\in { 0, 1}$. Each $z_{i}$ is a vector containing k binary variables. $z_i$ can be seen as an one-hot encoding that indicates which cluster belongs to. As for $\\pi \\in \\mathbb{R}^K$, $\\pi$ is the weight of the Gaussian mixture model of each component.\n$$ p(Z | \\pi) = \\prod_{n=1}^{N}\\prod_{k=1}^{K}\\pi_{k}^{z_{nk}} $$\nThen, we define the components of the Gaussian mixture model. Each component follows Gaussian distribution and is parametrized by the mean $\\mu_k$ and covariance matrix $\\Lambda_k^{-1}$. Thus, the conditional distribution of the observed data $X \\in \\mathbb{R}^{N \\times D}$, given the variables $Z, \\mu, \\Lambda$ is\n$$ p(X | Z, \\mu, \\Lambda) = \\prod_{n=1}^{N} \\prod_{k=1}^{K} \\mathcal{N}(x_{n} | \\mu_{k}, \\Lambda_{k}^{-1})^{z_{nk}} $$\nwhere data $X$ contains $N$ data points and $D$ dimensions, parameter $\\mu \\in \\mathbb{R}^K, \\mu = { \\mu_1, …, \\mu_K }$ and $\\Lambda \\in \\mathbb{R}^{K \\times D \\times D}, \\Lambda_k \\in \\mathbb{R}^{D \\times D}, \\Lambda = { \\Lambda_1, …, \\Lambda_K }$ are the mean and the covariance matrix of each component of Gaussian mixture model.\n3.3 Dirichlet Distribution Next, we introduce another prior over the parameters. We choose the symmetric Dirichlet distribution over the mixing proportions $\\pi$. Support $x_1, …, x_K$ where $x_i \\in (0, 1)$ and $\\sum^K_{i=1} x_i = 1, K \u003e 2$ with parameters $\\alpha_1, …, \\alpha_K \u003e 0$\n$$ X \\sim \\mathcal{Dir}(\\alpha) = \\frac{1}{B(\\alpha)} \\prod^K_{i=1} x^{\\alpha_i - 1}_{i} $$\nwhere the Beta function $B(\\alpha)=\\frac{\\prod^K_{i=1} \\Gamma(\\alpha_i)}{\\Gamma(\\sum^K_{i=1} \\alpha_i)}$ and $\\alpha$ and $X$ are a set of random variables that $\\alpha = { \\alpha_1, …, \\alpha_K}$ and $X = { X_1, …, X_K}$. Note that $x_i$ is a sample value generated by $X_i$.\nExpectation\nThe mean of the Dirichlet distribution is\n$$ E[X_i] = \\frac{\\alpha_i}{\\sum^K_{k=1} \\alpha_k} $$\n$$ E[ln \\ X_i] = \\psi(\\alpha_i) - \\psi(\\sum^K_{k=1} \\alpha_k) $$\nwhere $\\psi$ is digamma function\n$$ \\psi(x) = \\frac{d}{dx} ln(\\Gamma(x)) = \\frac{\\Gamma’(x)}{\\Gamma(x)} \\approx ln(x) - \\frac{1}{2x} $$\nSymmetric Dirichlet distribution\nIn order to reduce the number of initial parameters, we use Symmetric Dirichlet distribution which is a special form of Dirichlet distribution that defined as the following\n$$ X \\sim \\mathcal{SymmDir}(\\alpha_0) = \\frac{\\Gamma(\\alpha_0 K)}{\\Gamma(\\alpha_0)^K} \\prod^K_{i=1} x^{\\alpha_0-1}i = f(x_1, …, x{K-1}; \\alpha_0) $$ where $X = { X_1, …, X_{K-1} }$. The $\\alpha$ parameter of the symmetric Dirichlet is a scalar which means all the elements $\\alpha_i$ of the $\\alpha$ are the same $\\alpha = { \\alpha_0, …, \\alpha_0 }$.\nWith Gaussian Mixture Model\nThus, we can model the distribution of the weights of Gaussian mixture model as a symmetric Dirichlet distribution.\n$$ p(\\pi) = \\mathcal{Dir}(\\pi | \\alpha_0) = \\frac{1}{B(\\alpha_0)} \\prod^K_{k=1} \\pi^{\\alpha_0 - 1}{k} = C(\\alpha_0) \\prod^K{k=1} \\pi^{\\alpha_0 - 1}_{k} $$\n3.4 Gaussian-Wishart Distribution If a normal distribution whose parameters follow the Wishart distribution. It is called Gaussian-Wishart distribution. Support $\\mu \\in \\mathbb{R}^D$ and $\\Lambda \\in \\mathbb{R}^{D \\times D}$, they are generated from Gaussian-Wishart distribution which is defined as\n$$ (\\mu, \\Lambda) \\sim \\mathcal{NW}(m_0, \\beta_0, W_0, \\nu_0) = \\mathcal{N}(\\mu | m_0, (\\beta_0 \\Lambda)^{-1} )\\mathcal{W}(\\Lambda | W_0, \\nu_0) $$\nwhere $m_0 \\in \\mathbb{R}^D$ is the location, $W \\in \\mathbb{R}^{D \\times D}$ represent the scale matrix, $\\beta_0 \\in \\mathbb{R}, \\beta_0 \u003e 0$, and $\\nu \\in \\mathbb{R}, \\nu \u003e D - 1$.\nPosterior\nAfter making $n$ observations ${ x_1, …, x_n }$ with mean $\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i$, the posterior distribution of the parameters is\n$$ (\\mu, \\Lambda) \\sim \\mathcal{NW}(m_n, \\beta_n, W_n, \\nu_n) $$\nwhere\n$$ \\beta_n = \\beta_0 + n $$\n$$ m_n = \\frac{\\beta_0 m_0 + n \\bar{x}}{\\beta_0 + n} $$\n$$ \\nu_n = \\nu_0 + n $$\n$$W^{-1}_n = W^{-1}_0 + \\sum_{i=1}^{n} (x_i - \\bar{x}) (x_i - \\bar{x})^{\\top} + \\frac{n \\beta_0}{n + \\beta_0} (\\bar{x} - m_0) (\\bar{x} - m_0)^{\\top}$$\nWith Gaussian Mixture Model\nWe define the Gaussian mixture model with Gaussian-Wishart prior.\n$$ p(\\mu, \\Lambda) = p(\\mu | \\Lambda) p(\\Lambda) = \\prod^K_{k=1} \\mathcal{N}(\\mu_k | m_0, (\\beta_0 \\Lambda_k)^{-1}) \\mathcal{W}(\\Lambda_k | W_0, \\nu_0) $$\n3.5 The Algorithm E-Step\nE-Step aims to update the variational distribution on latent variables $Z$\n$$ ln \\ q(Z; \\phi^{Z}) \\propto \\mathbb{E}_{q(\\theta; \\phi^{\\theta})} [log \\ p(Y, Z, \\theta)] $$\nThus, we can derive\n$$ ln\\ q(Z) \\propto \\mathbb{E}_{\\pi, \\mu, \\Lambda} [\\text{ln};p(X, Z, \\pi, \\mu, \\Lambda)] $$\n$$= \\mathbb{E}_{\\pi} [ln \\ p(Z | \\pi)] + \\mathbb{E}_{\\mu, \\Lambda}[ln \\ p(X | Z, \\mu, \\Lambda)] + \\mathbb{E}_{\\pi, \\mu, \\Lambda}[ln \\ p(\\pi, \\mu, \\Lambda)]$$\n$$= \\mathbb{E}_{\\pi} [ln \\ p(Z | \\pi)] + \\mathbb{E}_{\\mu, \\Lambda}[ln \\ p(X | Z, \\mu, \\Lambda)] + C$$\nwhere $C$ is a constant,\n$$\\mathbb{E}_{\\pi} [ln \\ p(Z | \\pi)] = \\mathbb{E}_{\\pi} \\Big[ ln \\ \\prod_{n=1}^{N}\\prod_{k=1}^{K}\\pi_{k}^{z_{nk}} \\Big]$$\n$$= \\mathbb{E}_{\\pi} \\Big[ \\sum_{n=1}^{N}\\sum_{k=1}^{K} z_{nk} \\ ln \\ \\pi_{k} \\Big]$$\n$$ = \\sum_{n=1}^{N}\\sum_{k=1}^{K} z_{nk} \\ \\mathbb{E}{\\pi} [ln \\ \\pi{k}] $$\nand\n$$\\mathbb{E}_{\\mu, \\Lambda}[ln \\ p(X | Z, \\mu, \\Lambda)] = \\mathbb{E}_{\\mu, \\Lambda} \\Big[ ln \\ \\prod_{n=1}^{N} \\prod_{k=1}^{K} \\mathcal{N}(x_{n} | \\mu_{k}, \\Lambda_{k}^{-1})^{z_{nk}} \\Big]$$\n$$ = \\sum_{n=1}^{N} \\sum_{k=1}^{K} z_{nk} \\ \\mathbb{E}_{\\mu_k, \\Lambda_k} \\Big[ ln \\ \\frac{e^{-\\frac{1}{2} (x_n - \\mu_k)^{\\top} \\Lambda (x_n - \\mu_k)}}{\\sqrt{(2 \\pi)^D det(\\Lambda_k^{-1})}} \\Big] $$\n$$ = \\sum_{n=1}^{N} \\sum_{k=1}^{K} z_{nk} \\ \\mathbb{E}_{\\mu_k, \\Lambda_k} \\Big[ -\\frac{1}{2} (x_n - \\mu_k)^{\\top} \\Lambda (x_n - \\mu_k) - \\frac{1}{2} ln ((2 \\pi)^D det(\\Lambda_k^{-1})) \\Big] $$\n$$ = \\sum_{n=1}^{N} \\sum_{k=1}^{K} z_{nk} \\Big( -\\frac{1}{2}\\mathbb{E}{\\mu_k, \\Lambda_k} \\Big[ (x_n - \\mu_k)^{\\top} \\Lambda (x_n - \\mu_k) \\Big] - \\frac{D}{2} ln \\ 2 \\pi + \\mathbb{E}{\\Lambda_k} \\Big[ ln \\ det(\\Lambda_k) \\Big] \\Big) $$\nDue to simplification, let\n$$ ln \\ \\rho_{nk} = \\mathbb{E}{\\pi} [ln \\ \\pi{k}] - \\frac{1}{2}\\mathbb{E}{\\mu_k, \\Lambda_k} \\Big[ (x_n - \\mu_k)^{\\top} \\Lambda (x_n - \\mu_k) \\Big] - \\frac{D}{2} ln \\ 2 \\pi + \\mathbb{E}{\\Lambda_k} \\Big[ ln \\ det(\\Lambda_k) \\Big] $$\nThus,\n$$ ln\\ q(Z) \\propto \\sum_{n=1}^{N}\\sum_{k=1}^{K} z_{nk} ln \\ \\rho_{nk} $$\nIn order to normalize the factor of $\\rho_{nk}$, we divide the $\\rho_{nk}$ by $\\sum_{j=1}^K \\rho_{nj}$ and obtain the $r_{nk}$.\n$$ ln\\ q(Z) \\propto \\sum_{n=1}^{N}\\sum_{k=1}^{K} z_{nk} ln \\ r_{nk}, \\text{where} \\ r_{nk} = \\frac{\\rho_{nk}}{\\sum_{j=1}^K \\rho_{nj}} $$\nNote that since each data point only belongs to one cluster and $z_{nk}$ is an indicator variable(if data point $i$ belongs to cluster $k$, $z_{ik} = 1$. Otherwise, $z_{ik} = 0$), thus, $\\frac{1}{K} \\sum_{j=1}^K z_{nj} = \\frac{1}{K}$. Therefore, $z_{nk}$ can be seen as a kind of probability that represents how possible does the $n$-th data point belongs to $k$-th cluster. We aims to optimize the expectation $\\mathbb{E}[z_{nk}] = 1$, when the $n$-th data point belongs to $k$-th cluster.\n$$\\mathbb{E}_{z_{nk}} [z_{nk}] = r_{nk}$$\nFor convenience, we also define some useful variables.\n$$ N_k = \\sum_{n=1}^N r_{nk}, \\quad \\bar{x}k = \\frac{1}{N_k} \\sum{n=1}^N r_{nk} x_n, \\quad S_k = \\frac{1}{N_k} r_{nk} (x_n - \\bar{x}_k) (x_n - \\bar{x}_k)^{\\top} $$\nM-Step\nE-Step aims to update the variational distribution on variables $\\theta$\n$$ ln \\ q(\\theta; \\phi^{\\theta}) \\propto \\mathbb{E}_{q(Z; \\phi^{Z})} [log \\ p(Y, Z, \\theta)] $$\nThus, we can derive\n$$ ln\\ q(\\pi, \\mu, \\Lambda) \\propto \\mathbb{E}_{Z} [ln \\ p(X, Z, \\pi, \\mu, \\Lambda)] $$\n$$= \\mathbb{E}_{Z} [ln \\ p(X | Z, \\pi, \\mu, \\Lambda)] + \\mathbb{E}_{Z} [ln \\ p(Z | \\pi)] + \\mathbb{E}_{Z} [ln \\ p(\\pi)] + \\mathbb{E}_{Z} [ln \\ p(\\mu, \\Lambda)]$$\nWe assume the joint distribution of parameters follows mean field theorem such that the parameters of each component are independent $q(\\pi, \\mu, \\Lambda) = q(\\pi) \\prod_{i=1}^N q(\\mu_i, \\Lambda_i)$. With it, the problem would be easier to solve.\nThe Posterior of Dirichlet Distribution\n$$\\mathbb{E}_{Z} [ln \\ q(Z | \\pi)] + \\mathbb{E}_{Z} [ln \\ q(\\pi)]$$\n$$= \\mathbb{E}_Z \\Big[ ln \\ \\frac{1}{B(\\alpha_0)} \\prod^K_{k=1} \\pi^{\\alpha_0 - 1}_{k} + ln \\ \\prod_{n=1}^{N}\\prod_{k=1}^{K}\\pi_{k}^{z_{nk}} \\Big]$$\n$$= \\mathbb{E}_Z \\Big[ -ln \\ B(\\alpha_0) + \\sum^K_{k=1} (\\alpha_0 - 1) ln \\ \\pi_{k} + \\sum_{n=1}^{N}\\sum_{k=1}^{K} z_{nk} ln \\ \\pi_{k} \\Big]$$\n$$= -ln \\ B(\\alpha_0) + \\sum^K_{k=1} (\\alpha_0 - 1) ln \\ \\pi_{k} + \\sum_{n=1}^{N}\\sum_{k=1}^{K} \\mathbb{E}_Z [z_{nk}] ln \\ \\pi_{k}$$\nIn order to evaluate the posterior distribution with observed data points ${ x_1, …, x_N }$ and the result of E-step, replace the $\\mathbb{E}_Z [z_{nk}]$ with $r_{nk}$.\n$$ = -ln \\ B(\\alpha_0) + \\sum^K_{k=1} (\\alpha_0 - 1) ln \\ \\pi_{k} + \\sum_{k=1}^{K} \\sum_{n=1}^{N} r_{nk} ln \\ \\pi_{k} $$\n$$ = -ln \\ B(\\alpha_0) + \\sum^K_{k=1} (\\alpha_0 - 1) ln \\ \\pi_{k} + \\sum_{k=1}^{K} \\Big( ln \\ (\\pi_{k}) \\sum_{n=1}^{N} r_{nk} \\Big) $$\n$$ = -ln \\ B(\\alpha_0) + \\sum_{k=1}^{K} (\\alpha_0 + N_k - 1) ln \\ \\pi_{k} $$\nSince the posterior distribution of Dirichlet is also Dirichlet, thus we can derive\n$$ = ln \\ \\frac{1}{B(\\alpha)} \\prod_{k=1}^{K} \\pi_{k}^{(\\alpha_0 + N_k - 1)} $$\n$$ = ln \\ \\mathcal{Dir}(\\pi | \\alpha) $$\nwhere $\\alpha \\in \\mathbb{R}^K, \\ \\alpha = { \\alpha_1, …, \\alpha_K }, \\ \\alpha_k = \\alpha_0 + N_k$ is the parameter of the Dirichlet distribution.\nThe Posterior of Gaussian-Wishart Distribution\nThe posterior distribution parametrized by $m_k, \\beta_k, W_k, \\nu_k$ is\n$$\\mathbb{E}_{Z} [ln \\ q(\\mu, \\Lambda)] = \\mathbb{E}_{Z}\\Big[ ln \\ \\prod^K_{k=1} \\mathcal{N}(\\mu_k | m_k, (\\beta_k \\Lambda_k)^{-1}) \\mathcal{W}(\\Lambda_k | W_k, \\nu_k) \\Big]$$\nwhere the parameters of the prior $\\lambda$ we’ve given before.\n$$ \\beta_k = \\beta_0 + N_k $$\n$$ m_k = \\frac{\\beta_0 m_0 + N_k \\bar{x}_k}{\\beta_k} $$\n$$ \\nu_k = \\nu + N_k + 1 $$\n$$ W^{-1}_k = W^{-1}_0 + N_k S_k + \\frac{N_k \\beta_0}{N_k + \\beta_0} (\\bar{x}_k - m_0) (\\bar{x}_k - m_0)^{\\top} $$\nSo far, we’ve derive the parameters of the prior of the bayesian model. Let’s move on to the next iteration of VBEM.\nIn order to conduct the E-step in the next iteration, we need the parameters of Gaussian mixture model which is denoted by $\\theta$. We denote the parameters of Gaussian mixture as $\\pi^, \\Lambda^$\n$$ ln \\ \\pi_k^* = \\mathbb{E}[ln \\ \\pi_k] = \\psi(\\alpha_k) - \\psi \\Big(\\sum_{k=1}^K \\alpha_k \\Big) $$\n$$ ln \\ \\Lambda_k^* = \\mathbb{E}[ln \\ det(\\Lambda_k)] = \\sum_{d=1}^{D} \\psi \\Big( \\frac{\\nu_{k} + 1 - d}{2} \\Big) + D \\ ln 2 + ln \\ det(W_k) $$\nPredict Probability Density\nThe probability dense function of the VB-GMM is a sum of Student-t distribution. We can derive the PDF from the joint distribution.\n$$ q(x^* | X) = \\sum_{z^} \\int_{\\pi} \\int_{\\mu} \\int_{\\Lambda} q(x^| z^, \\mu, \\Lambda) q(z^ | \\pi) q(\\pi, \\mu, \\Lambda | X) $$\n$$ = \\frac{1}{\\alpha^} \\sum_{k=1}^K \\alpha_k \\mathcal{St}(x^ | m_k, \\frac{(\\nu_k + 1 - D) \\beta_k}{1 + \\beta_k} W_k, \\nu_k + 1 - D) $$\nVB-GMM Pseudo Code\nRepeat until ELBO converge or reach the limit of iteration\nE Step\nCompute $ln \\ \\rho_{nk} = \\mathbb{E}{\\pi} [ln \\ \\pi{k}] - \\frac{1}{2}\\mathbb{E}{\\mu_k, \\Lambda_k} \\Big[ (x_n - \\mu_k)^{\\top} \\Lambda (x_n - \\mu_k) \\Big] - \\frac{D}{2} ln \\ 2 \\pi + \\mathbb{E}{\\Lambda_k} \\Big[ ln \\ det(\\Lambda_k) \\Big]$ where $1 \\leq n \\leq N$ and $1 \\leq k ","wordCount":"2283","inLanguage":"en","image":"https://frankccccc.github.io/blog/img/just_imgs/underwater_ice.jpg","datePublished":"2021-07-09T19:37:43+08:00","dateModified":"2021-07-09T19:37:43+08:00","author":{"@type":"Person","name":"SY Chou"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://frankccccc.github.io/blog/posts/toward_vbgmm/"},"publisher":{"@type":"Organization","name":"Golden Hat","logo":{"@type":"ImageObject","url":"https://frankccccc.github.io/blog/img/just_imgs/gold_empty_circle.svg"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><noscript><style type=text/css>#theme-toggle,.top-link{display:none}</style></noscript><header class=header><nav class=nav><div class=logo><a href=https://frankccccc.github.io/blog/ accesskey=h title="Golden Hat (Alt + H)"><img src=/blog/img/just_imgs/gold_empty_circle.svg alt=logo aria-label=logo height=35>Golden Hat</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu onscroll=menu_on_scroll()><li><a href=https://frankccccc.github.io/blog/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://frankccccc.github.io/blog/series title=Series><span>Series</span></a></li><li><a href=https://frankccccc.github.io/blog/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://frankccccc.github.io/blog/tags/ title=Tags><span>Tags</span></a></li></ul></nav><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://frankccccc.github.io/blog/>Home</a>&nbsp;»&nbsp;<a href=https://frankccccc.github.io/blog/posts/>Posts</a></div><h1 class=post-title>Toward VB-GMM<div class=entry-isdraft><sup>&nbsp;&nbsp;[draft]</sup></div></h1><div class=post-meta>July 9, 2021&nbsp;·&nbsp;11 min&nbsp;·&nbsp;SY Chou</div></header><figure class=entry-cover><img src=https://frankccccc.github.io/blog/img/just_imgs/underwater_ice.jpg alt></figure><div class=toc><details open><summary accesskey=c title="(Alt + C)"><div class=details>Table of Contents</div></summary><div class=inner><ul><li><a href=#3-variational-bayesian-gaussian-mixture-modelvb-gmm aria-label="3. Variational Bayesian Gaussian Mixture Model(VB-GMM)">3. Variational Bayesian Gaussian Mixture Model(VB-GMM)</a><ul><li><a href=#31-graphical-model aria-label="3.1 Graphical Model">3.1 Graphical Model</a></li><li><a href=#32-gaussian-mixture-model aria-label="3.2 Gaussian Mixture Model">3.2 Gaussian Mixture Model</a></li><li><a href=#33-dirichlet-distribution aria-label="3.3 Dirichlet Distribution">3.3 Dirichlet Distribution</a></li><li><a href=#34-gaussian-wishart-distribution aria-label="3.4 Gaussian-Wishart Distribution">3.4 Gaussian-Wishart Distribution</a></li><li><a href=#35-the-algorithm aria-label="3.5 The Algorithm">3.5 The Algorithm</a></li></ul></li><li><a href=#4-simulation aria-label="4. Simulation">4. Simulation</a></li><li><a href=#5-examine-on-real-dataset aria-label="5. Examine on Real Dataset">5. Examine on Real Dataset</a></li></ul></div></details></div><div class=post-content><p><strong>Note: the code in R is on my <a href=https://github.com/FrankCCCCC/ml_collection/tree/master/vbgmm>Github</a></strong></p><h2 id=3-variational-bayesian-gaussian-mixture-modelvb-gmm>3. Variational Bayesian Gaussian Mixture Model(VB-GMM)<a hidden class=anchor aria-hidden=true href=#3-variational-bayesian-gaussian-mixture-modelvb-gmm>#</a></h2><h3 id=31-graphical-model>3.1 Graphical Model<a hidden class=anchor aria-hidden=true href=#31-graphical-model>#</a></h3><p><strong>Gaussian Mixture Model & Clustering</strong></p><p><img src=/blog/img/toward_vbgmm/imgs/graphical_model.png alt></p><p>The variational Bayesian Gaussian mixture model(VB-GMM) can be represented as the above graphical model. We see each data point as a Gaussian mixture distribution with $K$ components. We also denote the number of data points as $N$. Each $x_n$ is a Gaussian mixture distribution with a weight $\pi_n$ corresponds to a data point. $z_n$ is an one-hot latent variable that indicates which cluster(component) does the data point belongs to. Finally, A component $k$ follows the Gaussian distribution with mean $\mu_k$ and covariance matrix $\Lambda_k$. $\Lambda = { \Lambda_1, &mldr;, \Lambda_K }$ and $\mu = { \mu_1, &mldr;, \mu_K }$ are vectors denote the parameters of Gaussian mixture distribution.</p><p>Thus, the joint distribution of the VB-GMM is</p><p>$$
p(X, Z, \pi, \mu, \Lambda) = p(X | Z, \pi, \mu, \Lambda) p(Z | \pi) p(\pi) p(\mu | \Lambda) p(\Lambda)
$$</p><p>$p(X | Z, \pi, \mu, \Lambda)$ denotes the Gaussian mixture model given on the latent variables and parameters. $p(Z | \pi)$ denotes the latent variables. As for priors, $p(\pi)$ denotes the prior distribution on the latent variables $Z$ and $p(\mu | \Lambda) p(\Lambda)$ denotes the priors distribution on the Gaussian distribution $X$.</p><h3 id=32-gaussian-mixture-model>3.2 Gaussian Mixture Model<a hidden class=anchor aria-hidden=true href=#32-gaussian-mixture-model>#</a></h3><p>Suppose each data point $x_n \in \mathbb{R}^D$ has dimension $D$. We define the latent variables $Z = { z_1, &mldr;, z_N }, Z \in \mathbb{R}^{N \times K}$, where $z_i ={z_{i1}, &mldr;, z_{iK} }, z_i \in \mathbb{R}^K, z_{ij} \in { 0, 1}$. Each $z_{i}$ is a vector containing k binary variables. $z_i$ can be seen as an one-hot encoding that indicates which cluster belongs to. As for $\pi \in \mathbb{R}^K$, $\pi$ is the weight of the Gaussian mixture model of each component.</p><p>$$
p(Z | \pi) = \prod_{n=1}^{N}\prod_{k=1}^{K}\pi_{k}^{z_{nk}}
$$</p><p>Then, we define the components of the Gaussian mixture model. Each component follows Gaussian distribution and is parametrized by the mean $\mu_k$ and covariance matrix $\Lambda_k^{-1}$. Thus, the conditional distribution of the observed data $X \in \mathbb{R}^{N \times D}$, given the variables $Z, \mu, \Lambda$ is</p><p>$$
p(X | Z, \mu, \Lambda) = \prod_{n=1}^{N} \prod_{k=1}^{K} \mathcal{N}(x_{n} | \mu_{k}, \Lambda_{k}^{-1})^{z_{nk}}
$$</p><p>where data $X$ contains $N$ data points and $D$ dimensions, parameter $\mu \in \mathbb{R}^K, \mu = { \mu_1, &mldr;, \mu_K }$ and $\Lambda \in \mathbb{R}^{K \times D \times D}, \Lambda_k \in \mathbb{R}^{D \times D}, \Lambda = { \Lambda_1, &mldr;, \Lambda_K }$ are the mean and the covariance matrix of each component of Gaussian mixture model.</p><h3 id=33-dirichlet-distribution>3.3 Dirichlet Distribution<a hidden class=anchor aria-hidden=true href=#33-dirichlet-distribution>#</a></h3><p>Next, we introduce another prior over the parameters. We choose the symmetric Dirichlet distribution over the mixing proportions $\pi$. Support $x_1, &mldr;, x_K$ where $x_i \in (0, 1)$ and $\sum^K_{i=1} x_i = 1, K > 2$ with parameters $\alpha_1, &mldr;, \alpha_K > 0$</p><p>$$
X \sim \mathcal{Dir}(\alpha) = \frac{1}{B(\alpha)} \prod^K_{i=1} x^{\alpha_i - 1}_{i}
$$</p><p>where the Beta function $B(\alpha)=\frac{\prod^K_{i=1} \Gamma(\alpha_i)}{\Gamma(\sum^K_{i=1} \alpha_i)}$ and $\alpha$ and $X$ are a set of random variables that $\alpha = { \alpha_1, &mldr;, \alpha_K}$ and $X = { X_1, &mldr;, X_K}$. Note that $x_i$ is a sample value generated by $X_i$.</p><p><strong>Expectation</strong></p><p>The mean of the Dirichlet distribution is</p><p>$$
E[X_i] = \frac{\alpha_i}{\sum^K_{k=1} \alpha_k}
$$</p><p>$$
E[ln \ X_i] = \psi(\alpha_i) - \psi(\sum^K_{k=1} \alpha_k)
$$</p><p>where $\psi$ is <strong>digamma</strong> function</p><p>$$
\psi(x) = \frac{d}{dx} ln(\Gamma(x)) = \frac{\Gamma&rsquo;(x)}{\Gamma(x)} \approx ln(x) - \frac{1}{2x}
$$</p><p><strong>Symmetric Dirichlet distribution</strong></p><p>In order to reduce the number of initial parameters, we use <strong>Symmetric Dirichlet distribution</strong> which is a special form of Dirichlet distribution that defined as the following</p><p>$$
X \sim \mathcal{SymmDir}(\alpha_0) = \frac{\Gamma(\alpha_0 K)}{\Gamma(\alpha_0)^K} \prod^K_{i=1} x^{\alpha_0-1}<em>i = f(x_1, &mldr;, x</em>{K-1}; \alpha_0)
$$
where $X = { X_1, &mldr;, X_{K-1} }$. The $\alpha$ parameter of the symmetric Dirichlet is a scalar which means all the elements $\alpha_i$ of the $\alpha$ are the same $\alpha = { \alpha_0, &mldr;, \alpha_0 }$.</p><p><strong>With Gaussian Mixture Model</strong></p><p>Thus, we can model the distribution of the weights of Gaussian mixture model as a symmetric Dirichlet distribution.</p><p>$$
p(\pi) = \mathcal{Dir}(\pi | \alpha_0) = \frac{1}{B(\alpha_0)} \prod^K_{k=1} \pi^{\alpha_0 - 1}<em>{k} = C(\alpha_0) \prod^K</em>{k=1} \pi^{\alpha_0 - 1}_{k}
$$</p><h3 id=34-gaussian-wishart-distribution>3.4 Gaussian-Wishart Distribution<a hidden class=anchor aria-hidden=true href=#34-gaussian-wishart-distribution>#</a></h3><p>If a normal distribution whose parameters follow the Wishart distribution. It is called <strong>Gaussian-Wishart distribution</strong>. Support $\mu \in \mathbb{R}^D$ and $\Lambda \in \mathbb{R}^{D \times D}$, they are generated from Gaussian-Wishart distribution which is defined as</p><p>$$
(\mu, \Lambda) \sim \mathcal{NW}(m_0, \beta_0, W_0, \nu_0) = \mathcal{N}(\mu | m_0, (\beta_0 \Lambda)^{-1} )\mathcal{W}(\Lambda | W_0, \nu_0)
$$</p><p>where $m_0 \in \mathbb{R}^D$ is the location, $W \in \mathbb{R}^{D \times D}$ represent the scale matrix, $\beta_0 \in \mathbb{R}, \beta_0 > 0$, and $\nu \in \mathbb{R}, \nu > D - 1$.</p><p><strong>Posterior</strong></p><p>After making $n$ observations ${ x_1, &mldr;, x_n }$ with mean $\bar{x} = \frac{1}{n} \sum_{i=1}^{n} x_i$, the posterior distribution of the parameters is</p><p>$$
(\mu, \Lambda) \sim \mathcal{NW}(m_n, \beta_n, W_n, \nu_n)
$$</p><p>where</p><p>$$
\beta_n = \beta_0 + n
$$</p><p>$$
m_n = \frac{\beta_0 m_0 + n \bar{x}}{\beta_0 + n}
$$</p><p>$$
\nu_n = \nu_0 + n
$$</p><p><code>$$W^{-1}_n = W^{-1}_0 + \sum_{i=1}^{n} (x_i - \bar{x}) (x_i - \bar{x})^{\top} + \frac{n \beta_0}{n + \beta_0} (\bar{x} - m_0) (\bar{x} - m_0)^{\top}$$</code></p><p><strong>With Gaussian Mixture Model</strong></p><p>We define the Gaussian mixture model with Gaussian-Wishart prior.</p><p>$$
p(\mu, \Lambda) = p(\mu | \Lambda) p(\Lambda) = \prod^K_{k=1} \mathcal{N}(\mu_k | m_0, (\beta_0 \Lambda_k)^{-1}) \mathcal{W}(\Lambda_k | W_0, \nu_0)
$$</p><h3 id=35-the-algorithm>3.5 The Algorithm<a hidden class=anchor aria-hidden=true href=#35-the-algorithm>#</a></h3><p><strong>E-Step</strong></p><p>E-Step aims to update the variational distribution on latent variables $Z$</p><p>$$
ln \ q(Z; \phi^{Z}) \propto \mathbb{E}_{q(\theta; \phi^{\theta})} [log \ p(Y, Z, \theta)]
$$</p><p>Thus, we can derive</p><p>$$
ln\ q(Z) \propto \mathbb{E}_{\pi, \mu, \Lambda} [\text{ln};p(X, Z, \pi, \mu, \Lambda)]
$$</p><p><code>$$= \mathbb{E}_{\pi} [ln \ p(Z | \pi)] + \mathbb{E}_{\mu, \Lambda}[ln \ p(X | Z, \mu, \Lambda)] + \mathbb{E}_{\pi, \mu, \Lambda}[ln \ p(\pi, \mu, \Lambda)]$$</code></p><p><code>$$= \mathbb{E}_{\pi} [ln \ p(Z | \pi)] + \mathbb{E}_{\mu, \Lambda}[ln \ p(X | Z, \mu, \Lambda)] + C$$</code></p><p>where $C$ is a constant,</p><p><code>$$\mathbb{E}_{\pi} [ln \ p(Z | \pi)] = \mathbb{E}_{\pi} \Big[ ln \ \prod_{n=1}^{N}\prod_{k=1}^{K}\pi_{k}^{z_{nk}} \Big]$$</code></p><p><code>$$= \mathbb{E}_{\pi} \Big[ \sum_{n=1}^{N}\sum_{k=1}^{K} z_{nk} \ ln \ \pi_{k} \Big]$$</code></p><p>$$
= \sum_{n=1}^{N}\sum_{k=1}^{K} z_{nk} \ \mathbb{E}<em>{\pi} [ln \ \pi</em>{k}]
$$</p><p>and</p><p><code>$$\mathbb{E}_{\mu, \Lambda}[ln \ p(X | Z, \mu, \Lambda)] = \mathbb{E}_{\mu, \Lambda} \Big[ ln \ \prod_{n=1}^{N} \prod_{k=1}^{K} \mathcal{N}(x_{n} | \mu_{k}, \Lambda_{k}^{-1})^{z_{nk}} \Big]$$</code></p><p>$$
= \sum_{n=1}^{N} \sum_{k=1}^{K} z_{nk} \ \mathbb{E}_{\mu_k, \Lambda_k} \Big[ ln \ \frac{e^{-\frac{1}{2} (x_n - \mu_k)^{\top} \Lambda (x_n - \mu_k)}}{\sqrt{(2 \pi)^D det(\Lambda_k^{-1})}} \Big]
$$</p><p>$$
= \sum_{n=1}^{N} \sum_{k=1}^{K} z_{nk} \ \mathbb{E}_{\mu_k, \Lambda_k} \Big[ -\frac{1}{2} (x_n - \mu_k)^{\top} \Lambda (x_n - \mu_k) - \frac{1}{2} ln ((2 \pi)^D det(\Lambda_k^{-1})) \Big]
$$</p><p>$$
= \sum_{n=1}^{N} \sum_{k=1}^{K} z_{nk} \Big( -\frac{1}{2}\mathbb{E}<em>{\mu_k, \Lambda_k} \Big[ (x_n - \mu_k)^{\top} \Lambda (x_n - \mu_k) \Big] - \frac{D}{2} ln \ 2 \pi + \mathbb{E}</em>{\Lambda_k} \Big[ ln \ det(\Lambda_k) \Big] \Big)
$$</p><p>Due to simplification, let</p><p>$$
ln \ \rho_{nk} = \mathbb{E}<em>{\pi} [ln \ \pi</em>{k}] - \frac{1}{2}\mathbb{E}<em>{\mu_k, \Lambda_k} \Big[ (x_n - \mu_k)^{\top} \Lambda (x_n - \mu_k) \Big] - \frac{D}{2} ln \ 2 \pi + \mathbb{E}</em>{\Lambda_k} \Big[ ln \ det(\Lambda_k) \Big]
$$</p><p>Thus,</p><p>$$
ln\ q(Z) \propto \sum_{n=1}^{N}\sum_{k=1}^{K} z_{nk} ln \ \rho_{nk}
$$</p><p>In order to normalize the factor of $\rho_{nk}$, we divide the $\rho_{nk}$ by $\sum_{j=1}^K \rho_{nj}$ and obtain the $r_{nk}$.</p><p>$$
ln\ q(Z) \propto \sum_{n=1}^{N}\sum_{k=1}^{K} z_{nk} ln \ r_{nk}, \text{where} \ r_{nk} = \frac{\rho_{nk}}{\sum_{j=1}^K \rho_{nj}}
$$</p><p>Note that since each data point only belongs to one cluster and $z_{nk}$ is an indicator variable(if data point $i$ belongs to cluster $k$, $z_{ik} = 1$. Otherwise, $z_{ik} = 0$), thus, $\frac{1}{K} \sum_{j=1}^K z_{nj} = \frac{1}{K}$. Therefore, $z_{nk}$ can be seen as a kind of probability that represents how possible does the $n$-th data point belongs to $k$-th cluster. We aims to optimize the expectation $\mathbb{E}[z_{nk}] = 1$, when the $n$-th data point belongs to $k$-th cluster.</p><p><code>$$\mathbb{E}_{z_{nk}} [z_{nk}] = r_{nk}$$</code></p><p>For convenience, we also define some useful variables.</p><p>$$
N_k = \sum_{n=1}^N r_{nk}, \quad \bar{x}<em>k = \frac{1}{N_k} \sum</em>{n=1}^N r_{nk} x_n, \quad S_k = \frac{1}{N_k} r_{nk} (x_n - \bar{x}_k) (x_n - \bar{x}_k)^{\top}
$$</p><p><strong>M-Step</strong></p><p>E-Step aims to update the variational distribution on variables $\theta$</p><p>$$
ln \ q(\theta; \phi^{\theta}) \propto \mathbb{E}_{q(Z; \phi^{Z})} [log \ p(Y, Z, \theta)]
$$</p><p>Thus, we can derive</p><p>$$
ln\ q(\pi, \mu, \Lambda) \propto \mathbb{E}_{Z} [ln \ p(X, Z, \pi, \mu, \Lambda)]
$$</p><p><code>$$= \mathbb{E}_{Z} [ln \ p(X | Z, \pi, \mu, \Lambda)] + \mathbb{E}_{Z} [ln \ p(Z | \pi)] + \mathbb{E}_{Z} [ln \ p(\pi)] + \mathbb{E}_{Z} [ln \ p(\mu, \Lambda)]$$</code></p><p>We assume the joint distribution of parameters follows <strong>mean field theorem</strong> such that the parameters of each component are independent $q(\pi, \mu, \Lambda) = q(\pi) \prod_{i=1}^N q(\mu_i, \Lambda_i)$. With it, the problem would be easier to solve.</p><p><strong>The Posterior of Dirichlet Distribution</strong></p><p><code>$$\mathbb{E}_{Z} [ln \ q(Z | \pi)] + \mathbb{E}_{Z} [ln \ q(\pi)]$$</code></p><p><code>$$= \mathbb{E}_Z \Big[ ln \ \frac{1}{B(\alpha_0)} \prod^K_{k=1} \pi^{\alpha_0 - 1}_{k} + ln \ \prod_{n=1}^{N}\prod_{k=1}^{K}\pi_{k}^{z_{nk}} \Big]$$</code></p><p><code>$$= \mathbb{E}_Z \Big[ -ln \ B(\alpha_0) + \sum^K_{k=1} (\alpha_0 - 1) ln \ \pi_{k} + \sum_{n=1}^{N}\sum_{k=1}^{K} z_{nk} ln \ \pi_{k} \Big]$$</code></p><p><code>$$= -ln \ B(\alpha_0) + \sum^K_{k=1} (\alpha_0 - 1) ln \ \pi_{k} + \sum_{n=1}^{N}\sum_{k=1}^{K} \mathbb{E}_Z [z_{nk}] ln \ \pi_{k}$$</code></p><p>In order to evaluate the posterior distribution with observed data points ${ x_1, &mldr;, x_N }$ and the result of E-step, replace the <code>$\mathbb{E}_Z [z_{nk}]$</code> with <code>$r_{nk}$</code>.</p><p>$$
= -ln \ B(\alpha_0) + \sum^K_{k=1} (\alpha_0 - 1) ln \ \pi_{k} + \sum_{k=1}^{K} \sum_{n=1}^{N} r_{nk} ln \ \pi_{k}
$$</p><p>$$
= -ln \ B(\alpha_0) + \sum^K_{k=1} (\alpha_0 - 1) ln \ \pi_{k} + \sum_{k=1}^{K} \Big( ln \ (\pi_{k}) \sum_{n=1}^{N} r_{nk} \Big)
$$</p><p>$$
= -ln \ B(\alpha_0) + \sum_{k=1}^{K} (\alpha_0 + N_k - 1) ln \ \pi_{k}
$$</p><p>Since the posterior distribution of Dirichlet is also Dirichlet, thus we can derive</p><p>$$
= ln \ \frac{1}{B(\alpha)} \prod_{k=1}^{K} \pi_{k}^{(\alpha_0 + N_k - 1)}
$$</p><p>$$
= ln \ \mathcal{Dir}(\pi | \alpha)
$$</p><p>where $\alpha \in \mathbb{R}^K, \ \alpha = { \alpha_1, &mldr;, \alpha_K }, \ \alpha_k = \alpha_0 + N_k$ is the parameter of the Dirichlet distribution.</p><p><strong>The Posterior of Gaussian-Wishart Distribution</strong></p><p>The posterior distribution parametrized by $m_k, \beta_k, W_k, \nu_k$ is</p><p><code>$$\mathbb{E}_{Z} [ln \ q(\mu, \Lambda)] = \mathbb{E}_{Z}\Big[ ln \ \prod^K_{k=1} \mathcal{N}(\mu_k | m_k, (\beta_k \Lambda_k)^{-1}) \mathcal{W}(\Lambda_k | W_k, \nu_k) \Big]$$</code></p><p>where the parameters of the prior $\lambda$ we&rsquo;ve given before.</p><p>$$
\beta_k = \beta_0 + N_k
$$</p><p>$$
m_k = \frac{\beta_0 m_0 + N_k \bar{x}_k}{\beta_k}
$$</p><p>$$
\nu_k = \nu + N_k + 1
$$</p><p>$$
W^{-1}_k = W^{-1}_0 + N_k S_k + \frac{N_k \beta_0}{N_k + \beta_0} (\bar{x}_k - m_0) (\bar{x}_k - m_0)^{\top}
$$</p><p>So far, we&rsquo;ve derive the parameters of the prior of the bayesian model. Let&rsquo;s move on to the next iteration of VBEM.</p><p>In order to conduct the E-step in the next iteration, we need the parameters of Gaussian mixture model which is denoted by $\theta$. We denote the parameters of Gaussian mixture as $\pi^<em>, \Lambda^</em>$</p><p>$$
ln \ \pi_k^* = \mathbb{E}[ln \ \pi_k] = \psi(\alpha_k) - \psi \Big(\sum_{k=1}^K \alpha_k \Big)
$$</p><p>$$
ln \ \Lambda_k^* = \mathbb{E}[ln \ det(\Lambda_k)] = \sum_{d=1}^{D} \psi \Big( \frac{\nu_{k} + 1 - d}{2} \Big) + D \ ln 2 + ln \ det(W_k)
$$</p><p><strong>Predict Probability Density</strong></p><p>The probability dense function of the VB-GMM is a sum of Student-t distribution. We can derive the PDF from the joint distribution.</p><p>$$
q(x^* | X) = \sum_{z^<em>} \int_{\pi} \int_{\mu} \int_{\Lambda} q(x^</em>| z^<em>, \mu, \Lambda) q(z^</em> | \pi) q(\pi, \mu, \Lambda | X)
$$</p><p>$$
= \frac{1}{\alpha^<em>} \sum_{k=1}^K \alpha_k \mathcal{St}(x^</em> | m_k, \frac{(\nu_k + 1 - D) \beta_k}{1 + \beta_k} W_k, \nu_k + 1 - D)
$$</p><p><strong>VB-GMM Pseudo Code</strong></p><hr><p>Repeat until ELBO converge or reach the limit of iteration</p><p>E Step</p><ul><li><p>Compute $ln \ \rho_{nk} = \mathbb{E}<em>{\pi} [ln \ \pi</em>{k}] - \frac{1}{2}\mathbb{E}<em>{\mu_k, \Lambda_k} \Big[ (x_n - \mu_k)^{\top} \Lambda (x_n - \mu_k) \Big] - \frac{D}{2} ln \ 2 \pi + \mathbb{E}</em>{\Lambda_k} \Big[ ln \ det(\Lambda_k) \Big]$ where $1 \leq n \leq N$ and $1 \leq k &lt;K$</p></li><li><p>Compute $ln \ r_{nk} = LogSumExp(ln \ \rho_{nk})$</p></li><li><p>Compute $r_{nk} = e^{(ln \ \rho_{nk})}$</p></li><li><p>Compute $N_k = \sum_{n=1}^N r_{nk}$</p></li><li><p>Compute <code>$\bar{x}_k = \frac{1}{N_k} \sum_{n=1}^N r_{nk} x_n$</code></p></li><li><p>Compute $S_k = \frac{1}{N_k} r_{nk} (x_n - \bar{x}_k) (x_n - \bar{x}_k)^{\top}$</p></li></ul><p>M Step</p><ul><li><p>Update Dirichlet distribution</p><ul><li>Compute $\alpha_k = \alpha_0 + N_k, \ for \ 1 \leq k \leq K$</li></ul></li><li><p>Update Gaussian Mixture distribution</p><ul><li>Compute $ln \ \pi_k^* = \mathbb{E}[ln \ \pi_k] = \psi(\alpha_k) - \psi \Big(\sum_{k=1}^K \alpha_k \Big)$</li></ul></li><li><p>Update Gaussian-Wishart distribution</p><ul><li><p>Compute $\beta_k = \beta_0 + N_k, \ for \ 1 \leq k \leq K$</p></li><li><p>Compute $m_k = \frac{\beta_0 m_0 + N_k \bar{x}_k}{\beta_k}, \ for \ 1 \leq k \leq K$</p></li><li><p>Compute $\nu_k = \nu + N_k + 1, \ for \ 1 \leq k \leq K$</p></li><li><p>Compute $W^{-1}_k = W^{-1}_0 + N_k S_k + \frac{N_k \beta_0}{N_k + \beta_0} (\bar{x}_k - m_0) (\bar{x}_k - m_0)^{\top}, \ for \ 1 \leq k \leq K$</p></li></ul></li><li><p>Update posterior for next iteration</p><ul><li><p>Compute $ln \ \Lambda_k^* = \sum_{d=1}^{D} \psi \Big( \frac{\nu_{k} + 1 - d}{2} \Big) + D \ ln 2 + ln \ det(W_k), \ for \ 1 \leq k \leq K$</p></li><li><p>Compute $ln \ \pi_k^* = \psi(\alpha_k) - \psi \Big(\sum_{k=1}^K \alpha_k \Big), \ for \ 1 \leq k \leq K$</p></li></ul></li></ul><hr><h2 id=4-simulation>4. Simulation<a hidden class=anchor aria-hidden=true href=#4-simulation>#</a></h2><p>We generate the simulation data by bivariate Gaussian mixture distribution with 5 modals. We use K-means which is given 5 clusters as hyperparameter. VB-GMM out-performs than K-means while clustering an unbalanced dataset. Even though the K-mean already has correct hyperparameters.</p><p>VB-GMM not only deals with unbalanced dataset well but also self-adapts to the best number of clusters which is very close to the ground truth.</p><p><strong>K-means</strong>
<img src=/blog/img/toward_vbgmm/simulate/kmean.jpg alt></p><p><strong>VB-GMM</strong></p><p><img src=/blog/img/toward_vbgmm/simulate/vbgmm.jpg alt></p><p><img src=/blog/img/toward_vbgmm/simulate/gganim_plot0019.png alt></p><p>With the animation, we can see the number of clusters of VB-GMM keep reducing until it find a best fit to the dataset. Please refer to the <a href=https://github.com/FrankCCCCC/math_new/blob/master/statistical_computing/Mid/simulate/animate.gif>link</a>.</p><p><img src=/blog/img/toward_vbgmm/simulate/animate.gif alt></p><h2 id=5-examine-on-real-dataset>5. Examine on Real Dataset<a hidden class=anchor aria-hidden=true href=#5-examine-on-real-dataset>#</a></h2><p>The result is similar to the simulation. VB-GMM usually out-perform than K-means while clustering unbalanced dataset.</p><p><strong>Wine Dataset</strong></p><p><img src=/blog/img/toward_vbgmm/wine/kmean.jpg alt></p><p><img src=/blog/img/toward_vbgmm/wine/vbgmm.jpg alt></p><p><strong>Beaver1 Dataset</strong></p><p><img src=/blog/img/toward_vbgmm/beaver1/kmean.jpg alt></p><p><img src=/blog/img/toward_vbgmm/beaver1/vbgmm.jpg alt></p><p><strong>Beaver2 Dataset</strong></p><p><img src=/blog/img/toward_vbgmm/beaver2/kmean.jpg alt></p><p><img src=/blog/img/toward_vbgmm/beaver2/vbgmm.jpg alt></p></div><footer class=post-footer><ul class=post-tags><li><a href=https://frankccccc.github.io/blog/tags/em/>EM</a></li><li><a href=https://frankccccc.github.io/blog/tags/machine-learning/>machine learning</a></li><li><a href=https://frankccccc.github.io/blog/tags/statistics/>statistics</a></li><li><a href=https://frankccccc.github.io/blog/tags/bayes/>bayes</a></li></ul><nav class=paginav><a class=prev href=https://frankccccc.github.io/blog/posts/move_blog/><span class=title>« Prev Page</span><br><span>部落格搬家記</span></a>
<a class=next href=https://frankccccc.github.io/blog/posts/a_paper_review_learning_to_adapt/><span class=title>Next Page »</span><br><span>A Paper Review: Learning to Adapt</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Toward VB-GMM on twitter" href="https://twitter.com/intent/tweet/?text=Toward%20VB-GMM&url=https%3a%2f%2ffrankccccc.github.io%2fblog%2fposts%2ftoward_vbgmm%2f&hashtags=EM%2cmachinelearning%2cstatistics%2cbayes"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Toward VB-GMM on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2ffrankccccc.github.io%2fblog%2fposts%2ftoward_vbgmm%2f&title=Toward%20VB-GMM&summary=Toward%20VB-GMM&source=https%3a%2f%2ffrankccccc.github.io%2fblog%2fposts%2ftoward_vbgmm%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Toward VB-GMM on reddit" href="https://reddit.com/submit?url=https%3a%2f%2ffrankccccc.github.io%2fblog%2fposts%2ftoward_vbgmm%2f&title=Toward%20VB-GMM"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Toward VB-GMM on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2ffrankccccc.github.io%2fblog%2fposts%2ftoward_vbgmm%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Toward VB-GMM on whatsapp" href="https://api.whatsapp.com/send?text=Toward%20VB-GMM%20-%20https%3a%2f%2ffrankccccc.github.io%2fblog%2fposts%2ftoward_vbgmm%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Toward VB-GMM on telegram" href="https://telegram.me/share/url?text=Toward%20VB-GMM&url=https%3a%2f%2ffrankccccc.github.io%2fblog%2fposts%2ftoward_vbgmm%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer><div style=padding-top:2.5rem;padding-bottom:2.5rem;text-align:left><h1 style=padding-bottom:.5rem>COMMENTS</h1><h5>Your comments will encouage me to share more~~</h5></div><div id=utter-container></div><script src=https://utteranc.es/client.js repo=frankccccc/blog issue-term=title theme=photon-dark crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2022 <a href=https://frankccccc.github.io/blog/>Golden Hat</a></span>
<span>&#183;</span>
<span>Powered by <a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span>
<span>&#183;</span>
<span>Theme <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)"><button class=top-link id=top-link type=button accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></button></a>
<script defer src=/blog/assets/js/highlight.min.b95bacdc39e37a332a9f883b1e78be4abc1fdca2bc1f2641f55e3cd3dabd4d61.js integrity="sha256-uVus3DnjejMqn4g7Hni+Srwf3KK8HyZB9V4809q9TWE=" onload=hljs.initHighlightingOnLoad()></script>
<script>window.onload=function(){localStorage.getItem("menu-scroll-position")&&(document.getElementById("menu").scrollLeft=localStorage.getItem("menu-scroll-position"))},document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})});var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")};function menu_on_scroll(){localStorage.setItem("menu-scroll-position",document.getElementById("menu").scrollLeft)}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>