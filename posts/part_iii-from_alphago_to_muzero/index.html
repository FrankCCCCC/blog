<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Part III - From AlphaGo to MuZero | Golden Hat</title><meta name=keywords content="RL,deep learning,model-based RL"><meta name=description content="Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model It is just the paper proposing MuZero. MuZero is quite famous when I write this note(Jan 2021). Lots of people tried to reproduce the incredible performance of the paper. Some of well-known implementations like muzero-general give a clear and modular implementation of MuZero. If you are interested in MuZero, you can play with it. Well, let&rsquo;s diving into the paper."><meta name=author content="SY Chou"><link rel=canonical href=https://frankccccc.github.io/blog/posts/part_iii-from_alphago_to_muzero/><script async src="https://www.googletagmanager.com/gtag/js?id=G-S9MCZ2NDS7"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','G-S9MCZ2NDS7')</script><link href=/blog/assets/css/stylesheet.min.3768d020b7a9fd49f21ddb7d521ce9b187ce47c2929b883041396159d4d25553.css integrity="sha256-N2jQILep/UnyHdt9UhzpsYfOR8KSm4gwQTlhWdTSVVM=" rel="preload stylesheet" as=style><link rel=icon href=https://frankccccc.github.io/blog/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://frankccccc.github.io/blog/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://frankccccc.github.io/blog/favicon-32x32.png><link rel=apple-touch-icon href=https://frankccccc.github.io/blog/apple-touch-icon.png><link rel=mask-icon href=https://frankccccc.github.io/blog/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><meta name=generator content="Hugo 0.81.0"><script async src="https://www.googletagmanager.com/gtag/js?id=G-S9MCZ2NDS7"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','G-S9MCZ2NDS7')</script><meta property="og:title" content="Part III - From AlphaGo to MuZero"><meta property="og:description" content="Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model It is just the paper proposing MuZero. MuZero is quite famous when I write this note(Jan 2021). Lots of people tried to reproduce the incredible performance of the paper. Some of well-known implementations like muzero-general give a clear and modular implementation of MuZero. If you are interested in MuZero, you can play with it. Well, let&rsquo;s diving into the paper."><meta property="og:type" content="article"><meta property="og:url" content="https://frankccccc.github.io/blog/posts/part_iii-from_alphago_to_muzero/"><meta property="og:image" content="https://frankccccc.github.io/blog/img/just_imgs/muzero.webp"><meta property="article:published_time" content="2021-03-04T17:02:31+08:00"><meta property="article:modified_time" content="2021-03-04T17:02:31+08:00"><meta property="og:see_also" content="https://frankccccc.github.io/blog/posts/part_ii-from_alphago_to_muzero/"><meta property="og:see_also" content="https://frankccccc.github.io/blog/posts/part_i-from_alphago_to_muzero/"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://frankccccc.github.io/blog/img/just_imgs/muzero.webp"><meta name=twitter:title content="Part III - From AlphaGo to MuZero"><meta name=twitter:description content="Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model It is just the paper proposing MuZero. MuZero is quite famous when I write this note(Jan 2021). Lots of people tried to reproduce the incredible performance of the paper. Some of well-known implementations like muzero-general give a clear and modular implementation of MuZero. If you are interested in MuZero, you can play with it. Well, let&rsquo;s diving into the paper."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://frankccccc.github.io/blog/posts/"},{"@type":"ListItem","position":2,"name":"Part III - From AlphaGo to MuZero","item":"https://frankccccc.github.io/blog/posts/part_iii-from_alphago_to_muzero/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Part III - From AlphaGo to MuZero","name":"Part III - From AlphaGo to MuZero","description":"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model It is just the paper proposing MuZero. MuZero is quite famous when I write this note(Jan 2021). Lots of people …","keywords":["RL","deep learning","model-based RL"],"articleBody":"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model It is just the paper proposing MuZero. MuZero is quite famous when I write this note(Jan 2021). Lots of people tried to reproduce the incredible performance of the paper. Some of well-known implementations like muzero-general give a clear and modular implementation of MuZero. If you are interested in MuZero, you can play with it. Well, let’s diving into the paper.\nIntrodution The main idea of MuZero is to predict the future that are directly relevant for planning. The model receives the observation (e.g. an image of the Go board or the Atari screen) as an input and transforms it into a hidden state. The hidden state is then updated iteratively by a recurrent process that receives the previous hidden state and a hypothetical next action. At every one of these steps the model predicts the policy (e.g. the move to play), value function (e.g. the predicted winner), and immediate reward (e.g. the points scored by playing a move).\nAlgorithm The MuZero consist of 3 components: dynamic function, prediction function, representation function:\n  Dynamic Function $g(s^t, a^{t+1})=r^{t+1}, s^{t+1}$\n  Prediction Function $f(s^t)=p^t, v^t$\n  Representation Function $h(o^0)=s^0$\n  We denote $o^0$ as the initial observation, $s^0$ as the initial hidden state, $p^t$ as the policy function, $v^t$ as value function, and $r^t$ as reward function at time step $t$. These 3 components compose the deterministic latent dynamic model for MuZero. (The paper says stochastic transitions is left for future works)\nThe MuZero plan like Figure1 part A. Given a previous hidden state $s^{k−1}$ and a candidate action $a^k$, the dynamics function $g$ produces an immediate reward $r^k$ and a new hidden state $s^k$. The policy $p^k$ and value function $v^k$ are computed from the hidden state $s^k$ by a prediction function $f$.\nThe MuZero act in the environment like part B of Figure1. A Monte-Carlo Tree Search is performed at each timestep $t$, as described in A. An action $a_{t+1}$ is sampled from the search policy $\\pi_t$. The environment receives the action and generates a new observation $o_{t+1}$ and reward $u_{t+1}$. At the end of the episode the trajectory data is stored into a replay buffer.\nThe MuZero model $\\mu_{\\theta}$ with parameters $\\theta$, conditioned on past observations $o_1, …, o_t$ and future actions $a_{t+1}, …, a_{t+k}$.\nThe model always predicts the policy $p_t^k$, value function $v_t^k$, and immdiate reward $r_t^k$ after $k$ time steps at timestep $t$. For more detailed, the three future quantities are as following:\n  Policy\n$$ p_t^k \\approx \\pi (a_{t+k+1} | o_{1}, …, o_t, a_{t+1}, …, a_{t+k}) $$\nwhere $\\pi$ is the policy used to select real actions\n  Value Function\n$$ v_t^k \\approx E[u_{t+k+1} + \\gamma \\ u_{t+k+2} + … | o_1 , …, o_t, a_{t+1}, …, a_{t+k}] $$\nwhere $u$ is the true, observation reward. $\\gamma$ is the discount factor of the environment.\n  Immediate Reward\n$$ r_t^k \\approx u_{t + k} $$\n  The MuZero train in the environment like part C of Figure1.\nAll parameters of the model are trained jointly to accurately match the policy, value, and reward, for every hypothetical step $k$, to corresponding target values observed after $k$ actual time-steps t have elapsed.(That is predict the policy, value, and reward after $k$ steps from current time-step $t$.) The training objective is to minimise the error between predicted policy $p_t^k$ and MCTS search policy $\\pi_{t+k}$.\nFor trade-off between accuracy and stability, we allow for long episodes with discounting and intermediate rewards by bootstrapping $n$ steps into the future from the search value. Final outcomes {lose, draw, win} in board games are treated as rewards $u_t \\in \\{ −1, 0, +1 \\}$ occurring at the final step of the episode.\nThus, MuZero define $z_t$ as folowing:\n$$ z_t = u_{t+1} + \\gamma u_{t+2} + … + \\gamma^{n-1} u_{t+n} + \\gamma^n u_{t+n} $$\nThen, the loss function of MuZero is\n$$ l_t(\\theta) = \\sum_{k=0}^K l^{p}(\\pi_{t+k}, p_t^k) + \\sum_{k=0}^K l^v(z_{t+k}, v_t^k) + \\sum_{k=1}^K l^r(u_{t+k}, r_t^k) + c ||\\theta||^2 $$\nwhere $l^p$, $l^v$ and $l^r$ are loss functions for policy, value and reward, respectively. $c$ is a L2 regularization constant.\nExperiments \u0026 Results MuZero performs quite well both on board game and Atari 57.\nBoard Game\nAtari 57\n","wordCount":"697","inLanguage":"en","image":"https://frankccccc.github.io/blog/img/just_imgs/muzero.webp","datePublished":"2021-03-04T17:02:31+08:00","dateModified":"2021-03-04T17:02:31+08:00","author":{"@type":"Person","name":"SY Chou"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://frankccccc.github.io/blog/posts/part_iii-from_alphago_to_muzero/"},"publisher":{"@type":"Organization","name":"Golden Hat","logo":{"@type":"ImageObject","url":"https://frankccccc.github.io/blog/favicon.ico"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove('dark')</script><noscript><style type=text/css>#theme-toggle,.top-link{display:none}</style></noscript><header class=header><nav class=nav><div class=logo><a href=https://frankccccc.github.io/blog/ accesskey=h title="Golden Hat (Alt + H)"><img src=/blog/img/just_imgs/gold_empty_circle.svg alt=logo aria-label=logo height=35>Golden Hat</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu onscroll=menu_on_scroll()><li><a href=https://frankccccc.github.io/blog/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://frankccccc.github.io/blog/series title=Series><span>Series</span></a></li><li><a href=https://frankccccc.github.io/blog/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://frankccccc.github.io/blog/tags/ title=Tags><span>Tags</span></a></li></ul></nav><script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$'],['\\[','\\]']],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:['script','noscript','style','textarea','pre']}},window.addEventListener('load',a=>{document.querySelectorAll("mjx-container").forEach(function(a){a.parentElement.classList+='has-jax'})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://frankccccc.github.io/blog/>Home</a>&nbsp;»&nbsp;<a href=https://frankccccc.github.io/blog/posts/>Posts</a></div><h1 class=post-title>Part III - From AlphaGo to MuZero<div class=entry-isdraft><sup>&nbsp;&nbsp;[draft]</sup></div></h1><div class=post-meta>March 4, 2021&nbsp;·&nbsp;4 min&nbsp;·&nbsp;SY Chou</div></header><figure class=entry-cover><img src=https://frankccccc.github.io/blog/img/just_imgs/muzero.webp alt></figure><div class=post-content><h2 id=mastering-atari-go-chess-and-shogi-by-planning-with-a-learned-model>Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model<a hidden class=anchor aria-hidden=true href=#mastering-atari-go-chess-and-shogi-by-planning-with-a-learned-model>#</a></h2><p>It is just the paper proposing <strong>MuZero</strong>. MuZero is quite famous when I write this note(Jan 2021). Lots of people tried to reproduce the incredible performance of the paper. Some of well-known implementations like <a href=https://github.com/werner-duvaud/muzero-general>muzero-general</a> give a clear and modular implementation of MuZero. If you are interested in MuZero, you can play with it. Well, let&rsquo;s diving into the paper.</p><h3 id=introdution>Introdution<a hidden class=anchor aria-hidden=true href=#introdution>#</a></h3><p>The main idea of MuZero is to predict the future that are directly relevant for planning. The <strong>model receives the observation (e.g. an image of the Go board or the Atari screen)</strong> as an input and transforms it into a hidden state. The <strong>hidden state</strong> is then <strong>updated iteratively by a recurrent process</strong> that receives the previous hidden state and a hypothetical next action. At every one of these steps the model predicts the <strong>policy</strong> (e.g. the move to play), <strong>value function</strong> (e.g. the predicted winner), and <strong>immediate reward</strong> (e.g. the points scored by playing a move).</p><h3 id=algorithm>Algorithm<a hidden class=anchor aria-hidden=true href=#algorithm>#</a></h3><p><img src=/blog/img/alphago_to_muzero/muzero/muzero_algo.png alt></p><p>The MuZero consist of 3 components: dynamic function, prediction function, representation function:</p><ul><li><p><strong>Dynamic Function</strong> $g(s^t, a^{t+1})=r^{t+1}, s^{t+1}$</p></li><li><p><strong>Prediction Function</strong> $f(s^t)=p^t, v^t$</p></li><li><p><strong>Representation Function</strong> $h(o^0)=s^0$</p></li></ul><p>We denote $o^0$ as the initial observation, $s^0$ as the initial hidden state, $p^t$ as the policy function, $v^t$ as value function, and $r^t$ as reward function at time step $t$. These 3 components compose the <strong>deterministic</strong> latent dynamic model for MuZero. (The paper says stochastic transitions is left for future works)</p><p>The MuZero plan like Figure1 part A. Given a previous hidden state $s^{k−1}$ and a candidate action $a^k$, the dynamics function $g$ produces an immediate reward $r^k$ and a new hidden state $s^k$. The policy $p^k$ and value function $v^k$ are computed from the hidden state $s^k$ by a prediction function $f$.</p><p>The MuZero act in the environment like part B of Figure1. A Monte-Carlo Tree Search is performed at each timestep $t$, as described in A. An action $a_{t+1}$ is sampled from the search policy $\pi_t$. The environment receives the action and generates a new observation $o_{t+1}$ and reward $u_{t+1}$. At the end of the episode the trajectory data is stored into a replay buffer.</p><p>The MuZero model $\mu_{\theta}$ with parameters $\theta$, conditioned on past observations $o_1, &mldr;, o_t$ and future actions $a_{t+1}, &mldr;, a_{t+k}$.</p><p>The model always predicts the policy $p_t^k$, value function $v_t^k$, and immdiate reward $r_t^k$ after $k$ time steps at timestep $t$. For more detailed, <strong>the three future quantities</strong> are as following:</p><ul><li><p><strong>Policy</strong></p><p>$$
p_t^k \approx \pi (a_{t+k+1} | o_{1}, &mldr;, o_t, a_{t+1}, &mldr;, a_{t+k})
$$</p><p>where $\pi$ is the policy used to select real actions</p></li><li><p><strong>Value Function</strong></p><p>$$
v_t^k \approx E[u_{t+k+1} + \gamma \ u_{t+k+2} + &mldr; | o_1 , &mldr;, o_t, a_{t+1}, &mldr;, a_{t+k}]
$$</p><p>where $u$ is the true, observation reward. $\gamma$ is the discount factor of the environment.</p></li><li><p><strong>Immediate Reward</strong></p><p>$$
r_t^k \approx u_{t + k}
$$</p></li></ul><p>The MuZero train in the environment like part C of Figure1.<br>All parameters of the model are <strong>trained jointly to accurately match the policy, value, and reward, for every hypothetical step $k$, to corresponding target values observed after $k$ actual time-steps <em><strong>t</strong></em> have elapsed.(That is predict the policy, value, and reward after $k$ steps from current time-step $t$.)</strong> The training objective is to <strong>minimise the error between predicted policy</strong> $p_t^k$ and <strong>MCTS search policy</strong> $\pi_{t+k}$.</p><p>For trade-off between accuracy and stability, we allow for long episodes with discounting and intermediate rewards by bootstrapping $n$ steps into the future from the search value. Final outcomes {lose, draw, win} in board games are treated as rewards $u_t \in \{ −1, 0, +1 \}$ occurring at the final step of the episode.</p><p>Thus, MuZero define $z_t$ as folowing:</p><p><img src=/blog/img/alphago_to_muzero/muzero/muzero_def_z.png alt></p><p>$$
z_t = u_{t+1} + \gamma u_{t+2} + &mldr; + \gamma^{n-1} u_{t+n} + \gamma^n u_{t+n}
$$</p><p>Then, the loss function of MuZero is</p><p><img src=/blog/img/alphago_to_muzero/muzero/muzero_loss.png alt></p><p>$$
l_t(\theta) = \sum_{k=0}^K l^{p}(\pi_{t+k}, p_t^k) + \sum_{k=0}^K l^v(z_{t+k}, v_t^k) + \sum_{k=1}^K l^r(u_{t+k}, r_t^k) + c ||\theta||^2
$$</p><p>where $l^p$, $l^v$ and $l^r$ are loss functions for policy, value and reward, respectively. $c$ is a L2 regularization constant.</p><h3 id=experiments--results>Experiments & Results<a hidden class=anchor aria-hidden=true href=#experiments--results>#</a></h3><p>MuZero performs quite well both on board game and Atari 57.</p><p><strong>Board Game</strong></p><p><img src=/blog/img/alphago_to_muzero/muzero/muzero_board_exp.png alt></p><p><strong>Atari 57</strong></p><p><img src=/blog/img/alphago_to_muzero/muzero/muzero_atari_exp.png alt></p><p><img src=/blog/img/alphago_to_muzero/muzero/muzero_atari_exp2.png alt></p></div><footer class=post-footer><ul class=post-tags><li><a href=https://frankccccc.github.io/blog/tags/rl/>RL</a></li><li><a href=https://frankccccc.github.io/blog/tags/deep-learning/>deep learning</a></li><li><a href=https://frankccccc.github.io/blog/tags/model-based-rl/>model-based RL</a></li></ul><nav class=paginav><a class=next href=https://frankccccc.github.io/blog/posts/part_ii-from_alphago_to_muzero/><span class=title>Next Page »</span><br><span>Part II - From AlphaGo to MuZero</span></a></nav></footer><div style=padding-top:2.5rem;padding-bottom:2.5rem;text-align:left><h1 style=padding-bottom:.5rem>COMMENTS</h1><h5>Your comments will encouage me to share more~~</h5></div><div id=utter-container></div><script src=https://utteranc.es/client.js repo=frankccccc/blog issue-term=title theme=photon-dark crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2021 <a href=https://frankccccc.github.io/blog/>Golden Hat</a></span>
<span>&#183;</span>
<span>Powered by <a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span>
<span>&#183;</span>
<span>Theme <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)"><button class=top-link id=top-link type=button accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></button></a>
<script defer src=/blog/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5+kdJvBz5iKbt6B5PJI=" onload=hljs.initHighlightingOnLoad()></script><script>window.onload=function(){localStorage.getItem("menu-scroll-position")&&(document.getElementById('menu').scrollLeft=localStorage.getItem("menu-scroll-position"))},document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})});var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")};function menu_on_scroll(){localStorage.setItem("menu-scroll-position",document.getElementById('menu').scrollLeft)}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script></body></html>