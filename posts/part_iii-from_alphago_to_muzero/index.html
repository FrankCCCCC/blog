<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Part III - From AlphaGo to MuZero | Golden Hat</title><meta name=keywords content="AlphaGo,AlphaGo Zero,AlphaZero,MuZero,model-based RL,reinforcement learning,RL,deep reinforcement learning,DRL,deep learning,DL,DeepMind"><meta name=description content="A paper review of Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model and an introduction of MuZero"><meta name=author content="SY Chou"><link rel=canonical href=https://frankccccc.github.io/blog/posts/part_iii-from_alphago_to_muzero/><script async src="https://www.googletagmanager.com/gtag/js?id=G-S9MCZ2NDS7"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-S9MCZ2NDS7")</script><link href=/blog/assets/css/stylesheet.min.3768d020b7a9fd49f21ddb7d521ce9b187ce47c2929b883041396159d4d25553.css integrity="sha256-N2jQILep/UnyHdt9UhzpsYfOR8KSm4gwQTlhWdTSVVM=" rel="preload stylesheet" as=style><link rel=icon href=https://frankccccc.github.io/blog/img/just_imgs/gold_empty_circle.svg><link rel=icon type=image/png sizes=16x16 href=https://frankccccc.github.io/blog/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://frankccccc.github.io/blog/favicon-32x32.png><link rel=apple-touch-icon href=https://frankccccc.github.io/blog/apple-touch-icon.png><link rel=mask-icon href=https://frankccccc.github.io/blog/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><meta name=generator content="Hugo 0.101.0"><script async src="https://www.googletagmanager.com/gtag/js?id=G-S9MCZ2NDS7"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-S9MCZ2NDS7")</script><meta property="og:title" content="Part III - From AlphaGo to MuZero"><meta property="og:description" content="A paper review of Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model and an introduction of MuZero"><meta property="og:type" content="article"><meta property="og:url" content="https://frankccccc.github.io/blog/posts/part_iii-from_alphago_to_muzero/"><meta property="og:image" content="https://frankccccc.github.io/blog/img/just_imgs/muzero.webp"><meta property="article:published_time" content="2021-03-04T17:02:31+08:00"><meta property="article:modified_time" content="2021-03-04T17:02:31+08:00"><meta property="og:see_also" content="https://frankccccc.github.io/blog/posts/part_ii-from_alphago_to_muzero/"><meta property="og:see_also" content="https://frankccccc.github.io/blog/posts/part_i-from_alphago_to_muzero/"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://frankccccc.github.io/blog/img/just_imgs/muzero.webp"><meta name=twitter:title content="Part III - From AlphaGo to MuZero"><meta name=twitter:description content="A paper review of Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model and an introduction of MuZero"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://frankccccc.github.io/blog/posts/"},{"@type":"ListItem","position":2,"name":"Part III - From AlphaGo to MuZero","item":"https://frankccccc.github.io/blog/posts/part_iii-from_alphago_to_muzero/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Part III - From AlphaGo to MuZero","name":"Part III - From AlphaGo to MuZero","description":"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model It is just the paper proposing MuZero. MuZero is quite famous when I write this note(Jan 2021). Lots of people …","keywords":["AlphaGo","AlphaGo Zero","AlphaZero","MuZero","model-based RL","reinforcement learning","RL","deep reinforcement learning","DRL","deep learning","DL","DeepMind"],"articleBody":"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model It is just the paper proposing MuZero. MuZero is quite famous when I write this note(Jan 2021). Lots of people tried to reproduce the incredible performance of the paper. Some of well-known implementations like muzero-general give a clear and modular implementation of MuZero. If you are interested in MuZero, you can play with it. Well, let’s diving into the paper.\nIntrodution The main idea of MuZero is to predict the future that are directly relevant for planning. The model receives the observation (e.g. an image of the Go board or the Atari screen) as an input and transforms it into a hidden state. The hidden state is then updated iteratively by a recurrent process that receives the previous hidden state and a hypothetical next action. At every one of these steps the model predicts the policy (e.g. the move to play), value function (e.g. the predicted winner), and immediate reward (e.g. the points scored by playing a move).\nAlgorithm Components The MuZero consist of 3 components: dynamic function, prediction function, representation function:\nDynamic Function $g(s^t, a^{t+1})=r^{t+1}, s^{t+1}$\nPrediction Function $f(s^t)=p^t, v^t$\nRepresentation Function $h(o^0)=s^0$\nWe denote $o^0$ as the initial observation, $s^0$ as the initial hidden state, $p^t$ as the policy function, $v^t$ as value function, and $r^t$ as reward function at time step $t$. These 3 components compose the deterministic latent dynamic model for MuZero. (The paper says stochastic transitions is left for future works)\nThe MuZero plan like Figure1 part A. Given a previous hidden state $s^{k−1}$ and a candidate action $a^k$, the dynamics function $g$ produces an immediate reward $r^k$ and a new hidden state $s^k$. The policy $p^k$ and value function $v^k$ are computed from the hidden state $s^k$ by a prediction function $f$.\nActing The MuZero act in the environment like part B of Figure1. A Monte-Carlo Tree Search is performed at each timestep $t$, as described in A. An action $a_{t+1}$ is sampled from the search policy $\\pi_t$. The environment receives the action and generates a new observation $o_{t+1}$ and reward $u_{t+1}$. At the end of the episode the trajectory data is stored into a replay buffer.\nThe MuZero model $\\mu_{\\theta}$ with parameters $\\theta$, conditioned on past observations $o_1, …, o_t$ and future actions $a_{t+1}, …, a_{t+k}$.\nThe model always predicts the policy $p_t^k$, value function $v_t^k$, and immdiate reward $r_t^k$ after $k$ time steps at timestep $t$. For more detailed, the three future quantities are as following:\nPolicy\n$$ p_t^k \\approx \\pi (a_{t+k+1} | o_{1}, …, o_t, a_{t+1}, …, a_{t+k}) $$\nwhere $\\pi$ is the policy used to select real actions\nValue Function\n$$ v_t^k \\approx E[u_{t+k+1} + \\gamma \\ u_{t+k+2} + … | o_1 , …, o_t, a_{t+1}, …, a_{t+k}] $$\nwhere $u$ is the true, observation reward. $\\gamma$ is the discount factor of the environment.\nImmediate Reward\n$$ r_t^k \\approx u_{t + k} $$\nTraining The MuZero train in the environment like part C of Figure1.\nAll parameters of the model are trained jointly to accurately match the policy, value, and reward, for every hypothetical step $k$, to corresponding target values observed after $k$ actual time-steps t have elapsed.(That is predict the policy, value, and reward after $k$ steps from current time-step $t$.) The training objective is to minimise the error between predicted policy $p_t^k$ and MCTS search policy $\\pi_{t+k}$.\nFor trade-off between accuracy and stability, we allow for long episodes with discounting and intermediate rewards by bootstrapping $n$ steps into the future from the search value. Final outcomes {lose, draw, win} in board games are treated as rewards $u_t \\in \\{ −1, 0, +1 \\}$ occurring at the final step of the episode.\nThus, MuZero define $z_t$ as folowing:\n$$ z_t = u_{t+1} + \\gamma u_{t+2} + … + \\gamma^{n-1} u_{t+n} + \\gamma^n u_{t+n} $$\nThen, the loss function of MuZero is\n$$ l_t(\\theta) = \\sum_{k=0}^K l^{p}(\\pi_{t+k}, p_t^k) + \\sum_{k=0}^K l^v(z_{t+k}, v_t^k) + \\sum_{k=1}^K l^r(u_{t+k}, r_t^k) + c ||\\theta||^2 $$\nwhere $l^p$, $l^v$ and $l^r$ are loss functions for policy, value and reward, respectively. $c$ is a L2 regularization constant.\nExperiments \u0026 Results MuZero performs quite well both on board game and Atari 57.\nBoard Game Atari 57 ","wordCount":"700","inLanguage":"en","image":"https://frankccccc.github.io/blog/img/just_imgs/muzero.webp","datePublished":"2021-03-04T17:02:31+08:00","dateModified":"2021-03-04T17:02:31+08:00","author":{"@type":"Person","name":"SY Chou"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://frankccccc.github.io/blog/posts/part_iii-from_alphago_to_muzero/"},"publisher":{"@type":"Organization","name":"Golden Hat","logo":{"@type":"ImageObject","url":"https://frankccccc.github.io/blog/img/just_imgs/gold_empty_circle.svg"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><noscript><style type=text/css>#theme-toggle,.top-link{display:none}</style></noscript><header class=header><nav class=nav><div class=logo><a href=https://frankccccc.github.io/blog/ accesskey=h title="Golden Hat (Alt + H)"><img src=/blog/img/just_imgs/gold_empty_circle.svg alt=logo aria-label=logo height=35>Golden Hat</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu onscroll=menu_on_scroll()><li><a href=https://frankccccc.github.io/blog/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://frankccccc.github.io/blog/series title=Series><span>Series</span></a></li><li><a href=https://frankccccc.github.io/blog/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://frankccccc.github.io/blog/tags/ title=Tags><span>Tags</span></a></li></ul></nav><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://frankccccc.github.io/blog/>Home</a>&nbsp;»&nbsp;<a href=https://frankccccc.github.io/blog/posts/>Posts</a></div><h1 class=post-title>Part III - From AlphaGo to MuZero<div class=entry-isdraft><sup>&nbsp;&nbsp;[draft]</sup></div></h1><div class=post-description>A paper review of Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model and an introduction of MuZero</div><div class=post-meta>March 4, 2021&nbsp;·&nbsp;4 min&nbsp;·&nbsp;SY Chou</div></header><figure class=entry-cover><img src=https://frankccccc.github.io/blog/img/just_imgs/muzero.webp alt></figure><div class=post-content><h1 id=mastering-atari-go-chess-and-shogi-by-planning-with-a-learned-model>Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model<a hidden class=anchor aria-hidden=true href=#mastering-atari-go-chess-and-shogi-by-planning-with-a-learned-model>#</a></h1><p>It is just the paper proposing <strong>MuZero</strong>. MuZero is quite famous when I write this note(Jan 2021). Lots of people tried to reproduce the incredible performance of the paper. Some of well-known implementations like <a href=https://github.com/werner-duvaud/muzero-general>muzero-general</a> give a clear and modular implementation of MuZero. If you are interested in MuZero, you can play with it. Well, let&rsquo;s diving into the paper.</p><h2 id=introdution>Introdution<a hidden class=anchor aria-hidden=true href=#introdution>#</a></h2><p>The main idea of MuZero is to predict the future that are directly relevant for planning. The <strong>model receives the observation (e.g. an image of the Go board or the Atari screen)</strong> as an input and transforms it into a hidden state. The <strong>hidden state</strong> is then <strong>updated iteratively by a recurrent process</strong> that receives the previous hidden state and a hypothetical next action. At every one of these steps the model predicts the <strong>policy</strong> (e.g. the move to play), <strong>value function</strong> (e.g. the predicted winner), and <strong>immediate reward</strong> (e.g. the points scored by playing a move).</p><h2 id=algorithm>Algorithm<a hidden class=anchor aria-hidden=true href=#algorithm>#</a></h2><p><img src=/blog/img/alphago_to_muzero/muzero/muzero_algo.png alt></p><h3 id=components>Components<a hidden class=anchor aria-hidden=true href=#components>#</a></h3><p>The MuZero consist of 3 components: dynamic function, prediction function, representation function:</p><ul><li><p><strong>Dynamic Function</strong> $g(s^t, a^{t+1})=r^{t+1}, s^{t+1}$</p></li><li><p><strong>Prediction Function</strong> $f(s^t)=p^t, v^t$</p></li><li><p><strong>Representation Function</strong> $h(o^0)=s^0$</p></li></ul><p>We denote $o^0$ as the initial observation, $s^0$ as the initial hidden state, $p^t$ as the policy function, $v^t$ as value function, and $r^t$ as reward function at time step $t$. These 3 components compose the <strong>deterministic</strong> latent dynamic model for MuZero. (The paper says stochastic transitions is left for future works)</p><p>The MuZero plan like Figure1 part A. Given a previous hidden state $s^{k−1}$ and a candidate action $a^k$, the dynamics function $g$ produces an immediate reward $r^k$ and a new hidden state $s^k$. The policy $p^k$ and value function $v^k$ are computed from the hidden state $s^k$ by a prediction function $f$.</p><h3 id=acting>Acting<a hidden class=anchor aria-hidden=true href=#acting>#</a></h3><p>The MuZero act in the environment like part B of Figure1. A Monte-Carlo Tree Search is performed at each timestep $t$, as described in A. An action $a_{t+1}$ is sampled from the search policy $\pi_t$. The environment receives the action and generates a new observation $o_{t+1}$ and reward $u_{t+1}$. At the end of the episode the trajectory data is stored into a replay buffer.</p><p>The MuZero model $\mu_{\theta}$ with parameters $\theta$, conditioned on past observations $o_1, &mldr;, o_t$ and future actions $a_{t+1}, &mldr;, a_{t+k}$.</p><p>The model always predicts the policy $p_t^k$, value function $v_t^k$, and immdiate reward $r_t^k$ after $k$ time steps at timestep $t$. For more detailed, <strong>the three future quantities</strong> are as following:</p><ul><li><p><strong>Policy</strong></p><p>$$
p_t^k \approx \pi (a_{t+k+1} | o_{1}, &mldr;, o_t, a_{t+1}, &mldr;, a_{t+k})
$$</p><p>where $\pi$ is the policy used to select real actions</p></li><li><p><strong>Value Function</strong></p><p>$$
v_t^k \approx E[u_{t+k+1} + \gamma \ u_{t+k+2} + &mldr; | o_1 , &mldr;, o_t, a_{t+1}, &mldr;, a_{t+k}]
$$</p><p>where $u$ is the true, observation reward. $\gamma$ is the discount factor of the environment.</p></li><li><p><strong>Immediate Reward</strong></p><p>$$
r_t^k \approx u_{t + k}
$$</p></li></ul><h3 id=training>Training<a hidden class=anchor aria-hidden=true href=#training>#</a></h3><p>The MuZero train in the environment like part C of Figure1.<br>All parameters of the model are <strong>trained jointly to accurately match the policy, value, and reward, for every hypothetical step $k$, to corresponding target values observed after $k$ actual time-steps <em><strong>t</strong></em> have elapsed.(That is predict the policy, value, and reward after $k$ steps from current time-step $t$.)</strong> The training objective is to <strong>minimise the error between predicted policy</strong> $p_t^k$ and <strong>MCTS search policy</strong> $\pi_{t+k}$.</p><p>For trade-off between accuracy and stability, we allow for long episodes with discounting and intermediate rewards by bootstrapping $n$ steps into the future from the search value. Final outcomes {lose, draw, win} in board games are treated as rewards $u_t \in \{ −1, 0, +1 \}$ occurring at the final step of the episode.</p><p>Thus, MuZero define $z_t$ as folowing:</p><p>$$
z_t = u_{t+1} + \gamma u_{t+2} + &mldr; + \gamma^{n-1} u_{t+n} + \gamma^n u_{t+n}
$$</p><p>Then, the loss function of MuZero is</p><p>$$
l_t(\theta) = \sum_{k=0}^K l^{p}(\pi_{t+k}, p_t^k) + \sum_{k=0}^K l^v(z_{t+k}, v_t^k) + \sum_{k=1}^K l^r(u_{t+k}, r_t^k) + c ||\theta||^2
$$</p><p>where $l^p$, $l^v$ and $l^r$ are loss functions for policy, value and reward, respectively. $c$ is a L2 regularization constant.</p><h2 id=experiments--results>Experiments & Results<a hidden class=anchor aria-hidden=true href=#experiments--results>#</a></h2><p>MuZero performs quite well both on board game and Atari 57.</p><h3 id=board-game>Board Game<a hidden class=anchor aria-hidden=true href=#board-game>#</a></h3><p><img src=/blog/img/alphago_to_muzero/muzero/muzero_board_exp.png alt></p><h3 id=atari-57>Atari 57<a hidden class=anchor aria-hidden=true href=#atari-57>#</a></h3><p><img src=/blog/img/alphago_to_muzero/muzero/muzero_atari_exp.png alt></p><p><img src=/blog/img/alphago_to_muzero/muzero/muzero_atari_exp2.png alt></p></div><footer class=post-footer><ul class=post-tags><li><a href=https://frankccccc.github.io/blog/tags/rl/>RL</a></li><li><a href=https://frankccccc.github.io/blog/tags/deep-learning/>deep learning</a></li><li><a href=https://frankccccc.github.io/blog/tags/model-based-rl/>model-based RL</a></li></ul><nav class=paginav><a class=prev href=https://frankccccc.github.io/blog/posts/part_i_toward_nngp_and_ntk/><span class=title>« Prev Page</span><br><span>Part I - Toward NNGP and NTK</span></a>
<a class=next href=https://frankccccc.github.io/blog/posts/part_ii-from_alphago_to_muzero/><span class=title>Next Page »</span><br><span>Part II - From AlphaGo to MuZero</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Part III - From AlphaGo to MuZero on twitter" href="https://twitter.com/intent/tweet/?text=Part%20III%20-%20From%20AlphaGo%20to%20MuZero&url=https%3a%2f%2ffrankccccc.github.io%2fblog%2fposts%2fpart_iii-from_alphago_to_muzero%2f&hashtags=RL%2cdeeplearning%2cmodel-basedRL"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Part III - From AlphaGo to MuZero on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2ffrankccccc.github.io%2fblog%2fposts%2fpart_iii-from_alphago_to_muzero%2f&title=Part%20III%20-%20From%20AlphaGo%20to%20MuZero&summary=Part%20III%20-%20From%20AlphaGo%20to%20MuZero&source=https%3a%2f%2ffrankccccc.github.io%2fblog%2fposts%2fpart_iii-from_alphago_to_muzero%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Part III - From AlphaGo to MuZero on reddit" href="https://reddit.com/submit?url=https%3a%2f%2ffrankccccc.github.io%2fblog%2fposts%2fpart_iii-from_alphago_to_muzero%2f&title=Part%20III%20-%20From%20AlphaGo%20to%20MuZero"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Part III - From AlphaGo to MuZero on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2ffrankccccc.github.io%2fblog%2fposts%2fpart_iii-from_alphago_to_muzero%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Part III - From AlphaGo to MuZero on whatsapp" href="https://api.whatsapp.com/send?text=Part%20III%20-%20From%20AlphaGo%20to%20MuZero%20-%20https%3a%2f%2ffrankccccc.github.io%2fblog%2fposts%2fpart_iii-from_alphago_to_muzero%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Part III - From AlphaGo to MuZero on telegram" href="https://telegram.me/share/url?text=Part%20III%20-%20From%20AlphaGo%20to%20MuZero&url=https%3a%2f%2ffrankccccc.github.io%2fblog%2fposts%2fpart_iii-from_alphago_to_muzero%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer><div style=padding-top:2.5rem;padding-bottom:2.5rem;text-align:left><h1 style=padding-bottom:.5rem>COMMENTS</h1><h5>Your comments will encouage me to share more~~</h5></div><div id=utter-container></div><script src=https://utteranc.es/client.js repo=frankccccc/blog issue-term=title theme=photon-dark crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2022 <a href=https://frankccccc.github.io/blog/>Golden Hat</a></span>
<span>&#183;</span>
<span>Powered by <a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span>
<span>&#183;</span>
<span>Theme <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)"><button class=top-link id=top-link type=button accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></button></a>
<script defer src=/blog/assets/js/highlight.min.e85ad0406048e8176e1c7661b25d5c69297ddfe41dc4124cf75ecb99a4f7b3d1.js integrity="sha256-6FrQQGBI6BduHHZhsl1caSl93+QdxBJM917LmaT3s9E=" onload=hljs.initHighlightingOnLoad()></script>
<script>window.onload=function(){localStorage.getItem("menu-scroll-position")&&(document.getElementById("menu").scrollLeft=localStorage.getItem("menu-scroll-position"))},document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})});var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")};function menu_on_scroll(){localStorage.setItem("menu-scroll-position",document.getElementById("menu").scrollLeft)}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>