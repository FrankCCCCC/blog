<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Posts | Golden Hat</title><meta name=keywords content><meta name=description content="Machine Learning, Programming and, Murmuring"><meta name=author content="SY Chou"><link rel=canonical href=https://frankccccc.github.io/blog/posts/><script async src="https://www.googletagmanager.com/gtag/js?id=G-S9MCZ2NDS7"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','G-S9MCZ2NDS7')</script><link href=/blog/assets/css/stylesheet.min.3768d020b7a9fd49f21ddb7d521ce9b187ce47c2929b883041396159d4d25553.css integrity="sha256-N2jQILep/UnyHdt9UhzpsYfOR8KSm4gwQTlhWdTSVVM=" rel="preload stylesheet" as=style><link rel=icon href=https://frankccccc.github.io/blog/img/just_imgs/gold_empty_circle.svg><link rel=icon type=image/png sizes=16x16 href=https://frankccccc.github.io/blog/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://frankccccc.github.io/blog/favicon-32x32.png><link rel=apple-touch-icon href=https://frankccccc.github.io/blog/apple-touch-icon.png><link rel=mask-icon href=https://frankccccc.github.io/blog/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><meta name=generator content="Hugo 0.85.0"><link rel=alternate type=application/rss+xml href=https://frankccccc.github.io/blog/posts/index.xml><script async src="https://www.googletagmanager.com/gtag/js?id=G-S9MCZ2NDS7"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','G-S9MCZ2NDS7')</script><meta property="og:title" content="Posts"><meta property="og:description" content="Machine Learning, Programming and, Murmuring"><meta property="og:type" content="website"><meta property="og:url" content="https://frankccccc.github.io/blog/posts/"><meta property="og:updated_time" content="2021-07-09T19:37:43+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Posts"><meta name=twitter:description content="Machine Learning, Programming and, Murmuring"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://frankccccc.github.io/blog/posts/"}]}</script></head><body class="list dark" id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove('dark')</script><noscript><style type=text/css>#theme-toggle,.top-link{display:none}</style></noscript><header class=header><nav class=nav><div class=logo><a href=https://frankccccc.github.io/blog/ accesskey=h title="Golden Hat (Alt + H)"><img src=/blog/img/just_imgs/gold_empty_circle.svg alt=logo aria-label=logo height=35>Golden Hat</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu onscroll=menu_on_scroll()><li><a href=https://frankccccc.github.io/blog/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://frankccccc.github.io/blog/series title=Series><span>Series</span></a></li><li><a href=https://frankccccc.github.io/blog/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://frankccccc.github.io/blog/tags/ title=Tags><span>Tags</span></a></li></ul></nav><script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$'],['\\[','\\]']],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:['script','noscript','style','textarea','pre']}},window.addEventListener('load',a=>{document.querySelectorAll("mjx-container").forEach(function(a){a.parentElement.classList+='has-jax'})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></header><main class=main><header class=page-header><div class=breadcrumbs><a href=https://frankccccc.github.io/blog/>Home</a></div><h1>Posts</h1></header><article class=post-entry><figure class=entry-cover><img src=https://frankccccc.github.io/blog/img/just_imgs/arch.jpg alt></figure><header class=entry-header><h2>From EM To VBEM</h2></header><section class=entry-content><p>1. Introduction When we use K-Means or GMM to solve clustering problem, the most important hyperparameter is the number of the cluster. It is quite hard to decide and cause the good/bad performance significantly. In the mean time, K-Means also cannot handle unbalanced dataset well. However, the variational Bayesian Gaussian mixture model(VB-GMM) can solve these. VB-GMM is a Bayesian model that contains priors over the parameters of GMM. Thus, VB-GMM can be optimized by variational Bayesian expectation maximization(VBEM) and find the optimal cluster number automatically....</p></section><footer class=entry-footer>July 9, 2021&nbsp;·&nbsp;6 min&nbsp;·&nbsp;SY Chou</footer><a class=entry-link aria-label="post link to From EM To VBEM" href=https://frankccccc.github.io/blog/posts/from_em_to_vbem/></a></article><article class=post-entry><figure class=entry-cover><img src=https://frankccccc.github.io/blog/img/just_imgs/snow_forest.jpg alt></figure><header class=entry-header><h2>A Review of SVM and SMO</h2></header><section class=entry-content><p>Note: full code is on my github.
1. Abstract In this article, I will derive SMO algorithm and the Fourier kernel approximation which are well-known algorithm for kernel machine. SMO can solve optimization problem of SVM efficiently and the Fourier kernel approximation is a kind of kernel approximation that can speed up the computation of the kernel matrix. In the last section, I will conduct a evaluation of my manual SVM on the simulation dataset and “Women’s Clothing E-Commerce Review Dataset”....</p></section><footer class=entry-footer>July 8, 2021&nbsp;·&nbsp;16 min&nbsp;·&nbsp;SY Chou</footer><a class=entry-link aria-label="post link to A Review of SVM and SMO" href=https://frankccccc.github.io/blog/posts/smo/></a></article><article class=post-entry><figure class=entry-cover><img src=https://frankccccc.github.io/blog/img/just_imgs/wave_process.jpg alt></figure><header class=entry-header><h2>Part II - Toward NNGP and NTK</h2></header><section class=entry-content><p>Neural Tangent Kernel(NTK)
“In short, NTK represent the changes of the weights before and after the gradient descent update” Let’s start the journey of revealing the black-box neural networks.
Setup a Neural Network First of all, we need to define a simple neural network with 2 hidden layers
$$ y(x, w)$$
where $y$ is the neural network with weights $w \in \mathbb{R}^m$ and, ${ x, \bar{y} }_N$ is the dataset which is a set of the input data and the output data with $N$ data points....</p></section><footer class=entry-footer>February 19, 2021&nbsp;·&nbsp;10 min&nbsp;·&nbsp;SY Chou</footer><a class=entry-link aria-label="post link to Part II - Toward NNGP and NTK" href=https://frankccccc.github.io/blog/posts/toward_nngp_and_ntk/></a></article><article class=post-entry><figure class=entry-cover><img src=https://frankccccc.github.io/blog/img/just_imgs/ny_skyline.jpg alt></figure><header class=entry-header><h2>A Very Brief Introduction to Gaussian Process and Bayesian Optimization</h2></header><section class=entry-content><p>Gaussian Process Big Picture and Background Intuitively, Gaussian distribution define the state space, while Gaussian Process define the function space
Before we introduce Gaussian process, we should understand Gaussian distriution at first. For a RV(random variable) $X$ that follow Gaussian Distribution $\mathcal{N}(0, 1)$ should be following image:
The P.D.F should be
$$x \sim \mathcal{N}(\mu, \sigma) = \frac{1}{\sigma \sqrt{2 \pi}} e^{- \frac{1}{2} (\frac{- \mu}{\sigma})^2}$$
As for Multivariate Gaussian Distribution, given 2 RV $x$, $y$ both 2 RV follow Gaussian Distribution $\mathcal{N}(0, 1)$ we can illustrate it as...</p></section><footer class=entry-footer>February 16, 2021&nbsp;·&nbsp;12 min&nbsp;·&nbsp;SY Chou</footer><a class=entry-link aria-label="post link to A Very Brief Introduction to Gaussian Process and Bayesian Optimization" href=https://frankccccc.github.io/blog/posts/intro_gp_bayes_opt/></a></article><article class=post-entry><figure class=entry-cover><img src=https://frankccccc.github.io/blog/img/just_imgs/star_trail.jpg alt></figure><header class=entry-header><h2>A Set of Shannon Entropy</h2></header><section class=entry-content><p>Shannon Entropy For discrete random variable $X$ with events $\{ x_1, …, x_n \}$ and probability mass function $P(X)$, we defien the Shannon Entropy $H(X)$ as
$$H(X) = E[-log_b \ P(X)] = - \sum_{i = 1}^{i = n} \ P(x_i) log_b \ P(x_i)$$
where $b$ is the base of the logarithm. The unit of Shannon entropy is bit for $b = 2$ while nat for $b = e$
The Perspective of Venn Diagram We can illustrate the relation between joint entropy, conditional entropy, and mutual entropy as the following figure...</p></section><footer class=entry-footer>February 23, 2021&nbsp;·&nbsp;3 min&nbsp;·&nbsp;SY Chou</footer><a class=entry-link aria-label="post link to A Set of Shannon Entropy" href=https://frankccccc.github.io/blog/posts/a_set_of_shannon_entropy/></a></article><article class=post-entry><figure class=entry-cover><img src=https://frankccccc.github.io/blog/img/just_imgs/taipei3.jpg alt></figure><header class=entry-header><h2>部落格搬家記</h2></header><section class=entry-content><p>因為寫DL筆記時會用到大量數學符號，就索性把原先在Github上的DL_DB_Quick_Notes搬過來了，配合LATEX寫筆記順手很多，原先的Repo應該只會剩下收集Paper用。而最近生活上有些轉折，也許也會順便放些隨筆雜記，但就依心情而定。
目前用的主題是PaperMod，整體設計算令人滿意，只不過在Deploy Hugo遇到蠻多麻煩，這邊簡單記錄一下
設定Github Page Action 參考PaperMod ExampleSite的gh-pages.yml設定，自己再作一些修改，大致如下
name: Build GH-Pages on: push: paths-ignore: - 'images/**' - 'LICENSE' - 'README.md' branches: - master workflow_dispatch: # manual run jobs: deploy: runs-on: ubuntu-latest steps: - name: Git checkout uses: actions/checkout@v2 with: ref: master - name: Get Theme run: git submodule update --init --recursive - name: Update theme to Latest commit run: git submodule update --remote --merge - name: Setup hugo uses: peaceiris/actions-hugo@v2 with: hugo-version: 'latest' - name: Build run: hugo --buildDrafts --gc --verbose --minify - name: Deploy uses: peaceiris/actions-gh-pages@v3 with: github_token: ${{ secrets....</p></section><footer class=entry-footer>February 16, 2021&nbsp;·&nbsp;2 min&nbsp;·&nbsp;SY Chou</footer><a class=entry-link aria-label="post link to 部落格搬家記" href=https://frankccccc.github.io/blog/posts/move_blog/></a></article><article class=post-entry><figure class=entry-cover><img src=https://frankccccc.github.io/blog/img/just_imgs/underwater_ice.jpg alt></figure><header class=entry-header><h2>Toward VB-GMM<div class=entry-isdraft><sup>&nbsp;&nbsp;[draft]</sup></div></h2></header><section class=entry-content><p>3. Variational Bayesian Gaussian Mixture Model(VB-GMM) 3.1 Graphical Model Gaussian Mixture Model & Clustering
The variational Bayesian Gaussian mixture model(VB-GMM) can be represented as the above graphical model. We see each data point as a Gaussian mixture distribution with $K$ components. We also denote the number of data points as $N$. Each $x_n$ is a Gaussian mixture distribution with a weight $\pi_n$ corresponds to a data point. $z_n$ is an one-hot latent variable that indicates which cluster(component) does the data point belongs to....</p></section><footer class=entry-footer>July 9, 2021&nbsp;·&nbsp;11 min&nbsp;·&nbsp;SY Chou</footer><a class=entry-link aria-label="post link to Toward VB-GMM" href=https://frankccccc.github.io/blog/posts/toward_vbgmm/></a></article><article class=post-entry><figure class=entry-cover><img src=https://frankccccc.github.io/blog/img/just_imgs/grassland.jpg alt></figure><header class=entry-header><h2>A Paper Review: Learning to Adapt<div class=entry-isdraft><sup>&nbsp;&nbsp;[draft]</sup></div></h2></header><section class=entry-content><p>Introduciton Propose an efficient method for online adaptation. The algorithm efficiently trains a global model that is capable of using its recent experiences to quickly adapt, achieving fast online adaptation in dynamic environments.
They evaluate 2 version of approaches on stochastic continuous control tasks:
(1) Recurrence-Based Adaptive Learner (ReBAL)
(2) Gradient-Based Adaptive Learner (GrBAL)
Objective Setting-Up To adapt the dynamic environment, we require a learned model $p_{\theta}^*$ to adapt, using an update rule $u_{\psi}^*$ after seeing M data points from some new “task”....</p></section><footer class=entry-footer>March 15, 2021&nbsp;·&nbsp;2 min&nbsp;·&nbsp;SY Chou</footer><a class=entry-link aria-label="post link to A Paper Review: Learning to Adapt" href=https://frankccccc.github.io/blog/posts/a_paper_review_learning_to_adapt/></a></article><article class=post-entry><figure class=entry-cover><img src=https://frankccccc.github.io/blog/img/just_imgs/flow_sea.jpg alt></figure><header class=entry-header><h2>Part I - Toward NNGP and NTK<div class=entry-isdraft><sup>&nbsp;&nbsp;[draft]</sup></div></h2></header><section class=entry-content><p>Neural Network Gaussian Process(NNGP) Model the neural network as GP, aka neural network Gaussian Process(NNGP). Intuitively, the kernel of NNGP compute the distance between the output vectors of 2 input data points.
We define the following functions as neural networks with fully-conntected layers:
$$z_{i}^{1}(x) = b_i^{1} + \sum_{j=1}^{N_1} \ W_{ij}^{1}x_j^1(x), \ \ x_{j}^{1}(x) = \phi(b_i^{0} + \sum_{k=1}^{d_{in}} \ W_{ik}^{0}x_k(x))$$
where $b_i^{1}$ is the $i$th-bias of the second layer(the same as first hidden layer), $W_{ij}^{1}$ is the $i$th-weights of the first layer(the same as input layer) , function $\phi$ is the activation function, and $x$ is the input data of the neural network....</p></section><footer class=entry-footer>March 15, 2021&nbsp;·&nbsp;1 min&nbsp;·&nbsp;SY Chou</footer><a class=entry-link aria-label="post link to Part I - Toward NNGP and NTK" href=https://frankccccc.github.io/blog/posts/part_i_toward_nngp_and_ntk/></a></article><article class=post-entry><figure class=entry-cover><img src=https://frankccccc.github.io/blog/img/just_imgs/muzero.webp alt></figure><header class=entry-header><h2>Part III - From AlphaGo to MuZero<div class=entry-isdraft><sup>&nbsp;&nbsp;[draft]</sup></div></h2></header><section class=entry-content><p>Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model It is just the paper proposing MuZero. MuZero is quite famous when I write this note(Jan 2021). Lots of people tried to reproduce the incredible performance of the paper. Some of well-known implementations like muzero-general give a clear and modular implementation of MuZero. If you are interested in MuZero, you can play with it. Well, let’s diving into the paper....</p></section><footer class=entry-footer>March 4, 2021&nbsp;·&nbsp;4 min&nbsp;·&nbsp;SY Chou</footer><a class=entry-link aria-label="post link to Part III - From AlphaGo to MuZero" href=https://frankccccc.github.io/blog/posts/part_iii-from_alphago_to_muzero/></a></article><footer class=page-footer><nav class=pagination><a class=next href=https://frankccccc.github.io/blog/posts/page/2/>Next Page »</a></nav></footer></main><footer class=footer><span>&copy; 2021 <a href=https://frankccccc.github.io/blog/>Golden Hat</a></span>
<span>&#183;</span>
<span>Powered by <a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span>
<span>&#183;</span>
<span>Theme <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)"><button class=top-link id=top-link type=button accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></button></a>
<script>window.onload=function(){localStorage.getItem("menu-scroll-position")&&(document.getElementById('menu').scrollLeft=localStorage.getItem("menu-scroll-position"))},document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})});var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")};function menu_on_scroll(){localStorage.setItem("menu-scroll-position",document.getElementById('menu').scrollLeft)}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script></body></html>