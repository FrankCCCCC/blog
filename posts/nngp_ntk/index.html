<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Toward NNGP and NTK | Golden Hat</title><meta name=keywords content="gaussian process,deep learning,machine learning,bayes,NNGP,NTK"><meta name=description content="Neural Network Gaussian Process(NNGP) Neural Tangent Kernel(NTK) &ldquo;In short, NTK represent the changes of the weights before and after the gradient descent update&rdquo; Let&rsquo;s start the journey of revealing the black-box neural networks.
Setup a Neural Network First of all, we need to define a simple neural network with 2 hidden layers
$$ y(x, w)$$
where $y$ is the neural network with weights $w \in \mathbb{R}^m$ and, $\{ x, \bar{y} \}$ is the dataset which is a set of the input data and the output data with $N$ data points."><meta name=author content="SY Chou"><link rel=canonical href=https://frankccccc.github.io/blog/posts/nngp_ntk/><link href=https://frankccccc.github.io/blog/assets/css/stylesheet.min.f1bbfae023539c16ac553a8e32caeefc657000612d2bcc3d4cd998f504582f36.css integrity="sha256-8bv64CNTnBasVTqOMsru/GVwAGEtK8w9TNmY9QRYLzY=" rel="preload stylesheet" as=style><link rel=icon href=https://frankccccc.github.io/blog/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://frankccccc.github.io/blog/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://frankccccc.github.io/blog/favicon-32x32.png><link rel=apple-touch-icon href=https://frankccccc.github.io/blog/apple-touch-icon.png><link rel=mask-icon href=https://frankccccc.github.io/blog/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><meta name=generator content="Hugo 0.80.0"><meta property="og:title" content="Toward NNGP and NTK"><meta property="og:description" content="Neural Network Gaussian Process(NNGP) Neural Tangent Kernel(NTK) &ldquo;In short, NTK represent the changes of the weights before and after the gradient descent update&rdquo; Let&rsquo;s start the journey of revealing the black-box neural networks.
Setup a Neural Network First of all, we need to define a simple neural network with 2 hidden layers
$$ y(x, w)$$
where $y$ is the neural network with weights $w \in \mathbb{R}^m$ and, $\{ x, \bar{y} \}$ is the dataset which is a set of the input data and the output data with $N$ data points."><meta property="og:type" content="article"><meta property="og:url" content="https://frankccccc.github.io/blog/posts/nngp_ntk/"><meta property="og:image" content="https://frankccccc.github.io/blog/img/just_imgs/wave_process.jpg"><meta property="article:published_time" content="2021-02-19T20:46:29+08:00"><meta property="article:modified_time" content="2021-02-19T20:46:29+08:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://frankccccc.github.io/blog/img/just_imgs/wave_process.jpg"><meta name=twitter:title content="Toward NNGP and NTK"><meta name=twitter:description content="Neural Network Gaussian Process(NNGP) Neural Tangent Kernel(NTK) &ldquo;In short, NTK represent the changes of the weights before and after the gradient descent update&rdquo; Let&rsquo;s start the journey of revealing the black-box neural networks.
Setup a Neural Network First of all, we need to define a simple neural network with 2 hidden layers
$$ y(x, w)$$
where $y$ is the neural network with weights $w \in \mathbb{R}^m$ and, $\{ x, \bar{y} \}$ is the dataset which is a set of the input data and the output data with $N$ data points."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://frankccccc.github.io/blog/posts/"},{"@type":"ListItem","position":2,"name":"Toward NNGP and NTK","item":"https://frankccccc.github.io/blog/posts/nngp_ntk/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Toward NNGP and NTK","name":"Toward NNGP and NTK","description":"Neural Network Gaussian Process(NNGP) Neural Tangent Kernel(NTK) \u0026amp;ldquo;In short, NTK represent the changes of the weights before and after the gradient descent update\u0026amp;rdquo; …","keywords":["gaussian process","deep learning","machine learning","bayes","NNGP","NTK"],"articleBody":"Neural Network Gaussian Process(NNGP) Neural Tangent Kernel(NTK) “In short, NTK represent the changes of the weights before and after the gradient descent update” Let’s start the journey of revealing the black-box neural networks.\nSetup a Neural Network First of all, we need to define a simple neural network with 2 hidden layers\n$$ y(x, w)$$\nwhere $y$ is the neural network with weights $w \\in \\mathbb{R}^m$ and, $\\{ x, \\bar{y} \\}$ is the dataset which is a set of the input data and the output data with $N$ data points. Since we focus on analyze the weight $w$, we simplify the notation $y(x, w)$ as $y(w)$\nSuppose we have a regression task on the network $y(w)$, define\n$$L(w) = \\frac{1}{N} \\frac{1}{2} \\Vert y(w) - \\bar{y} \\Vert^2_2 $$\nwhere $L(w)$ is our object loss which we want to minimize. Since the term $\\frac{1}{N}$ is regardless to our goal, just ignore it and we get a simpler loss function\n$$L(w) = \\frac{1}{2} \\Vert y(w) - \\bar{y} \\Vert^2_2 $$\nTo The Limit of Infinite Width In order to measure the difference of the weights during training the neural network, we define a normalized metric as following\n$$\\frac{\\Vert w_n - w_0 \\Vert_2}{\\Vert w_0 \\Vert_2}$$\nwhere $w_n$ and $w_0$ are the weights at n-th training iteration and the initial weights. $\\Vert w_n - w_0 \\Vert_2$ means the quantity of the differnce between parameters $w_n$ and $w_0$ and it is normalized by the 2-norm $\\Vert w_0 \\Vert_2$\nAs we can see, the difference of the weights during training decrease as the width of network grows. As a result, the trained weights should be very close to the inital weights $w_0$ as the width of network goes to infinity.\nApply Taylor Expansion We’ve known the Taylor expansion is\n$$f(x) = \\Sigma_{n=0}^{\\infty} \\ \\frac{f^{n}(a)}{n!} (x - a)^{n}$$\nA function $f(w)$ expanded on the $w_0$ with first order approximation is\n$$f(w) \\approx f(w_0) + \\frac{df(w_0)}{dw} (w - w_0)$$\nIt is trivial that if $w$ is a vector, we need to replace the derivative $\\frac{df(w_0)}{dw}$ with gradient $\\nabla_{w} f(w_0)^{\\top}$\n$$f(w) \\approx f(w_0) + \\nabla_{w} f(w_0)^{\\top} \\ (w - w_0)$$\nApply to the network $y(w)$\n$$y(w) \\approx y(w_0) + \\nabla_{w} y(w_0)^{\\top} \\ (w - w_0)$$\nwhere $\\nabla_{w} y(w_0)$ and $y(w_0)$ are constants. Thus, the Taylor expansion of **$y(w)$ is just a linear model**. Though the expansion around $w_0$ is regardless to the proof of NTK, it is still **a useful tool to analyze the accuracy of the linear approximation with infinite-width network**.\nHowever, the most difficult thing is how can we guarantee the approximation is accurate enough? It is so complex that I wouldn’t put it in this article but I will provide an intuitive explaination of what does NTK mean? in the following article. Please keep reading it if you are interested in it.\nAn Simpler Explaination Simply, we only consider a 1-dimension network $f(x, w), \\ w, x, \\bar{y} \\in \\mathbb{R}$ for a dataset $x \\in X, \\ \\bar{y} \\in \\bar{Y}$ which are input data points and output data points respectively.\nFirst of all, let’s define the loss function of a neural network\n$$L_{1}(x, w) = \\frac{1}{2} \\Vert f(x, w) - \\bar{y} \\Vert^2_2$$\nThe gradient descent is\n$$w_{t+1} = w_0 + \\eta \\ \\frac{dL_1(x, w)}{dw} = w_0 + \\eta (f(x, w) - \\bar{y}) \\frac{df(x, w)}{dw}$$\nwhere $\\eta$ is the learning rate.\nNTK represent the changes of the weights before and after the gradient descent update. Thus, the NTK of 1-dimension network can be defined as\n$$k_{1}^{NTK}(x, x') = lim_{\\eta \\to 0} f(x, w + \\eta \\ \\frac{dL_1(x', w)}{dw}) - f(x, w)$$\n$$= lim_{\\eta \\to 0} f(x, w + \\eta \\ (f(x', w) - \\bar{y}) \\frac{df(x', w)}{dw}) - f(x, w)$$\nTo simplify the notation, let $\\eta \\ (f(x', w) - \\bar{y}) \\frac{df(x', w)}{dw}) = \\Delta w$\nSuppose the learning rate $\\eta$ is small enough and thus, $w \\approx w + \\Delta w$. We can expand around $w + \\Delta w$ with Taylor expansion\n$$f(x, w) \\approx f(x, w + \\Delta w) + \\frac{df(x', w + \\Delta w)}{dw} (w - (w + \\Delta w))$$\n$$ = f(x, w + \\Delta w) - \\frac{df(x', w + \\Delta w)}{dw}\\Delta w$$\nWe can get NTK\n$$k_{1}^{NTK}(x, x') = lim_{\\eta \\to 0} \\frac{f(x, w + \\eta \\ (f(x', w) - \\bar{y}) \\frac{df(x', w)}{dw}) - f(x, w)}{\\eta}$$\n$$ = lim_{\\eta \\to 0} \\frac{f(x, w + \\Delta w) - f(x, w)}{\\eta} = lim_{\\eta \\to 0} \\frac{f(x, w + \\Delta w) - (f(x, w + \\Delta w) - \\frac{df(x', w + \\Delta w)}{dw} \\Delta w)}{\\eta}$$\n$$= lim_{\\eta \\to 0} \\ \\frac{1}{\\eta} \\frac{df(x', w + \\Delta w)}{dw} = lim_{\\eta \\to 0} \\ \\frac{1}{\\eta} \\frac{df(x', w + \\eta \\ (f(x', w) - \\bar{y}) \\frac{df(x', w)}{dw}))}{dw}$$\nFlow And Velocity Field Before diving into NTK more deeply, we need to understand what is Flow and Velocity Field.\nVelocity Field Flow Gradient Flow We’ve know the update of the gradient descent is\n$$w_{t+1} = w_t - \\eta \\nabla_{w} L(w_t)$$\nLet the function $w(t) = w_t$ and define the gradient flow over weights is $\\dot{w}(t)$\n$$\\dot{w}(t) = - \\nabla_{w} L(w(t))$$\nActually, the meaning of the gradient flow $\\dot{w}(t)$ is likey the changing direction of gradient descent.\nWe expand the gradient of the loss function with chain rule\n$$ \\dot{w}(t) = - \\nabla_{w} L(w(t)) = - \\nabla_{w} \\frac{1}{2} \\Vert y(w(t)) - \\bar{y} \\Vert^2_2 $$\n$$ = - \\frac{1}{2} \\cdot 2 \\nabla_{w} y(w(t)) (y(w(t)) - \\bar{y}) = - \\nabla_{w} y(w) (y(w(t)) - \\bar{y}) $$\nNow we can derive the flow of the network $\\dot{y}(w(t))$\n$$ \\dot{y}(w(t)) = \\nabla_{w} y(w(t))^{\\top} \\dot{w}(t) $$\n$$ = -\\nabla_{w} y(w(t))^{\\top} \\nabla_{w} y(w) (y(w(t)) - \\bar{y}) = - \\nabla_{w} y(w_t)^{\\top} \\nabla_{w} y(w_t)(y(w_t) - \\bar{y}) $$\nActually, we are now very close to the neural tangent kernel(NTK). The NTK is a kernel matrix defined as\n$$\\Sigma_{NTK}(w_t, w_t) = \\nabla_{w} y(w_t)^{\\top} \\nabla_{w} y(w_t)$$ Since the weights of the infinite-width network doesn’t change during the training.\n$$y(w_t) \\approx y(w_0)$$\nWe get\n$$ -\\nabla_{w} y(w_t)^{\\top} \\nabla_{w} y(w_t) \\approx -\\nabla_{w} y(w_0)^{\\top} \\nabla_{w} y(w_0) $$\n$$= \\Sigma_{NTK}(w_0, w_0)$$ Again, $\\Sigma_{NTK}(w_0, w_0)$ is the Neural Tangent Kernel, NTK.\nIt is very surprise that NTK doesn’t depend on the input data but the inital weights. Well, why doesn’t NTK depend on the input data? Actually, it is proved by another work that neural network is just a kernel machine. It is a quite interesting work but I wouldn’t cover in this article.\nTo summary, the weights of an infinite width network almost don’t change during training. As a result, the kernel always stay almost the same. We can use NTK to analyze many properties of neural network and the neural networks are no longer black boxes.\nPapers NNGP\n Deep Neural Networks as Gaussian Processes  NTK\n Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent  Reference Gaussian Distribution\n StackExchange - Product of two multivariate normal distribution  NNGP\n Deep Gaussian Processes  NTK\n Understanding the Neural Tangent Kernel By Rajat’s Blog  Code for the blog rajatvd/NTK   Ultra-Wide Deep Nets and the Neural Tangent Kernel (NTK) CMU ML Blog: Ultra-Wide Deep Nets and the Neural Tangent Kernel (NTK) Some Intuition on the Neural Tangent Kernel 直观理解Neural Tangent Kernel  Flow\n  Let it flow - Gradient flow and gradient descent\n  Max Planck Science - Gradient Flow I\n  StackExchange - gradient flow and what is, for example, L2 gradient?\n  Notebook\n Colab Notebook  ","wordCount":"1209","inLanguage":"en","image":"https://frankccccc.github.io/blog/img/just_imgs/wave_process.jpg","datePublished":"2021-02-19T20:46:29+08:00","dateModified":"2021-02-19T20:46:29+08:00","author":{"@type":"Person","name":"SY Chou"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://frankccccc.github.io/blog/posts/nngp_ntk/"},"publisher":{"@type":"Organization","name":"Golden Hat","logo":{"@type":"ImageObject","url":"https://frankccccc.github.io/blog/favicon.ico"}}}</script></head><body class=dark id=top><script>if(localStorage.getItem("pref-theme")==="light"){document.body.classList.remove('dark')}</script><noscript><style type=text/css>#theme-toggle,.top-link{display:none}</style></noscript><header class=header><nav class=nav><div class=logo><a href=https://frankccccc.github.io/blog/ accesskey=h title="Golden Hat (Alt + H)">Golden Hat</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu onscroll=menu_on_scroll()><li><a href=https://frankccccc.github.io/blog/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://frankccccc.github.io/blog/series title=Series><span>Series</span></a></li><li><a href=https://frankccccc.github.io/blog/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://frankccccc.github.io/blog/tags/ title=Tags><span>Tags</span></a></li></ul></nav><script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$'],['\\[','\\]']],processEscapes:true,processEnvironments:true},options:{skipHtmlTags:['script','noscript','style','textarea','pre']}};window.addEventListener('load',(event)=>{document.querySelectorAll("mjx-container").forEach(function(x){x.parentElement.classList+='has-jax'})});</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://frankccccc.github.io/blog/>Home</a>&nbsp;»&nbsp;<a href=https://frankccccc.github.io/blog/posts/>Posts</a></div><h1 class=post-title>Toward NNGP and NTK<div class=entry-isdraft><sup>&nbsp;&nbsp;[draft]</sup></div></h1><div class=post-meta>February 19, 2021&nbsp;·&nbsp;6 min&nbsp;·&nbsp;SY Chou</div></header><figure class=entry-cover><img src=https://frankccccc.github.io/blog/img/just_imgs/wave_process.jpg alt></figure><div class=post-content><h1 id=neural-network-gaussian-processnngp>Neural Network Gaussian Process(NNGP)<a hidden class=anchor aria-hidden=true href=#neural-network-gaussian-processnngp>#</a></h1><h1 id=neural-tangent-kernelntk>Neural Tangent Kernel(NTK)<a hidden class=anchor aria-hidden=true href=#neural-tangent-kernelntk>#</a></h1><h1 id=in-short-ntk-represent-the-changes-of-the-weights-before-and-after-the-gradient-descent-update><em>&ldquo;In short, NTK represent the changes of the weights before and after the gradient descent update&rdquo;</em><a hidden class=anchor aria-hidden=true href=#in-short-ntk-represent-the-changes-of-the-weights-before-and-after-the-gradient-descent-update>#</a></h1><p>Let&rsquo;s start the journey of revealing the black-box neural networks.</p><h2 id=setup-a-neural-network>Setup a Neural Network<a hidden class=anchor aria-hidden=true href=#setup-a-neural-network>#</a></h2><p>First of all, we need to define a simple neural network with 2 hidden layers</p><p>$$ y(x, w)$$</p><p>where $y$ is the neural network with weights $w \in \mathbb{R}^m$ and, $\{ x, \bar{y} \}$ is the dataset which is a set of the input data and the output data with $N$ data points. Since we focus on analyze the weight $w$, we simplify the notation $y(x, w)$ as $y(w)$</p><p>Suppose we have a regression task on the network $y(w)$, define</p><p>$$L(w) = \frac{1}{N} \frac{1}{2} \Vert y(w) - \bar{y} \Vert^2_2 $$</p><p>where $L(w)$ is our object loss which we want to minimize. Since the term $\frac{1}{N}$ is regardless to our goal, just ignore it and we get a simpler loss function</p><p>$$L(w) = \frac{1}{2} \Vert y(w) - \bar{y} \Vert^2_2 $$</p><h2 id=to-the-limit-of-infinite-width>To The Limit of Infinite Width<a hidden class=anchor aria-hidden=true href=#to-the-limit-of-infinite-width>#</a></h2><p>In order to <strong>measure the difference of the weights during training the neural network</strong>, we define a normalized metric as following</p><p>$$\frac{\Vert w_n - w_0 \Vert_2}{\Vert w_0 \Vert_2}$$</p><p>where $w_n$ and $w_0$ are the weights at <em>n</em>-th training iteration and the initial weights. $\Vert w_n - w_0 \Vert_2$ means the quantity of the differnce between parameters $w_n$ and $w_0$ and it is normalized by the 2-norm $\Vert w_0 \Vert_2$</p><p><img src=https://frankccccc.github.io/blog/img/nngp_ntk/losses_3widths.png alt="losses with 3 widths"></p><p><img src=https://frankccccc.github.io/blog/img/nngp_ntk/weightchange_3widths.png alt="normalized weight-changes with 3 widths"></p><p>As we can see, <strong>the difference of the weights during training decrease as the width of network grows</strong>. As a result, <strong>the trained weights should be very close to the inital weights $w_0$ as the width of network goes to infinity</strong>.</p><h2 id=apply-taylor-expansion>Apply Taylor Expansion<a hidden class=anchor aria-hidden=true href=#apply-taylor-expansion>#</a></h2><p>We&rsquo;ve known the Taylor expansion is</p><p>$$f(x) = \Sigma_{n=0}^{\infty} \ \frac{f^{n}(a)}{n!} (x - a)^{n}$$</p><p>A function $f(w)$ expanded on the $w_0$ with first order approximation is</p><p>$$f(w) \approx f(w_0) + \frac{df(w_0)}{dw} (w - w_0)$$</p><p>It is trivial that if $w$ is a vector, we need to replace the derivative $\frac{df(w_0)}{dw}$ with gradient $\nabla_{w} f(w_0)^{\top}$</p><p>$$f(w) \approx f(w_0) + \nabla_{w} f(w_0)^{\top} \ (w - w_0)$$</p><p>Apply to the network $y(w)$</p><p>$$y(w) \approx y(w_0) + \nabla_{w} y(w_0)^{\top} \ (w - w_0)$$</p><p>where $\nabla_{w} y(w_0)$ and $y(w_0)$ are constants. Thus, the Taylor expansion of **$y(w)$ is just a linear model**. Though the expansion around $w_0$ is regardless to the proof of NTK, it is still **a useful tool to analyze the accuracy of the linear approximation with infinite-width network**.</p><p>However, the most difficult thing is <strong>how can we guarantee the approximation is accurate enough?</strong> It is so complex that I wouldn&rsquo;t put it in this article but I will provide an intuitive explaination of <strong>what does NTK mean?</strong> in the following article. Please keep reading it if you are interested in it.</p><h2 id=an-simpler-explaination>An Simpler Explaination<a hidden class=anchor aria-hidden=true href=#an-simpler-explaination>#</a></h2><p>Simply, we only consider a 1-dimension network $f(x, w), \ w, x, \bar{y} \in \mathbb{R}$ for a dataset $x \in X, \ \bar{y} \in \bar{Y}$ which are input data points and output data points respectively.</p><p>First of all, let&rsquo;s define the loss function of a neural network</p><p>$$L_{1}(x, w) = \frac{1}{2} \Vert f(x, w) - \bar{y} \Vert^2_2$$</p><p>The gradient descent is</p><p>$$w_{t+1} = w_0 + \eta \ \frac{dL_1(x, w)}{dw} = w_0 + \eta (f(x, w) - \bar{y}) \frac{df(x, w)}{dw}$$</p><p>where $\eta$ is the learning rate.</p><p><strong>NTK represent the changes of the weights before and after the gradient descent update</strong>. Thus, the NTK of 1-dimension network can be defined as</p><p>$$k_{1}^{NTK}(x, x') = lim_{\eta \to 0} f(x, w + \eta \ \frac{dL_1(x', w)}{dw}) - f(x, w)$$</p><p>$$= lim_{\eta \to 0} f(x, w + \eta \ (f(x', w) - \bar{y}) \frac{df(x', w)}{dw}) - f(x, w)$$</p><p>To simplify the notation, let $\eta \ (f(x', w) - \bar{y}) \frac{df(x', w)}{dw}) = \Delta w$</p><p>Suppose the learning rate $\eta$ is small enough and thus, $w \approx w + \Delta w$. We can expand around $w + \Delta w$ with Taylor expansion</p><p>$$f(x, w) \approx f(x, w + \Delta w) + \frac{df(x', w + \Delta w)}{dw} (w - (w + \Delta w))$$</p><p>$$ = f(x, w + \Delta w) - \frac{df(x', w + \Delta w)}{dw}\Delta w$$</p><p>We can get NTK</p><p>$$k_{1}^{NTK}(x, x') = lim_{\eta \to 0} \frac{f(x, w + \eta \ (f(x', w) - \bar{y}) \frac{df(x', w)}{dw}) - f(x, w)}{\eta}$$</p><p>$$ = lim_{\eta \to 0} \frac{f(x, w + \Delta w) - f(x, w)}{\eta} = lim_{\eta \to 0} \frac{f(x, w + \Delta w) - (f(x, w + \Delta w) - \frac{df(x', w + \Delta w)}{dw} \Delta w)}{\eta}$$</p><p>$$= lim_{\eta \to 0} \ \frac{1}{\eta} \frac{df(x', w + \Delta w)}{dw} = lim_{\eta \to 0} \ \frac{1}{\eta} \frac{df(x', w + \eta \ (f(x', w) - \bar{y}) \frac{df(x', w)}{dw}))}{dw}$$</p><h2 id=flow-and-velocity-field>Flow And Velocity Field<a hidden class=anchor aria-hidden=true href=#flow-and-velocity-field>#</a></h2><p>Before diving into NTK more deeply, we need to understand what is <strong>Flow</strong> and <strong>Velocity Field</strong>.</p><h3 id=velocity-field>Velocity Field<a hidden class=anchor aria-hidden=true href=#velocity-field>#</a></h3><h3 id=flow>Flow<a hidden class=anchor aria-hidden=true href=#flow>#</a></h3><h2 id=gradient-flow>Gradient Flow<a hidden class=anchor aria-hidden=true href=#gradient-flow>#</a></h2><p>We&rsquo;ve know the update of the gradient descent is</p><p>$$w_{t+1} = w_t - \eta \nabla_{w} L(w_t)$$</p><p>Let the function $w(t) = w_t$ and define the gradient flow over weights is $\dot{w}(t)$</p><p>$$\dot{w}(t) = - \nabla_{w} L(w(t))$$</p><p>Actually, the meaning of the gradient flow $\dot{w}(t)$ is likey <strong>the changing direction of gradient descent</strong>.</p><p>We expand the gradient of the loss function with chain rule</p><p>$$
\dot{w}(t) = - \nabla_{w} L(w(t)) = - \nabla_{w} \frac{1}{2} \Vert y(w(t)) - \bar{y} \Vert^2_2
$$</p><p>$$
= - \frac{1}{2} \cdot 2 \nabla_{w} y(w(t)) (y(w(t)) - \bar{y}) = - \nabla_{w} y(w) (y(w(t)) - \bar{y})
$$</p><p>Now we can derive the flow of the network $\dot{y}(w(t))$</p><p>$$
\dot{y}(w(t)) = \nabla_{w} y(w(t))^{\top} \dot{w}(t)
$$</p><p>$$
= -\nabla_{w} y(w(t))^{\top} \nabla_{w} y(w) (y(w(t)) - \bar{y}) = - \nabla_{w} y(w_t)^{\top} \nabla_{w} y(w_t)(y(w_t) - \bar{y})
$$</p><p>Actually, we are now very close to the neural tangent kernel(NTK). The NTK is a kernel matrix defined as</p><h2 id=sigma_ntkw_t-w_t--nabla_w-yw_ttop-nabla_w-yw_t>$$\Sigma_{NTK}(w_t, w_t) = \nabla_{w} y(w_t)^{\top} \nabla_{w} y(w_t)$$<a hidden class=anchor aria-hidden=true href=#sigma_ntkw_t-w_t--nabla_w-yw_ttop-nabla_w-yw_t>#</a></h2><p>Since the weights of the infinite-width network doesn&rsquo;t change during the training.</p><p>$$y(w_t) \approx y(w_0)$$</p><p>We get</p><p>$$
-\nabla_{w} y(w_t)^{\top} \nabla_{w} y(w_t) \approx -\nabla_{w} y(w_0)^{\top} \nabla_{w} y(w_0)
$$</p><h2 id=-sigma_ntkw_0-w_0>$$= \Sigma_{NTK}(w_0, w_0)$$<a hidden class=anchor aria-hidden=true href=#-sigma_ntkw_0-w_0>#</a></h2><p>Again, $\Sigma_{NTK}(w_0, w_0)$ is the Neural Tangent Kernel, NTK.</p><p>It is very surprise that <strong>NTK doesn&rsquo;t depend on the input data but the inital weights</strong>. Well, why doesn&rsquo;t NTK depend on the input data? Actually, it is proved by <a href=https://arxiv.org/abs/2012.00152>another work</a> that neural network is just a kernel machine. It is a quite interesting work but I wouldn&rsquo;t cover in this article.</p><p>To summary, <strong>the weights of an infinite width network almost don&rsquo;t change during training. As a result, the kernel always stay almost the same.</strong> We can use NTK to analyze many properties of neural network and the neural networks are no longer black boxes.</p><h1 id=papers>Papers<a hidden class=anchor aria-hidden=true href=#papers>#</a></h1><p>NNGP</p><ul><li><a href=https://arxiv.org/abs/1711.00165>Deep Neural Networks as Gaussian Processes</a></li></ul><p>NTK</p><ul><li><a href=https://arxiv.org/abs/1902.06720>Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent</a></li></ul><h1 id=reference>Reference<a hidden class=anchor aria-hidden=true href=#reference>#</a></h1><p>Gaussian Distribution</p><ul><li><a href=https://math.stackexchange.com/questions/3495719/product-of-two-multivariate-normal-distribution>StackExchange - Product of two multivariate normal distribution</a></li></ul><p>NNGP</p><ul><li><a href=http://inverseprobability.com/talks/notes/deep-gaussian-processes.html>Deep Gaussian Processes</a></li></ul><p>NTK</p><ul><li><a href=https://rajatvd.github.io/NTK/>Understanding the Neural Tangent Kernel By Rajat&rsquo;s Blog</a><ul><li>Code for the blog <a href=https://github.com/rajatvd/NTK>rajatvd/NTK</a></li></ul></li><li><a href=https://blog.ml.cmu.edu/2019/10/03/ultra-wide-deep-nets-and-the-neural-tangent-kernel-ntk/>Ultra-Wide Deep Nets and the Neural Tangent Kernel (NTK)</a></li><li><a href=https://blog.ml.cmu.edu/2019/10/03/ultra-wide-deep-nets-and-the-neural-tangent-kernel-ntk/>CMU ML Blog: Ultra-Wide Deep Nets and the Neural Tangent Kernel (NTK)</a></li><li><a href=https://www.inference.vc/neural-tangent-kernels-some-intuition-for-kernel-gradient-descent/>Some Intuition on the Neural Tangent Kernel</a></li><li><a href=https://zhuanlan.zhihu.com/p/339971642>直观理解Neural Tangent Kernel</a></li></ul><p>Flow</p><ul><li><p><a href=http://awibisono.github.io/2016/06/13/gradient-flow-gradient-descent.html>Let it flow - Gradient flow and gradient descent</a></p></li><li><p><a href="https://www.youtube.com/watch?v=pesXn-qwMvQ">Max Planck Science - Gradient Flow I</a></p></li><li><p><a href=https://math.stackexchange.com/questions/156236/gradient-flow-and-what-is-for-example-l2-gradient>StackExchange - gradient flow and what is, for example, L2 gradient?</a></p></li></ul><p>Notebook</p><ul><li><a href="https://colab.research.google.com/drive/1tLCfu0DCqN3RxLHA9rIARjBtC1VOHFNg?usp=sharing">Colab Notebook</a></li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://frankccccc.github.io/blog/tags/gaussian-process/>gaussian process</a></li><li><a href=https://frankccccc.github.io/blog/tags/deep-learning/>deep learning</a></li><li><a href=https://frankccccc.github.io/blog/tags/machine-learning/>machine learning</a></li><li><a href=https://frankccccc.github.io/blog/tags/bayes/>bayes</a></li><li><a href=https://frankccccc.github.io/blog/tags/nngp/>NNGP</a></li><li><a href=https://frankccccc.github.io/blog/tags/ntk/>NTK</a></li></ul><nav class=paginav><a class=next href=https://frankccccc.github.io/blog/posts/mle_map_bayes/><span class=title>Next Page »</span><br><span>An Insight Into MLE, MAP, and Bayesian Estimation</span></a></nav></footer><div style=padding-top:2.5rem;padding-bottom:2.5rem;text-align:left><h1 style=padding-bottom:.5rem>COMMENTS</h1><h5>Your comments will encouage me to share more~~</h5></div><div id=utter-container></div><script src=https://utteranc.es/client.js repo=frankccccc/blog issue-term=title theme=photon-dark crossorigin=anonymous async>console.log($theme)</script></article></main><footer class=footer><span>&copy; 2021 <a href=https://frankccccc.github.io/blog/>Golden Hat</a></span>
<span>&#183;</span>
<span>Powered by <a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span>
<span>&#183;</span>
<span>Theme <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)"><button class=top-link id=top-link type=button accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></button></a>
<script defer src=https://frankccccc.github.io/blog/assets/js/highlight.min.27cd435cc9ed6abb4b496581b151804f79f366c412620272bb94e2f5f598ebcc.js integrity="sha256-J81DXMntartLSWWBsVGAT3nzZsQSYgJyu5Ti9fWY68w=" onload=hljs.initHighlightingOnLoad();></script><script>window.onload=function(){if(localStorage.getItem("menu-scroll-position")){document.getElementById('menu').scrollLeft=localStorage.getItem("menu-scroll-position");}}
document.querySelectorAll('a[href^="#"]').forEach(anchor=>{anchor.addEventListener("click",function(e){e.preventDefault();var id=this.getAttribute("href").substr(1);if(!window.matchMedia('(prefers-reduced-motion: reduce)').matches){document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({behavior:"smooth"});}else{document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();}
if(id==="top"){history.replaceState(null,null," ");}else{history.pushState(null,null,`#${id}`);}});});var mybutton=document.getElementById("top-link");window.onscroll=function(){if(document.body.scrollTop>800||document.documentElement.scrollTop>800){mybutton.style.visibility="visible";mybutton.style.opacity="1";}else{mybutton.style.visibility="hidden";mybutton.style.opacity="0";}};function menu_on_scroll(){localStorage.setItem("menu-scroll-position",document.getElementById('menu').scrollLeft);}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{if(document.body.className.includes("dark")){document.body.classList.remove('dark');localStorage.setItem("pref-theme",'light');}else{document.body.classList.add('dark');localStorage.setItem("pref-theme",'dark');}})</script></body></html>