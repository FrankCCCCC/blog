<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts on Golden Hat</title><link>https://frankccccc.github.io/blog/posts/</link><description>Recent content in Posts on Golden Hat</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Fri, 19 Feb 2021 20:46:29 +0800</lastBuildDate><atom:link href="https://frankccccc.github.io/blog/posts/index.xml" rel="self" type="application/rss+xml"/><item><title>Toward NNGP and NTK</title><link>https://frankccccc.github.io/blog/posts/nngp_ntk/</link><pubDate>Fri, 19 Feb 2021 20:46:29 +0800</pubDate><guid>https://frankccccc.github.io/blog/posts/nngp_ntk/</guid><description>Reference StackExchange - Product of two multivariate normal distribution Deep Gaussian Processes</description></item><item><title>An Insight Into MLE, MAP, and Bayesian Estimation</title><link>https://frankccccc.github.io/blog/posts/mle_map_bayes/</link><pubDate>Fri, 19 Feb 2021 11:15:15 +0800</pubDate><guid>https://frankccccc.github.io/blog/posts/mle_map_bayes/</guid><description>The main different between 3 kinds of estimation is What do we assume for the prior? The Maximum Likelihood Estimation(MLE) doesn&amp;rsquo;t use any prior but only maiximize the probability according to the samples. On the other hand, MAP and Bayesian both use priors to estimate the probability. The Maximum A Posteriori(MAP) only use the probability of single event while Bayesian Estimation see a distribution as the prior.
To be continue&amp;hellip;</description></item><item><title>Part I - From Alphago to Muzero</title><link>https://frankccccc.github.io/blog/posts/alphago/</link><pubDate>Fri, 19 Feb 2021 01:14:40 +0800</pubDate><guid>https://frankccccc.github.io/blog/posts/alphago/</guid><description/></item><item><title>A Glimpse of Distributional RL</title><link>https://frankccccc.github.io/blog/posts/distributional_rl/</link><pubDate>Tue, 16 Feb 2021 20:36:18 +0800</pubDate><guid>https://frankccccc.github.io/blog/posts/distributional_rl/</guid><description/></item><item><title>An Introduction to Multi-Armed Bandit Problem and Solutions</title><link>https://frankccccc.github.io/blog/posts/bandit/</link><pubDate>Tue, 16 Feb 2021 20:11:41 +0800</pubDate><guid>https://frankccccc.github.io/blog/posts/bandit/</guid><description>Multi-Armed Bandit Problem Imagine you are in a casionoand face multiple slot machines. Each machine is configured with an unknown probability of how likely you would get a reward at one play. The question is What&amp;rsquo;s the strategy to get the highest long-term reward?
An illustration of multi-armed bandit problem, refer to Lil&amp;rsquo;Log The Multi-Armed Bandit Problem and Its Solutions
Definition Upper Confidence Bounds(UCB) The UCB algorithm give a realtion between upper bound and probability confidence.</description></item><item><title>A Very Brief Introduction to Gaussian Process and Bayesian Optimization</title><link>https://frankccccc.github.io/blog/posts/intro_gp_bayes_opt/</link><pubDate>Tue, 16 Feb 2021 17:28:58 +0800</pubDate><guid>https://frankccccc.github.io/blog/posts/intro_gp_bayes_opt/</guid><description>Gaussian Process Big Picture and Background Intuitively, Gaussian distribution define the state space, while Gaussian Process define the function space
Before we introduce Gaussian process, we should understand Gaussian distriution at first. For a RV(random variable) $X$ that follow Gaussian Distribution $\mathcal{N}(0, 1)$ should be following image:
The P.D.F should be
$$x \sim \mathcal{N}(\mu, \sigma) = \frac{1}{\sigma \sqrt{2 \pi}} e^{- \frac{1}{2} (\frac{- \mu}{\sigma})^2}$$
As for Multivariate Gaussian Distribution, given 2 RV $x$, $y$ both 2 RV follow Gaussian Distribution $\mathcal{N}(0, 1)$ we can illustrate it as</description></item><item><title>部落格搬家記</title><link>https://frankccccc.github.io/blog/posts/move_blog/</link><pubDate>Tue, 16 Feb 2021 17:28:58 +0800</pubDate><guid>https://frankccccc.github.io/blog/posts/move_blog/</guid><description>因為寫DL筆記時會用到大量數學符號，就索性把原先在Github上的DL_DB_Quick_Notes搬過來了，配合LATEX寫筆記順手很多，原先的Repo應該只會剩下收集Paper用。而最近生活上有些轉折，也許也會順便放些隨筆雜記，但就依心情而定。
目前用的主題是PaperMod，整體設計算令人滿意，只不過在Deploy HUGO遇到蠻多麻煩，這邊簡單記錄一下
設定Github Page Action Latex 設定 參考這篇
Step 1 首先在安裝好的主題裡面layouts/partials/mathjax_support.html新增.html檔
&amp;lt;script&amp;gt; MathJax = { tex: { inlineMath: [['$', '$'], ['\\(', '\\)']], displayMath: [['$$','$$'], ['\\[', '\\]']], processEscapes: true, processEnvironments: true }, options: { skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre'] } }; window.addEventListener('load', (event) =&amp;gt; { document.querySelectorAll(&amp;quot;mjx-container&amp;quot;).forEach(function(x){ x.parentElement.classList += 'has-jax'}) }); &amp;lt;/script&amp;gt; &amp;lt;script src=&amp;quot;https://polyfill.io/v3/polyfill.min.js?features=es6&amp;quot;&amp;gt;&amp;lt;/script&amp;gt; &amp;lt;script type=&amp;quot;text/javascript&amp;quot; id=&amp;quot;MathJax-script&amp;quot; async src=&amp;quot;https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js&amp;quot;&amp;gt;&amp;lt;/script&amp;gt; Step 2 在layouts/partials/header.html的&amp;lt;/head&amp;gt; tag裡面再新增這段code
{{ partial &amp;quot;mathjax_support.html&amp;quot; . }} Step 3 最後在assets/css/header.</description></item></channel></rss>