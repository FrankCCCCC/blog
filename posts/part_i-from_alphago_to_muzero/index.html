<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Part I - From AlphaGo to MuZero | Golden Hat</title><meta name=keywords content="RL,deep learning,model-based RL"><meta name=description content="AlphaGo is quite famous when I was a freshman of college. It somehow is the reason that I was addicted to Reinforcement Learning. Thus Our journey of model-based RL will start here. Although it is not the first one that propose model-based RL, I still believe it will give a big picture of model-based RL.
Mastering the game of Go with deep neural networks and tree search Introduction AlphaGo combines 2 kinds of model, including policy network and value network."><meta name=author content="SY Chou"><link rel=canonical href=https://frankccccc.github.io/blog/posts/part_i-from_alphago_to_muzero/><script async src="https://www.googletagmanager.com/gtag/js?id=G-S9MCZ2NDS7"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','G-S9MCZ2NDS7')</script><link href=/blog/assets/css/stylesheet.min.3768d020b7a9fd49f21ddb7d521ce9b187ce47c2929b883041396159d4d25553.css integrity="sha256-N2jQILep/UnyHdt9UhzpsYfOR8KSm4gwQTlhWdTSVVM=" rel="preload stylesheet" as=style><link rel=icon href=https://frankccccc.github.io/blog/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://frankccccc.github.io/blog/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://frankccccc.github.io/blog/favicon-32x32.png><link rel=apple-touch-icon href=https://frankccccc.github.io/blog/apple-touch-icon.png><link rel=mask-icon href=https://frankccccc.github.io/blog/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><meta name=generator content="Hugo 0.81.0"><script async src="https://www.googletagmanager.com/gtag/js?id=G-S9MCZ2NDS7"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','G-S9MCZ2NDS7')</script><meta property="og:title" content="Part I - From AlphaGo to MuZero"><meta property="og:description" content="AlphaGo is quite famous when I was a freshman of college. It somehow is the reason that I was addicted to Reinforcement Learning. Thus Our journey of model-based RL will start here. Although it is not the first one that propose model-based RL, I still believe it will give a big picture of model-based RL.
Mastering the game of Go with deep neural networks and tree search Introduction AlphaGo combines 2 kinds of model, including policy network and value network."><meta property="og:type" content="article"><meta property="og:url" content="https://frankccccc.github.io/blog/posts/part_i-from_alphago_to_muzero/"><meta property="og:image" content="https://frankccccc.github.io/blog/img/just_imgs/alphago.webp"><meta property="article:published_time" content="2021-02-19T01:14:40+08:00"><meta property="article:modified_time" content="2021-02-19T01:14:40+08:00"><meta property="og:see_also" content="https://frankccccc.github.io/blog/posts/part_iii-from_alphago_to_muzero/"><meta property="og:see_also" content="https://frankccccc.github.io/blog/posts/part_ii-from_alphago_to_muzero/"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://frankccccc.github.io/blog/img/just_imgs/alphago.webp"><meta name=twitter:title content="Part I - From AlphaGo to MuZero"><meta name=twitter:description content="AlphaGo is quite famous when I was a freshman of college. It somehow is the reason that I was addicted to Reinforcement Learning. Thus Our journey of model-based RL will start here. Although it is not the first one that propose model-based RL, I still believe it will give a big picture of model-based RL.
Mastering the game of Go with deep neural networks and tree search Introduction AlphaGo combines 2 kinds of model, including policy network and value network."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://frankccccc.github.io/blog/posts/"},{"@type":"ListItem","position":2,"name":"Part I - From AlphaGo to MuZero","item":"https://frankccccc.github.io/blog/posts/part_i-from_alphago_to_muzero/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Part I - From AlphaGo to MuZero","name":"Part I - From AlphaGo to MuZero","description":"AlphaGo is quite famous when I was a freshman of college. It somehow is the reason that I was addicted to Reinforcement Learning. Thus Our journey of model-based RL will start …","keywords":["RL","deep learning","model-based RL"],"articleBody":"AlphaGo is quite famous when I was a freshman of college. It somehow is the reason that I was addicted to Reinforcement Learning. Thus Our journey of model-based RL will start here. Although it is not the first one that propose model-based RL, I still believe it will give a big picture of model-based RL.\nMastering the game of Go with deep neural networks and tree search Introduction AlphaGo combines 2 kinds of model, including policy network and value network. The policy network takes the board position as input and output the probability of next action of each position. The value network also take the board position as input and output the winner of the game.\nWe pass in the board position as a 19×19 image and use convolutional layers to construct a representation of the position. We use these neural networks to reduce the effective depth and breadth of the search tree: evaluating positions using a value network, and sampling actions using a policy network.\nWe train the model in 2 stage. In the first stage, we use supervised learning with KGS dataset to train the policy network to predict the next action of humans. Then, in the second stage, we use reinforment learning and self-play to train the model by themself.\nStage1: Supervised Learning of Policy Network A fast rollout policy $p_{\\pi}$ and supervised learning (SL) policy network $p_{\\sigma}$ are trained to predict human expert moves in a data set of positions. The fast rollout policy $p_{\\pi}$ is trained only with some important features such as Stone colour to reduce the complexity of the model(faster but less accurate)while the SL policy network $p_{\\sigma}$ is trained with whole position of Go.\nWe trained a 13-layer policy network, which we call the SL policy network, from 30 million positions from the KGS Go Server. Then we update the policy network with the following function to maximize the probability of predicting the action of human experts:\n$$ \\triangle \\sigma \\propto \\frac{\\partial log \\ p_{\\sigma}(a|s)}{\\partial \\sigma} $$\nStage2: Reinforcement learning of policy networks We use policy gradient reinforcement learning (RL) to update the network. The RL policy network $p_{\\rho}$ is **identical in structure to the SL policy network**, its weights $\\rho$ are **initialized to the same values**, $\\rho = \\sigma$. We play games between the current policy network $p_{\\rho}$ and a **randomly selected previous iteration of the policy network to prevent overfit and stablize training**. To update the RL policy network, we use **policy gradient** to maximize the expected outcome:\n$$ \\triangle \\rho \\propto \\frac{\\partial log \\ p_{\\rho}(a_t|s_t)}{\\partial \\rho} z_t $$\nHere we use a reward function $r(s)$ that t is 0 for all non-terminal time steps $tStage2: Reinforcement learning of value networks Estimating a value function $v^p(s)$ that predicts the outcome from position $s$ of games played by using policy $p$ for both players\n$$ v^p(s) = E[z_t | s_t = s, a_{t … T} \\sim p] $$\nWe approximate the value function using a value network $v_{\\theta}(s)$ with weights $\\theta$, $v_{\\theta}(s) \\approx v^{p_{\\rho}}(s) \\approx v^*(s)$.\nWe define the loss function of the value network with mean squared error(MSE):\n$$ \\triangle \\theta \\propto \\frac{\\partial v_{\\theta}(s)}{\\partial \\theta} (z - v_{\\theta}(s)) $$\nBut how do we search the optimal value through policy network? There are 5 steps as Figure3:\n  Step 1: Selection\nEach simulation traverses the tree by selecting the edge with maximum action value $Q$, plus a bonus $u(P)$ that depends on a stored prior probability $P(s, a)$ for that edge.\n  Step 2: Expansion\nThe leaf node may be expanded. The new node is processed once by the policy network $p_{\\sigma}$ with output $P(s, a)=p_{\\sigma}(a|s)$.\nEach edge $(s, a)$ of the search tree stores an action value $Q(s, a)$, visit count $N(s, a)$, and prior probability $P(s, a)$.\nThe $u(s, a)$ is a kind of bonus that is proportional to the prior probability but decays with repeated visits to encourage exploration.\nAt each time step $t$ of each simulation, an action $a_t$ is selected from state $s_t$\n$$ a_t = \\mathop{\\arg\\max}_a (Q(s_t, a) + u(s_t, a)) $$\nso as to maximize action value plus a bonus\n$$ u(s, a) \\propto \\frac{P(s, a)}{1 + N(s, a)} $$\n  Step 3: Evaluation\nThe leaf node is evaluated in two very different ways: first, by the value network $v_{\\theta}(s_L)$; and second, by the outcome $z_L$ of a random rollout played out until terminal step $T$ using the fast rollout policy $p_{\\pi}$; these evaluations are combined, using a mixing parameter $\\lambda$, into a leaf evaluation $V(s_L)$.\n$$ V(s_L) = (1 - \\lambda) v_{\\theta}(s_L) + \\lambda z_L $$\n  Step 4: Backup\nAt the end of simulation, the action values and visit counts of all traversed edges are updated. Each edge accumulates the visit count and mean evaluation of all simulations passing through that edge as following:\n$$ N(s, a) = \\sum_{i = 1}^{n} 1(s, a, i) $$\n$$ Q(s, a) = \\frac{1}{N(s, a)} \\sum_{i = 1}^{n} 1(s, a, i) V(s_L^i) $$\nwhere $s_L^i$ is the leaf node from the ith simulation and $1(s, a, i)$ indicates whether an edge $(s, a)$ was traversed during the ith simulation.\n  ","wordCount":"878","inLanguage":"en","image":"https://frankccccc.github.io/blog/img/just_imgs/alphago.webp","datePublished":"2021-02-19T01:14:40+08:00","dateModified":"2021-02-19T01:14:40+08:00","author":{"@type":"Person","name":"SY Chou"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://frankccccc.github.io/blog/posts/part_i-from_alphago_to_muzero/"},"publisher":{"@type":"Organization","name":"Golden Hat","logo":{"@type":"ImageObject","url":"https://frankccccc.github.io/blog/favicon.ico"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove('dark')</script><noscript><style type=text/css>#theme-toggle,.top-link{display:none}</style></noscript><header class=header><nav class=nav><div class=logo><a href=https://frankccccc.github.io/blog/ accesskey=h title="Golden Hat (Alt + H)"><img src=/blog/img/just_imgs/gold_empty_circle.svg alt=logo aria-label=logo height=35>Golden Hat</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu onscroll=menu_on_scroll()><li><a href=https://frankccccc.github.io/blog/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://frankccccc.github.io/blog/series title=Series><span>Series</span></a></li><li><a href=https://frankccccc.github.io/blog/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://frankccccc.github.io/blog/tags/ title=Tags><span>Tags</span></a></li></ul></nav><script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$'],['\\[','\\]']],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:['script','noscript','style','textarea','pre']}},window.addEventListener('load',a=>{document.querySelectorAll("mjx-container").forEach(function(a){a.parentElement.classList+='has-jax'})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://frankccccc.github.io/blog/>Home</a>&nbsp;»&nbsp;<a href=https://frankccccc.github.io/blog/posts/>Posts</a></div><h1 class=post-title>Part I - From AlphaGo to MuZero<div class=entry-isdraft><sup>&nbsp;&nbsp;[draft]</sup></div></h1><div class=post-meta>February 19, 2021&nbsp;·&nbsp;5 min&nbsp;·&nbsp;SY Chou</div></header><figure class=entry-cover><img src=https://frankccccc.github.io/blog/img/just_imgs/alphago.webp alt></figure><div class=post-content><p><strong>AlphaGo</strong> is quite famous when I was a freshman of college. It somehow is the reason that I was addicted to Reinforcement Learning. Thus Our journey of model-based RL will start here. Although it is not the first one that propose model-based RL, I still believe it will give a big picture of model-based RL.</p><h1 id=mastering-the-game-of-go-with-deep-neural-networks-and-tree-search>Mastering the game of Go with deep neural networks and tree search<a hidden class=anchor aria-hidden=true href=#mastering-the-game-of-go-with-deep-neural-networks-and-tree-search>#</a></h1><h3 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h3><p>AlphaGo combines 2 kinds of model, including <strong>policy network and value network</strong>. The policy network takes the <strong>board position as input and output the probability of next action of each position.</strong> The value network also take the board position as input and <strong>output the winner of the game.</strong></p><p>We pass in the board position as a <strong>19×19 image and use convolutional layers</strong> to construct a representation of the position. We use these neural networks to reduce the effective depth and breadth of the search tree: evaluating positions using a value network, and sampling actions using a policy network.</p><p>We train the model in <strong>2 stage</strong>. In the <strong>first stage, we use supervised learning with KGS dataset to train the policy network to predict the next action of humans.</strong> Then, in the <strong>second stage, we use reinforment learning and self-play to train the model by themself.</strong></p><h3 id=stage1-supervised-learning-of-policy-network>Stage1: Supervised Learning of Policy Network<a hidden class=anchor aria-hidden=true href=#stage1-supervised-learning-of-policy-network>#</a></h3><p><img src=/blog/img/alphago_to_muzero/alphago/sl_policy_network.png alt></p><p>A fast rollout policy $p_{\pi}$ and supervised learning (SL) policy network $p_{\sigma}$ are trained to predict human expert moves in a data set of positions. The fast rollout policy $p_{\pi}$ is trained only with some important features such as Stone colour to reduce the complexity of the model(faster but less accurate) while the SL policy network $p_{\sigma}$ is trained with whole position of Go.</p><p>We trained a 13-layer policy network, which we call the <strong>SL policy network</strong>, from 30 million positions from the KGS Go Server. Then we update the policy network with the following function to <strong>maximize the probability of predicting the action of human experts</strong>:</p><p>$$
\triangle \sigma \propto \frac{\partial log \ p_{\sigma}(a|s)}{\partial \sigma}
$$</p><h3 id=stage2-reinforcement-learning-of-policy-networks>Stage2: Reinforcement learning of policy networks<a hidden class=anchor aria-hidden=true href=#stage2-reinforcement-learning-of-policy-networks>#</a></h3><p>We use policy gradient reinforcement learning (RL) to update the network. The RL policy network $p_{\rho}$ is **identical in structure to the SL policy network**, its weights $\rho$ are **initialized to the same values**, $\rho = \sigma$. We play games between the current policy network $p_{\rho}$ and a **randomly selected previous iteration of the policy network to prevent overfit and stablize training**. To update the RL policy network, we use **policy gradient** to maximize the expected outcome:</p><p>$$
\triangle \rho \propto \frac{\partial log \ p_{\rho}(a_t|s_t)}{\partial \rho} z_t
$$</p><p>Here we use a reward function $r(s)$ that t is 0 for all non-terminal time steps $t&lt;T$. The outcome $z_t = \pm r(s_T)$ is the terminal reward at the end of the game if the current player wins, $r(s_T) = +1$, loses $r(s_T) = -1$.</p><h3 id=stage2-reinforcement-learning-of-value-networks>Stage2: Reinforcement learning of value networks<a hidden class=anchor aria-hidden=true href=#stage2-reinforcement-learning-of-value-networks>#</a></h3><p>Estimating a value function $v^p(s)$ that predicts the outcome from position $s$ of games played by using policy $p$ for both players</p><p>$$
v^p(s) = E[z_t | s_t = s, a_{t &mldr; T} \sim p]
$$</p><p>We approximate the value function using a value network $v_{\theta}(s)$ with weights $\theta$, $v_{\theta}(s) \approx v^{p_{\rho}}(s) \approx v^*(s)$.</p><p>We define the loss function of the value network with <em><strong>mean squared error(MSE)</strong></em>:</p><p>$$
\triangle \theta \propto \frac{\partial v_{\theta}(s)}{\partial \theta} (z - v_{\theta}(s))
$$</p><p>But how do we search the optimal value through policy network? There are 5 steps as Figure3:</p><p><img src=/blog/img/alphago_to_muzero/alphago/mcts.png alt></p><ul><li><p><strong>Step 1: Selection</strong></p><p>Each simulation traverses the tree by selecting the edge with maximum action value $Q$, plus a bonus $u(P)$ that depends on a stored prior probability $P(s, a)$ for that edge.</p></li><li><p><strong>Step 2: Expansion</strong></p><p>The leaf node may be expanded. The new node is processed once by the policy network $p_{\sigma}$ with output $P(s, a)=p_{\sigma}(a|s)$.</p><p>Each edge $(s, a)$ of the search tree stores an action value $Q(s, a)$, visit count $N(s, a)$, and prior probability $P(s, a)$.</p><p>The $u(s, a)$ is a kind of bonus that is proportional to the prior probability but decays with repeated visits to encourage exploration.</p><p>At each time step $t$ of each simulation, an action $a_t$ is selected from state $s_t$</p><p>$$
a_t = \mathop{\arg\max}_a (Q(s_t, a) + u(s_t, a))
$$</p><p>so as to maximize action value plus a bonus</p><p>$$
u(s, a) \propto \frac{P(s, a)}{1 + N(s, a)}
$$</p></li><li><p><strong>Step 3: Evaluation</strong></p><p>The leaf node is evaluated in two very different ways: first, by the value network $v_{\theta}(s_L)$; and second, by the outcome $z_L$ of a random rollout played out until terminal step $T$ using the fast rollout policy $p_{\pi}$; these evaluations are combined, using a mixing parameter $\lambda$, into a leaf evaluation $V(s_L)$.</p><p>$$
V(s_L) = (1 - \lambda) v_{\theta}(s_L) + \lambda z_L
$$</p></li><li><p><strong>Step 4: Backup</strong></p><p>At the end of simulation, the action values and visit counts of all traversed edges are <strong>updated</strong>. <strong>Each edge accumulates the visit count and mean evaluation of all simulations passing through that edge as following:</strong></p><p>$$
N(s, a) = \sum_{i = 1}^{n} 1(s, a, i)
$$</p><p>$$
Q(s, a) = \frac{1}{N(s, a)} \sum_{i = 1}^{n} 1(s, a, i) V(s_L^i)
$$</p><p>where $s_L^i$ is the leaf node from the ith simulation and $1(s, a, i)$ indicates whether an edge $(s, a)$ was traversed during the ith simulation.</p></li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://frankccccc.github.io/blog/tags/rl/>RL</a></li><li><a href=https://frankccccc.github.io/blog/tags/deep-learning/>deep learning</a></li><li><a href=https://frankccccc.github.io/blog/tags/model-based-rl/>model-based RL</a></li></ul><nav class=paginav><a class=prev href=https://frankccccc.github.io/blog/posts/some_intuition_of_mle_map_and_bayesian_estimation/><span class=title>« Prev Page</span><br><span>Some Intuition Of MLE, MAP, and Bayesian Estimation</span></a>
<a class=next href=https://frankccccc.github.io/blog/posts/a_glimpse_of_distributional_rl/><span class=title>Next Page »</span><br><span>A Glimpse of Distributional RL</span></a></nav></footer><div style=padding-top:2.5rem;padding-bottom:2.5rem;text-align:left><h1 style=padding-bottom:.5rem>COMMENTS</h1><h5>Your comments will encouage me to share more~~</h5></div><div id=utter-container></div><script src=https://utteranc.es/client.js repo=frankccccc/blog issue-term=title theme=photon-dark crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2021 <a href=https://frankccccc.github.io/blog/>Golden Hat</a></span>
<span>&#183;</span>
<span>Powered by <a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span>
<span>&#183;</span>
<span>Theme <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)"><button class=top-link id=top-link type=button accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></button></a>
<script defer src=/blog/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5+kdJvBz5iKbt6B5PJI=" onload=hljs.initHighlightingOnLoad()></script><script>window.onload=function(){localStorage.getItem("menu-scroll-position")&&(document.getElementById('menu').scrollLeft=localStorage.getItem("menu-scroll-position"))},document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})});var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")};function menu_on_scroll(){localStorage.setItem("menu-scroll-position",document.getElementById('menu').scrollLeft)}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script></body></html>