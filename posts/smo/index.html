<!doctype html><html lang=en dir=auto>
<head><meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=robots content="index, follow">
<title>A Review of SVM and SMO | Golden Hat</title>
<meta name=keywords content="SVM,machine learning,numerical optimization">
<meta name=description content="Note: full code is on my github.
1. Abstract In this article, I will derive SMO algorithm and the Fourier kernel approximation which are well-known algorithm for kernel machine. SMO can solve optimization problem of SVM efficiently and the Fourier kernel approximation is a kind of kernel approximation that can speed up the computation of the kernel matrix. In the last section, I will conduct a evaluation of my manual SVM on the simulation dataset and &ldquo;Women&rsquo;s Clothing E-Commerce Review Dataset&rdquo;.">
<meta name=author content="SY Chou">
<link rel=canonical href=https://frankccccc.github.io/blog/posts/smo/>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-S9MCZ2NDS7"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','G-S9MCZ2NDS7')</script>
<link href=/blog/assets/css/stylesheet.min.3768d020b7a9fd49f21ddb7d521ce9b187ce47c2929b883041396159d4d25553.css integrity="sha256-N2jQILep/UnyHdt9UhzpsYfOR8KSm4gwQTlhWdTSVVM=" rel="preload stylesheet" as=style>
<link rel=icon href=https://frankccccc.github.io/blog/img/just_imgs/gold_empty_circle.svg>
<link rel=icon type=image/png sizes=16x16 href=https://frankccccc.github.io/blog/favicon-16x16.png>
<link rel=icon type=image/png sizes=32x32 href=https://frankccccc.github.io/blog/favicon-32x32.png>
<link rel=apple-touch-icon href=https://frankccccc.github.io/blog/apple-touch-icon.png>
<link rel=mask-icon href=https://frankccccc.github.io/blog/safari-pinned-tab.svg>
<meta name=theme-color content="#2e2e33">
<meta name=msapplication-TileColor content="#2e2e33">
<meta name=generator content="Hugo 0.92.1">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-S9MCZ2NDS7"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','G-S9MCZ2NDS7')</script><meta property="og:title" content="A Review of SVM and SMO">
<meta property="og:description" content="Note: full code is on my github.
1. Abstract In this article, I will derive SMO algorithm and the Fourier kernel approximation which are well-known algorithm for kernel machine. SMO can solve optimization problem of SVM efficiently and the Fourier kernel approximation is a kind of kernel approximation that can speed up the computation of the kernel matrix. In the last section, I will conduct a evaluation of my manual SVM on the simulation dataset and &ldquo;Women&rsquo;s Clothing E-Commerce Review Dataset&rdquo;.">
<meta property="og:type" content="article">
<meta property="og:url" content="https://frankccccc.github.io/blog/posts/smo/">
<meta property="og:image" content="https://frankccccc.github.io/blog/img/just_imgs/snow_forest.jpg"><meta property="article:published_time" content="2021-07-08T12:39:16+08:00">
<meta property="article:modified_time" content="2021-07-08T12:39:16+08:00">
<meta name=twitter:card content="summary_large_image">
<meta name=twitter:image content="https://frankccccc.github.io/blog/img/just_imgs/snow_forest.jpg">
<meta name=twitter:title content="A Review of SVM and SMO">
<meta name=twitter:description content="Note: full code is on my github.
1. Abstract In this article, I will derive SMO algorithm and the Fourier kernel approximation which are well-known algorithm for kernel machine. SMO can solve optimization problem of SVM efficiently and the Fourier kernel approximation is a kind of kernel approximation that can speed up the computation of the kernel matrix. In the last section, I will conduct a evaluation of my manual SVM on the simulation dataset and &ldquo;Women&rsquo;s Clothing E-Commerce Review Dataset&rdquo;.">
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://frankccccc.github.io/blog/posts/"},{"@type":"ListItem","position":2,"name":"A Review of SVM and SMO","item":"https://frankccccc.github.io/blog/posts/smo/"}]}</script>
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"A Review of SVM and SMO","name":"A Review of SVM and SMO","description":"Note: full code is on my github.\n1. Abstract In this article, I will derive SMO algorithm and the Fourier kernel approximation which are well-known algorithm for kernel machine. …","keywords":["SVM","machine learning","numerical optimization"],"articleBody":"Note: full code is on my github.\n1. Abstract In this article, I will derive SMO algorithm and the Fourier kernel approximation which are well-known algorithm for kernel machine. SMO can solve optimization problem of SVM efficiently and the Fourier kernel approximation is a kind of kernel approximation that can speed up the computation of the kernel matrix. In the last section, I will conduct a evaluation of my manual SVM on the simulation dataset and “Women’s Clothing E-Commerce Review Dataset”.\n2. Sequential Minimal Optimization(SMO) The SMO(Sequential Minimal Optimization) algorithm is proposed from the paper Sequential Minimal Optimization: A Fast Algorithm for Training Support Vector Machines in 1998 by J. Platt. In short, SMO picks 2 variables $\\alpha_i, \\alpha_j$ for every iteration, regulate them to satisfy KKT condition and, update them. In the following article, I will derive the whole algorithm and provide the evaluation on the simulation and real dataset.\nWe’ve known he dual problem of soft-SVM is\n$$ \\sup_{\\alpha} \\sum_{i=1}^{N} \\alpha_i - \\frac{1}{2} \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\alpha_i \\alpha_j y_i y_j k(x_i, x_j) \\ \\newline \\text{subject to} \\ 0 \\leq \\alpha_i \\leq C, \\sum_{i=1}^{N} \\alpha_i y_i= 0 $$\nWe also define the kernel.\n$$ k(x_i, x_j) = \\langle \\phi(x_i), \\phi(x_j) \\rangle $$\nwhere $\\phi$ is an embedding function projecting the data points to a high dimensional space.\nHowever, it’s very hard to solve because we need to optimize $N$ variables. As a result, J. Platt proposed SMO to solve this problem efficiently.\n2.1 Notation We denote the target function as $\\mathcal{L}(\\alpha, C)$\n$$ \\mathcal{L} (\\alpha) = \\sum_{i=1}^{N} \\alpha_i - \\frac{1}{2} \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\alpha_i \\alpha_j y_i y_j k(x_i, x_j) $$\nWe also denote the kernel of $x_1, x_2$ as $K_{1, 2} = k(x_1, x_2)$.\n2.2 Step 1. Update 2 Variable First, we need to pick 2 variables to update in sequence, so we split the variables $\\alpha_1, \\alpha_2$ from the summation.\n$$ \\mathcal{L}(\\alpha) = \\alpha_1 + \\alpha_2 - \\frac{1}{2} \\alpha_1^2 y_1^2 K_{1,1} - \\frac{1}{2} \\alpha_2^2 y_2^2 K_{2,2} \\newline -\\frac{1}{2} \\alpha_1 \\alpha_2 y_1 y_2 K_{1, 2} - \\frac{1}{2} \\alpha_2 \\alpha_1 y_2 y_1 K_{2, 1} \\newline -\\frac{1}{2} \\alpha_1 y_1 \\sum_{i=3}^{N} \\alpha_i y_i K_{i,1} -\\frac{1}{2} \\alpha_1 y_1 \\sum_{i=3}^{N} \\alpha_i y_i K_{1, i} \\newline -\\frac{1}{2} \\alpha_2 y_2 \\sum_{i=3}^{N} \\alpha_i y_i K_{i,2} -\\frac{1}{2} \\alpha_2 y_2 \\sum_{i=3}^{N} \\alpha_i y_i K_{2, i} \\newline +\\sum_{i=3}^{N} \\alpha_i - \\frac{1}{2} \\sum_{i=3}^{N} \\sum_{j=3}^{N} \\alpha_i \\alpha_j y_i y_j k(x_i, x_j) $$\n$$ = \\alpha_1 + \\alpha_2 - \\frac{1}{2} \\alpha_1^2 y_1^2 K_{1,1} - \\frac{1}{2} \\alpha_2^2 y_2^2 K_{2,2} - \\alpha_1 \\alpha_2 y_1 y_2 K_{1, 2} \\newline -\\alpha_1 y_1 \\sum_{i=3}^{N} \\alpha_i y_i K_{i,1} - \\alpha_2 y_2 \\sum_{i=3}^{N} \\alpha_i y_i K_{i,2} + \\mathcal{Const} $$\n$$ = \\alpha_1 + \\alpha_2 - \\frac{1}{2} \\alpha_1^2 K_{1,1} - \\frac{1}{2} \\alpha_2^2 K_{2,2} - \\alpha_1 \\alpha_2 y_1 y_2 K_{1, 2} \\newline -\\alpha_1 y_1 \\sum_{i=3}^{N} \\alpha_i y_i K_{i,1} - \\alpha_2 y_2 \\sum_{i=3}^{N} \\alpha_i y_i K_{i,2} + \\mathcal{Const} $$\nwhere $\\mathcal{Const} = \\sum_{i=3}^{N} \\alpha_i - \\frac{1}{2} \\sum_{i=3}^{N} \\sum_{j=3}^{N} \\alpha_i \\alpha_j y_i y_j k(x_i, x_j)$. We see it as a constant because it is regardless to $\\alpha_1, \\alpha_2$.\n2.2.1 The Relation Between The Update Values and The Hyperplane\nWe’ve derive the partial derivative of the dual problem.\n$$ \\frac{\\partial L(w, b, \\xi, \\alpha, \\mu)}{\\partial w} = w - \\sum_{i=1}^N \\alpha_i y_i x_i = 0 $$\nWe can get\n$$ w = \\sum_{i=1}^N \\alpha_i y_i x_i $$\nThus, we can rewrite the hyperplane $f_{\\phi}(x)$ with kernel.\n$$ f_{\\phi}(x) = w^{\\top} \\phi(x) + b = b + \\sum_{i=1}^N \\alpha_i y_i k(x_i, x) $$\nThe corresponding code:\ndef __f(self, i): return snp.dot((self.alpha * self.y), self.K[i, :]) + self.b We also denote $v_1, v_2$ as\n$$ v_1 = \\sum_{i=3}^{N} \\alpha_i y_i K_{i,1} = \\sum_{i=1}^{N} \\alpha_i y_i k(x_i, x_1) - \\alpha_1^{old} y_1 k(x_1, x_1) - \\alpha_2^{old} y_2 k(x_2, x_1) $$\n$$ = f_{\\phi}(x_1) - b - \\alpha_1^{old} y_1 K_{1, 1} - \\alpha_2^{old} y_2 K_{2, 1} $$\nand $v_2$ is similar.\n$$ v_2 = \\sum_{i=3}^{N} \\alpha_i y_i K_{i,2} = \\sum_{i=1}^{N} \\alpha_i y_i k(x_i, x_2) - \\alpha_1^{old} y_1 k(x_1, x_2) - \\alpha_2^{old} y_2 k(x_2, x_2) $$\n$$ = f_{\\phi}(x_2) - b - \\alpha_1^{old} y_1 K_{1, 2} - \\alpha_2^{old} y_2 K_{2, 2} $$\nwhere $\\alpha_1^{old}$ and $\\alpha_2^{old}$ are $\\alpha_1$ and $\\alpha_2$ of the previous iteration. Since we see $\\alpha_i, i \\geq 3$ as constant, $\\alpha_i$ shouldn’t depends on update variables $\\alpha_1, \\alpha_2$.\n2.2.2 Rewrite The Complementary Slackness\nThe constraint can be represented as\n$$ \\sum_{i=1}^{N} \\alpha_i y_i = \\alpha_1 y_1 + \\alpha_2 y_2 + \\sum_{i=3}^{N} \\alpha_i y_i = 0 $$\n$$ \\alpha_1 y_1 + \\alpha_2 y_2 = - \\sum_{i=3}^{N} \\alpha_i y_i = \\zeta $$\n$$ \\alpha_1 = \\frac{\\zeta - \\alpha_2 y_2}{y_1} $$\nSince $y_1$ is either 1 or -1, thus\n$$ \\alpha_1 = \\zeta y_1 - \\alpha_2 y_1 y_2 $$\nThe old ones are the same.\n$$ \\alpha_1^{old} = \\zeta y_1 - \\alpha_2^{old} y_1 y_2 $$\nReplace the symbol $\\alpha_1, v_1, v_2$\n$$ \\mathcal{L}(\\alpha) = (\\zeta y_1 - \\alpha_2 y_1 y_2) + \\alpha_2 \\newline -\\frac{1}{2} (\\zeta y_1 - \\alpha_2 y_1 y_2)^2 K_{1,1} - \\frac{1}{2} \\alpha_2^2 K_{2,2} - (\\zeta y_1 - \\alpha_2 y_1 y_2) \\alpha_2 y_1 y_2 K_{1, 2} \\newline -(\\zeta y_1 - \\alpha_2 y_1 y_2) y_1 v_1 - \\alpha_2 y_2 v_2 $$\n$$ = (\\zeta y_1 - \\alpha_2 y_1 y_2) + \\alpha_2 \\newline -\\frac{1}{2} (\\zeta^2 + \\alpha_2^2 - 2 \\zeta \\alpha_2 y_2) K_{1,1} - \\frac{1}{2} \\alpha_2^2 K_{2,2} - (\\zeta \\alpha_2 y_2 - \\alpha_2^2) K_{1, 2} \\newline -(\\zeta - \\alpha_2 y_2) v_1 - \\alpha_2 y_2 v_2 $$\n2.2.3 Combine the $v_1$, $v_2$ and $\\zeta$\n$$ v_1 - v_2 = [ \\ f_{\\phi}(x_1) - b - \\alpha_1^{old} y_1 K_{1, 1} - \\alpha_2^{old} y_2 K_{2, 1} \\ ] - [ \\ f_{\\phi}(x_2) - b - \\alpha_1^{old} y_1 K_{1, 2} - \\alpha_2^{old} y_2 K_{2, 2} \\ ] $$\n$$ = [ \\ f_{\\phi}(x_1) - b - ( \\zeta y_1 - \\alpha_2^{old} y_1 y_2) y_1 K_{1, 1} - \\alpha_2^{old} y_2 K_{2, 1} \\ ] - [ \\ f_{\\phi}(x_2) - b - ( \\zeta y_1 - \\alpha_2^{old} y_1 y_2) y_1 K_{1, 2} - \\alpha_2^{old} y_2 K_{2, 2} \\ ] $$\n$$ = [ \\ f_{\\phi}(x_1) - f_{\\phi}(x_2) \\ ] + [ \\ - ( \\zeta - \\alpha_2^{old} y_2) K_{1, 1} - \\alpha_2^{old} y_2 K_{2, 1} \\ ] - [ \\ - ( \\zeta - \\alpha_2^{old} y_2) K_{1, 2} - \\alpha_2^{old} y_2 K_{2, 2} \\ ] $$\n$$ = [ \\ f_{\\phi}(x_1) - f_{\\phi}(x_2) \\ ] + [ \\ - \\zeta K_{1, 1} + \\alpha_2^{old} y_2 K_{1, 1} - \\alpha_2^{old} y_2 K_{2, 1} \\ ] - [ \\ - \\zeta K_{1, 2} + \\alpha_2^{old} y_2 K_{1, 2} - \\alpha_2^{old} y_2 K_{2, 2} \\ ] $$\n$$ = f_{\\phi}(x_1) - f_{\\phi}(x_2) - \\zeta K_{1, 1} + \\zeta K_{1, 2} + ( K_{1, 1} + K_{2, 2} - 2 K_{1, 2} ) \\alpha_2^{old} y_2 $$\n2.2.4 Derive Gradient of $\\alpha_2$\n$$ \\frac{\\partial \\mathcal{L}(\\alpha)}{\\partial \\alpha_2} = -y_1 y_2 + 1 - \\frac{1}{2} (2 \\alpha_2 - 2 \\zeta y_2) K_{1,1} - \\alpha_2 K_{2, 2} - (\\zeta y_2 - 2 \\alpha_2) K_{1, 2} - (- y_2) v_1 - y_2 v_2 $$\n$$ = (- \\alpha_2 K_{1, 1} - \\alpha_2 K_{2, 2} + 2 \\alpha_2 K_{1, 2}) + \\zeta y_2 K_{1, 1}- \\zeta y_2 K_{1, 2} - y_1 y_2 + y_2 v_1 - y_2 v_2 + 1 $$\n$$ = -\\alpha_2 (K_{1, 1} + K_{2, 2} - 2 K_{1, 2}) + \\zeta y_2 K_{1, 1}- \\zeta y_2 K_{1, 2} - y_1 y_2 + y_2(v_1 - v_2) + 1 $$\nReplace $v_1 - v_2$ containing old $\\alpha_1^{old}, \\alpha_2^{old}$ (derived in 2.2.3)\n$$ \\frac{\\partial \\mathcal{L}(\\alpha)}{\\partial \\alpha_2} = -\\alpha_2 (K_{1, 1} + K_{2, 2} - 2 K_{1, 2}) + \\zeta y_2 K_{1, 1}- \\zeta y_2 K_{1, 2} - y_1 y_2 + y_2 [ \\ f_{\\phi}(x_1) - f_{\\phi}(x_2) - \\zeta K_{1, 1} + \\zeta K_{1, 2} + ( K_{1, 1} + K_{2, 2} - 2 K_{1, 2}) \\alpha_2^{old} y_2 \\ ] + 1 $$\n$$ = -(K_{1, 1} + K_{2, 2} - 2 K_{1, 2}) \\alpha_2 + ( K_{1, 1} + K_{2, 2} - 2 K_{1, 2} ) \\alpha_2^{old} + y_2 (f_{\\phi}(x_1) - f_{\\phi}(x_2) + y_2 - y_1) $$\nLet $\\eta$ and $E_i$ be\n$$ \\eta = K_{1, 1} + K_{2, 2} - 2 K_{1, 2}, \\quad E_i = f_{\\phi}(x_i) - y_i $$\n$$ \\frac{\\partial \\mathcal{L}_d(\\alpha)}{\\partial \\alpha_2} = - \\eta \\alpha_2 + \\eta \\alpha_2^{old} + y_2 (E_1 - E_2) $$\nSince we want to minimize the gradient, let the gradient be 0.\n$$ -\\eta \\alpha_2 + \\eta \\alpha_2^{old} + y_2 (E_1 - E_2) = 0 $$\nThen we can find the relation between new and old $\\alpha_2$ as following\n$$ \\alpha_2 = \\alpha_2^{old} + \\frac{y_2 (E_1 - E_2)}{\\eta} $$\nTo make the notation more clear to identify, we denote $\\alpha_2^{new}$ as the new value of the update.\n$$ \\alpha_2^{new} = \\alpha_2^{old} + \\frac{y_2 (E_1 - E_2)}{\\eta} $$\nThe corresponding code:\ndef __E(self, i): return self.__f(i) - self.y[i] def __eta(self, i, j): return self.K[i, i] + self.K[j, j] - 2 * self.K[i, j] def __alpha_j_new(self, i, j): E_i = self.__E(i) E_j = self.__E(j) eta = self.__eta(i, j) return self.alpha[j] + (self.y[j] * (E_i - E_j) / eta), E_i, E_j, eta 2.3 Step 2. Clip with Bosk Constraint The new values should satisfy the complementary slackness as\n$$ \\alpha_1 y_1 + \\alpha_2 y_2 = \\zeta, \\quad 0 \\leq \\alpha_i \\leq C $$\nSince $y_1, y_2$ may have different labels, thus we consider 2 cases. The first case is $y_1 \\neq y_2$ as the left part of the figure 1 and another case is $y_1 = y_2$ which corresponds to he right part of the figure.\nNote that there is another line in quadrant 3 in the case 2 but it doesn’t show in the figure due to the limit of the size.\nFigure 1\n2.3.1 Case 1: Inequality\nWhen $y_1 \\neq y_2$, the equation is either $\\alpha_1 - \\alpha_2 = k$ or $\\alpha_1 - \\alpha_2 = -k$ where $k = |\\zeta|$ is a positive constant.\nFirst, we consider the blue area $\\alpha_1 - \\alpha_2 = -k$. We can see $\\alpha_1 \\in [C, k] = [C, \\alpha_2 - \\alpha_1]$. The upper bound should be $C$ and the lower bound should be $\\alpha_2 - \\alpha_1$.\n$$ B_U = C, \\ B_L = \\alpha_2 - \\alpha_1 $$\nNext, we consider the grey area $\\alpha_1 - \\alpha_2 = k$. We can see $\\alpha_1 \\in [0, C-k] = [0, C + \\alpha_2 - \\alpha_1]$. The upper bound should be $C + \\alpha_2 - \\alpha_1$ and the lower bound should be 0.\n$$ B_U = C + \\alpha_2 - \\alpha_1, \\ B_L = 0 $$\nCombine 2 cases, both new and old values should satisfy the bosk constraint. The upper bound of $\\alpha_2^{new}$ can be written as\n$$ B_U = \\min(C, C + \\alpha_2^{old} - \\alpha_1^{old}) $$\nand the lower bound is\n$$ B_L = \\max(0, \\alpha_2^{old} - \\alpha_1^{old}) $$\n2.3.2 Case 2: Equality\nWhen $y_1 = y_2$, the equation is either $\\alpha_1 + \\alpha_2 = k$ or $\\alpha_1 + \\alpha_2 = -k$ where $k$ is a positive constant.\nIn similar way, we can derive the case of equality. The upper bound can be written as\n$$ B_U = \\min(C, \\alpha_2^{old} + \\alpha_1^{old}) $$\nand the lower bound is\n$$ B_L = \\max(0, \\alpha_2^{old} + \\alpha_1^{old} - C) $$\n2.3.3 Clip The Value\nAccording the bound we’ve derived, we need clip the updated variable $\\alpha_2^{new}$ to satisfy the constraint. In addition, we denote the new value after clipping as $\\alpha_2^*$.\n$$ \\alpha_2^* = CLIP(\\alpha_2^{new}, B_L, B_U) $$\nThe corresponding code:\ndef __bound(self, i, j): if self.y[i] == self.y[j]: B_U = min(self.C, self.alpha[j] + self.alpha[i]) B_L = max(0, self.alpha[j] + self.alpha[i] - self.C) else: B_U = min(self.C, self.C + self.alpha[j] - self.alpha[i]) B_L = max(0, self.alpha[j] - self.alpha[i]) return B_U, B_L def __update_alpha_j(self, i, j): B_U, B_L = self.__bound(i, j) alpha_j_star, E_i, E_j, eta = self.__alpha_j_new(i, j) return np.clip(alpha_j_star, B_L, B_U), E_i, E_j, eta 2.3.4 Update $\\alpha_1$\nWe’ve know the complementary slackness.\n$$ \\alpha_1^* y_1 + \\alpha_2^* y_2 = \\alpha_1^{old} y_1 + \\alpha_2^{old} y_2 = \\zeta $$\nMove the updated value $\\alpha_1^*$ to the left side and we can get\n$$ \\alpha_1^* = \\frac{\\alpha_1^{old} y_1 + \\alpha_2^{old} y_2 - \\alpha_2^* y_2}{y_1} $$\n$$ \\alpha_1^* = \\alpha_1^{old} + y_1 y_2(\\alpha_2^{old} - \\alpha_2^*) $$\nThe corresponding code:\ndef __update_alpha_i(self, i, j, alpha_j_star): return self.alpha[i] + self.y[i] * self.y[j] * (self.alpha[j] - alpha_j_star) 2.4 Step 3. Update Bias The only equation that contains bias $b$ is the function $f_{\\phi}(x) = b + \\sum_{i=1}^N \\alpha_i y_i k(x_i, x)$. When $0 \\lt \\alpha_i^* \\lt C$, it means that the data point $x_i$ is right on the margin such that $f_{\\phi}(x)=y_i$, $f_{\\phi}^(x_i) = y_i$ and the bias $b_1^, b_2^$ can be derived directly. Note that for convenience, $f_{\\phi}^(x_w) = \\sum_{i=3}^N \\alpha_i y_i K_{i, w} - \\alpha_1^* y_1 K_{1, w} - \\alpha_2^* y_2 K_{2, w} + b^* = y_w$ contains updated variables $\\alpha_2^, \\alpha_2^, b^*$.\nIf $0 $$ b_1^* = y_1 - \\sum_{i=3}^N \\alpha_i y_i K_{i, 1} - \\alpha_1^* y_1 K_{1, 1} - \\alpha_2^* y_2 K_{2, 1} $$\n$$ = (y_1 - f_{\\phi}(x_1) + \\alpha_1^{old} y_1 K_{1, 1} + \\alpha_2^{old} y_2 K_{2, 1} + b) - \\alpha_1^* y_1 K_{1, 1} - \\alpha_2^* y_2 K_{2, 1} $$\n$$ = -E_1 - y_1 K_{1, 1} (\\alpha_1^* - \\alpha_1^{old}) - y_2 K_{2, 1} (\\alpha_2^* - \\alpha_2^{old}) + b $$\nIf $0 $$ b_2^* = y_2 - \\sum_{i=3}^N \\alpha_i y_i K_{i, 2} - \\alpha_1^* y_1 K_{1, 2} - \\alpha_2^* y_2 K_{2, 2} $$\n$$ = (y_2 - f_{\\phi}(x_2) + \\alpha_1^{old} y_1 K_{1, 2} + \\alpha_2^{old} y_2 K_{2, 2} + b) - \\alpha_1^* y_1 K_{1, 2} - \\alpha_2^* y_2 K_{2, 2} $$\n$$ = -E_2 - y_1 K_{1, 2} (\\alpha_1^* - \\alpha_1^{old}) - y_2 K_{2, 2} (\\alpha_2^* - \\alpha_2^{old}) + b $$\nWhen the data point $x_i, x_j$ are both not on the margin, we choose the average of $b_1^* \\ and \\ b_2^*$ as the updated value.\n$$ b^* = \\frac{b_1^* + b_2^*}{2} $$\nThe code of updating bias.\ndef __update_b(self, i, j, alpha_i_star, alpha_j_star, E_i, E_j): b_star = 0 b_i_star = -E_i - self.y[i] * self.K[i, i] * (alpha_i_star - self.alpha[i]) - self.y[j] * self.K[j, i] * (alpha_j_star - self.alpha[j]) + self.b b_j_star = -E_j - self.y[i] * self.K[i, j] * (alpha_i_star - self.alpha[i]) - self.y[j] * self.K[j, j] * (alpha_j_star - self.alpha[j]) + self.b if alpha_i_star  self.C and alpha_i_star = 0: b_star = b_i_star elif alpha_j_star  self.C and alpha_j_star = 0: b_star = b_j_star else: b_star = (b_i_star + b_j_star) / 2 return b_star For more detail, please see the pseudo code.\n2.5 Pseudo Code  Given $C$, otherwise the default value is $C = 5$\nGiven $\\epsilon$, otherwise the default value is $\\epsilon = 10^{-6}$\nGiven $\\text{max-iter}$, otherwise the default value is $\\text{max-iter} = 10^{3}$\nFor all $\\alpha_i = 0, 1 \\leq i \\leq N$\n$b = 0$\n$move = \\infty$\nwhile($move  \\epsilon$ and $iter \\leq \\text{max-iter}$):\n  $\\alpha_1^* = \\alpha_2^* = b^* = move = 0$\n  for($n$ in $N/2$):\n  Choose the index $i, j$ from 1 to $N$\n  $E_i = f(x_i) - y_i$\n  $E_j = f(x_j) - y_j$\n  $\\eta = K_{i, i} + K_{j, j} - 2 K_{i, j}$\n  $\\alpha_j^{new} = \\alpha_j + \\frac{y_j (E_i - E_j)}{\\eta}$\nBosk Constraint\n  if($y_i = y_j$):\n $B_U = \\min(C, \\alpha_j + \\alpha_i)$ $B_L = \\max(0, \\alpha_j + \\alpha_i - C)$    else:\n $B_U = \\min(C, C + \\alpha_j - \\alpha_i)$ $B_L = \\max(0, \\alpha_j - \\alpha_i)$    $\\alpha_j^* = CLIP(\\alpha_j^{new}, B_L, B_U)$\n  $\\alpha_i^* = \\alpha_i + y_i y_j(\\alpha_j - \\alpha_j^*)$\nUpdate Bias\n  $b_i^* = - E_i - y_i K_{i, i} (\\alpha_i^* - \\alpha_i) - y_j K_{j, i} (\\alpha_j^* - \\alpha_j) + b$\n  $b_j^* = - E_j - y_i K_{i, j} (\\alpha_i^* - \\alpha_i) - y_j K_{j, j} (\\alpha_j^* - \\alpha_j) + b$\n  if($0 \\leq \\alpha_i \\leq C$):\n $b^* = b_i^*$    else if($0 \\leq \\alpha_j \\leq C$):\n $b^* = b_j^*$    else:\n $b^* = \\frac{b_i^* + b_j^*}{2}$    $move = move + |\\alpha_1^* - \\alpha_1| + |\\alpha_2^* - \\alpha_2| + |b^* - b|$\n  Let $\\alpha_i = \\alpha_i^* \\quad \\alpha_j = \\alpha_j^* \\quad b = b^*$\n    $iter = iter + 1$\n   Here is the Python code:\ndef fit(self, X, y): self.X = np.array(X) self.y = np.reshape(np.array(y), (-1, )) self.n, self.dim = self.X.shape self.K = self.cal_kernel(self.X) self.alpha = np.zeros(self.n) self.b = 0 iter = 0 loss = np.inf move = np.inf while iter  self.max_iter and move  self.epsilon: loss = move = 0 for i in range(self.n): j = self.__choose_j(i) alpha_j_star, E_i, E_j, eta = self.__update_alpha_j(i, j) if eta  0: self.warning('Eta ) continue alpha_i_star = self.__update_alpha_i(i, j, alpha_j_star) if abs(alpha_j_star - self.alpha[j])  0.00001: self.warning('alpha_j not moving enough') continue b_star = self.__update_b(i, j, alpha_i_star, alpha_j_star, E_i, E_j) # Calculate the movement of alpha and b move = move + abs(alpha_i_star - self.alpha[i]) + abs(alpha_j_star - self.alpha[j]) + abs(b_star - self.b) # Update variables self.alpha[i] = alpha_i_star self.alpha[j] = alpha_j_star self.b = b_star # Calculate the loss loss = sum(map(lambda x: abs(self.__E(x)), np.arange(self.n))) # Calculate the accuracy acc = self.acc() self.loss_history.append(loss) self.move_history.append(move) self.acc_history.append(acc) # if not skip: iter += 1 self.info(\"Iter: \", iter, \" | Loss: \", loss, \" | Move: \", move, \" | Acc: \", acc) 3. Fourier Kernel Approximation The Fourier kernel approximation is proposed from the paper Random Features for Large-Scale Kernel Machines on NIPS'07. It’s a widely-used approximation to accelerate the kernel computing especially for the high dimensional dataset. For a dataset with dimension $D$ and data points $N$, the time complexity of computing the exact kernel is $\\mathcal{O}(DN^2)$ and the Fourier kernel approximation is $\\mathcal{O}(SN^3)$ with $S$ samples. While the dimension goes up, the approximation remains the same computing time because it is regardless to the dimension of the dataset.\n3.1 Bochner’s Theorem If $\\phi: \\mathbb{R}^n \\to \\mathbb{C}$ is a positive definite, continuous, and satisfies $\\phi(0)=1$, then there is some Borel probability measure $\\mu \\in \\mathbb{R}^n$ such that $\\phi = \\hat{\\mu}$\nThus, we can extend the Bochner’s theorem to kernel.\n3.2 Theorem 1 According to Bochner’s theorem, a continuous kernel $k(x, y) = k(x-y) \\in \\mathbb{R}^d$ is positive definite if and only if $k(\\delta)$ is the Fourier transform of a non-negative measure.\nIf a shift-invariant kernel $k(\\delta)$ is a properly scaled, Bochner’s theorem guarantees that its Fourier transform $p(\\omega)$ is a proper probability distribution. Defining $\\zeta_{\\omega}(x) = e^{j \\omega' x}$, we have\n$$ k(x-y) = \\int_{\\omega} p(\\omega) e^{j \\omega' (x - y)} d \\omega = E_{\\omega}[\\zeta_{\\omega}(x) \\zeta_{\\omega}(y)] $$\nwhere $\\zeta_{\\omega}(x) \\zeta_{\\omega}(y)$ is an unbiased estimate of $k(x, y)$ when $\\omega$ is drawn from $p(\\omega)$.\nWith Mote-Carlo simulation, we can approximate the integral with the summation over the probability $p(\\omega)$.\n$$ z(x)' z(y) = \\frac{1}{D} \\sum_{j=1}^D \\mathbb{z}{w_j}(x) \\mathbb{z}{w_j}(y) $$\n$$ \\mathbb{z}_{\\omega}(x) = \\sqrt{2} cos(\\omega x + b) \\ \\text{where} \\ \\omega \\sim p(\\omega) $$\nIn order to approximate the RBF kernel $k(k, y) = e^{-\\frac{||x - y||_2^2}{2}}$, we draw $\\omega$ from Fourier transformed distribution $p(\\omega) = \\mathcal{N}(0, 1)$.\n4. Experiments 4.1 Simulation With Exact Kernel The parameters of SVM:\n C: 0.6 $\\gamma$ of RBF: 2  Here we generate 3 kinds of data. The first row is generated by a Gaussian mixture model. The second row is like a moon generated by Scikit-Learn package. The third one is also generated by Scikit-Learn package and the package generate 2 circles, one is in the inner side and the other one is in the outer side.\nThe SMO and kernel seem work properly even under noise and nonlinear dataset.\n4.2 Simulation With Approximated Kernel We draw 200 samples from $p(\\omega)$ to approximate the RBF kernel. As we can see, the testing accuracies are close to the ones of exact kernels in most of cases.\n4.3 Real Dataset 4.3.1 PCA Preprocess\nApply SVM on the “Women’s Clothing E-Commerce Review Dataset” with C = 0.6 and $\\gamma$ of RBF kernel = 2, the training accuracy is 82.03% and the testing accuracy is 81.54%. The accuracy, loss and, the movement of variables are showed in the following graph.\nAs we can see, the movement of variable gets smaller during training and converge around 50 and the accuracy remains about 82%.\n4.3.2 LDA Preprocess\nThe training accuracy is also 82.03% and the testing accuracy is 81.54%, but the curves are smoother than the ones of PCA.\n","wordCount":"3363","inLanguage":"en","image":"https://frankccccc.github.io/blog/img/just_imgs/snow_forest.jpg","datePublished":"2021-07-08T12:39:16+08:00","dateModified":"2021-07-08T12:39:16+08:00","author":{"@type":"Person","name":"SY Chou"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://frankccccc.github.io/blog/posts/smo/"},"publisher":{"@type":"Organization","name":"Golden Hat","logo":{"@type":"ImageObject","url":"https://frankccccc.github.io/blog/img/just_imgs/gold_empty_circle.svg"}}}</script>
</head>
<body class=dark id=top>
<script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove('dark')</script>
<noscript>
<style type=text/css>#theme-toggle,.top-link{display:none}</style>
</noscript>
<header class=header>
<nav class=nav>
<div class=logo>
<a href=https://frankccccc.github.io/blog/ accesskey=h title="Golden Hat (Alt + H)">
<img src=/blog/img/just_imgs/gold_empty_circle.svg alt=logo aria-label=logo height=35>Golden Hat</a>
<span class=logo-switches>
<button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
</button>
</span>
</div>
<ul id=menu onscroll=menu_on_scroll()>
<li>
<a href=https://frankccccc.github.io/blog/search/ title="Search (Alt + /)" accesskey=/>
<span>Search</span>
</a>
</li>
<li>
<a href=https://frankccccc.github.io/blog/series title=Series>
<span>Series</span>
</a>
</li>
<li>
<a href=https://frankccccc.github.io/blog/categories/ title=Categories>
<span>Categories</span>
</a>
</li>
<li>
<a href=https://frankccccc.github.io/blog/tags/ title=Tags>
<span>Tags</span>
</a>
</li></ul>
</nav>
<script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$'],['\\[','\\]']],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:['script','noscript','style','textarea','pre']}},window.addEventListener('load',a=>{document.querySelectorAll("mjx-container").forEach(function(a){a.parentElement.classList+='has-jax'})})</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script>
</header>
<main class=main>
<article class=post-single>
<header class=post-header>
<div class=breadcrumbs>
<a href=https://frankccccc.github.io/blog/>Home</a>&nbsp;»&nbsp;<a href=https://frankccccc.github.io/blog/posts/>Posts</a>
</div>
<h1 class=post-title>
A Review of SVM and SMO
</h1>
<div class=post-meta>
July 8, 2021&nbsp;·&nbsp;16 min&nbsp;·&nbsp;SY Chou
</div>
</header>
<figure class=entry-cover><img src=https://frankccccc.github.io/blog/img/just_imgs/snow_forest.jpg alt>
</figure>
<div class=toc>
<details open>
<summary accesskey=c title="(Alt + C)">
<div class=details>Table of Contents</div>
</summary>
<div class=inner><ul><li>
<a href=#1-abstract aria-label="1. Abstract">1. Abstract</a></li><li>
<a href=#2-sequential-minimal-optimizationsmo aria-label="2. Sequential Minimal Optimization(SMO)">2. Sequential Minimal Optimization(SMO)</a><ul>
<li>
<a href=#21-notation aria-label="2.1 Notation">2.1 Notation</a></li><li>
<a href=#22-step-1-update-2-variable aria-label="2.2 Step 1. Update 2 Variable">2.2 Step 1. Update 2 Variable</a></li><li>
<a href=#23-step-2-clip-with-bosk-constraint aria-label="2.3 Step 2. Clip with Bosk Constraint">2.3 Step 2. Clip with Bosk Constraint</a></li><li>
<a href=#24-step-3-update-bias aria-label="2.4 Step 3. Update Bias">2.4 Step 3. Update Bias</a></li><li>
<a href=#25-pseudo-code aria-label="2.5 Pseudo Code">2.5 Pseudo Code</a></li></ul>
</li><li>
<a href=#3-fourier-kernel-approximation aria-label="3. Fourier Kernel Approximation">3. Fourier Kernel Approximation</a><ul>
<li>
<a href=#31-bochners-theorem aria-label="3.1 Bochner&amp;rsquo;s Theorem">3.1 Bochner&rsquo;s Theorem</a></li><li>
<a href=#32-theorem-1 aria-label="3.2 Theorem 1">3.2 Theorem 1</a></li></ul>
</li><li>
<a href=#4-experiments aria-label="4. Experiments">4. Experiments</a><ul>
<li>
<a href=#41-simulation-with-exact-kernel aria-label="4.1 Simulation With Exact Kernel">4.1 Simulation With Exact Kernel</a></li><li>
<a href=#42-simulation-with-approximated-kernel aria-label="4.2 Simulation With Approximated Kernel">4.2 Simulation With Approximated Kernel</a></li><li>
<a href=#43-real-dataset aria-label="4.3 Real Dataset">4.3 Real Dataset</a></li></ul>
</li></ul>
</div>
</details>
</div>
<div class=post-content>
<p><strong>Note: full code is on my <a href=https://github.com/FrankCCCCC/ml_collection/tree/master/svm>github</a>.</strong></p>
<h2 id=1-abstract>1. Abstract<a hidden class=anchor aria-hidden=true href=#1-abstract>#</a></h2>
<p>In this article, I will derive SMO algorithm and the Fourier kernel approximation which are well-known algorithm for kernel machine. <strong>SMO</strong> can solve optimization problem of SVM efficiently and the <strong>Fourier kernel approximation</strong> is a kind of kernel approximation that can speed up the computation of the kernel matrix. In the last section, I will conduct a evaluation of my manual SVM <strong>on the simulation dataset and &ldquo;Women&rsquo;s Clothing E-Commerce Review Dataset&rdquo;</strong>.</p>
<h2 id=2-sequential-minimal-optimizationsmo>2. Sequential Minimal Optimization(SMO)<a hidden class=anchor aria-hidden=true href=#2-sequential-minimal-optimizationsmo>#</a></h2>
<p>The SMO(Sequential Minimal Optimization) algorithm is proposed from the paper <strong>Sequential Minimal Optimization: A Fast Algorithm for Training Support Vector Machines</strong> in 1998 by J. Platt. In short, SMO picks 2 variables $\alpha_i, \alpha_j$ for every iteration, regulate them to satisfy KKT condition and, update them. In the following article, I will derive the whole algorithm and provide the evaluation on the simulation and real dataset.</p>
<p>We&rsquo;ve known he dual problem of soft-SVM is</p>
<p>$$
\sup_{\alpha} \sum_{i=1}^{N} \alpha_i - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_i \alpha_j y_i y_j k(x_i, x_j) \ \newline \text{subject to} \ 0 \leq \alpha_i \leq C, \sum_{i=1}^{N} \alpha_i y_i= 0
$$</p>
<p>We also define the kernel.</p>
<p>$$
k(x_i, x_j) = \langle \phi(x_i), \phi(x_j) \rangle
$$</p>
<p>where $\phi$ is an embedding function projecting the data points to a high dimensional space.</p>
<p>However, it&rsquo;s very hard to solve because we need to optimize $N$ variables. As a result, J. Platt proposed SMO to solve this problem efficiently.</p>
<h3 id=21-notation>2.1 Notation<a hidden class=anchor aria-hidden=true href=#21-notation>#</a></h3>
<p>We denote the target function as $\mathcal{L}(\alpha, C)$</p>
<p>$$
\mathcal{L} (\alpha) = \sum_{i=1}^{N} \alpha_i - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_i \alpha_j y_i y_j k(x_i, x_j)
$$</p>
<p>We also denote the kernel of $x_1, x_2$ as $K_{1, 2} = k(x_1, x_2)$.</p>
<h3 id=22-step-1-update-2-variable>2.2 Step 1. Update 2 Variable<a hidden class=anchor aria-hidden=true href=#22-step-1-update-2-variable>#</a></h3>
<p>First, we need to pick 2 variables to update in sequence, so we split the variables $\alpha_1, \alpha_2$ from the summation.</p>
<p>$$
\mathcal{L}(\alpha) =
\alpha_1 + \alpha_2 -
\frac{1}{2} \alpha_1^2 y_1^2 K_{1,1} - \frac{1}{2} \alpha_2^2 y_2^2 K_{2,2} \newline
-\frac{1}{2} \alpha_1 \alpha_2 y_1 y_2 K_{1, 2} - \frac{1}{2} \alpha_2 \alpha_1 y_2 y_1 K_{2, 1} \newline
-\frac{1}{2} \alpha_1 y_1 \sum_{i=3}^{N} \alpha_i y_i K_{i,1} -\frac{1}{2} \alpha_1 y_1 \sum_{i=3}^{N} \alpha_i y_i K_{1, i} \newline
-\frac{1}{2} \alpha_2 y_2 \sum_{i=3}^{N} \alpha_i y_i K_{i,2} -\frac{1}{2} \alpha_2 y_2 \sum_{i=3}^{N} \alpha_i y_i K_{2, i} \newline
+\sum_{i=3}^{N} \alpha_i - \frac{1}{2} \sum_{i=3}^{N} \sum_{j=3}^{N} \alpha_i \alpha_j y_i y_j k(x_i, x_j)
$$</p>
<p>$$
= \alpha_1 + \alpha_2 -
\frac{1}{2} \alpha_1^2 y_1^2 K_{1,1} - \frac{1}{2} \alpha_2^2 y_2^2 K_{2,2} - \alpha_1 \alpha_2 y_1 y_2 K_{1, 2} \newline
-\alpha_1 y_1 \sum_{i=3}^{N} \alpha_i y_i K_{i,1} - \alpha_2 y_2 \sum_{i=3}^{N} \alpha_i y_i K_{i,2} + \mathcal{Const}
$$</p>
<p>$$
= \alpha_1 + \alpha_2 -
\frac{1}{2} \alpha_1^2 K_{1,1} - \frac{1}{2} \alpha_2^2 K_{2,2} - \alpha_1 \alpha_2 y_1 y_2 K_{1, 2} \newline
-\alpha_1 y_1 \sum_{i=3}^{N} \alpha_i y_i K_{i,1} - \alpha_2 y_2 \sum_{i=3}^{N} \alpha_i y_i K_{i,2} + \mathcal{Const}
$$</p>
<p>where $\mathcal{Const} = \sum_{i=3}^{N} \alpha_i - \frac{1}{2} \sum_{i=3}^{N} \sum_{j=3}^{N} \alpha_i \alpha_j y_i y_j k(x_i, x_j)$. We see it as a constant because it is regardless to $\alpha_1, \alpha_2$.</p>
<p><strong>2.2.1 The Relation Between The Update Values and The Hyperplane</strong></p>
<p>We&rsquo;ve derive the partial derivative of the dual problem.</p>
<p>$$
\frac{\partial L(w, b, \xi, \alpha, \mu)}{\partial w} = w - \sum_{i=1}^N \alpha_i y_i x_i = 0
$$</p>
<p>We can get</p>
<p>$$
w = \sum_{i=1}^N \alpha_i y_i x_i
$$</p>
<p>Thus, we can rewrite the hyperplane $f_{\phi}(x)$ with kernel.</p>
<p>$$
f_{\phi}(x) = w^{\top} \phi(x) + b = b + \sum_{i=1}^N \alpha_i y_i k(x_i, x)
$$</p>
<p>The corresponding code:</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__f</span>(self, i):
    <span style=color:#66d9ef>return</span> snp<span style=color:#f92672>.</span>dot((self<span style=color:#f92672>.</span>alpha <span style=color:#f92672>*</span> self<span style=color:#f92672>.</span>y), self<span style=color:#f92672>.</span>K[i, :]) <span style=color:#f92672>+</span> self<span style=color:#f92672>.</span>b
</code></pre></div><p>We also denote $v_1, v_2$ as</p>
<p>$$
v_1 = \sum_{i=3}^{N} \alpha_i y_i K_{i,1} = \sum_{i=1}^{N} \alpha_i y_i k(x_i, x_1) - \alpha_1^{old} y_1 k(x_1, x_1) - \alpha_2^{old} y_2 k(x_2, x_1)
$$</p>
<p>$$
= f_{\phi}(x_1) - b - \alpha_1^{old} y_1 K_{1, 1} - \alpha_2^{old} y_2 K_{2, 1}
$$</p>
<p>and $v_2$ is similar.</p>
<p>$$
v_2 = \sum_{i=3}^{N} \alpha_i y_i K_{i,2} = \sum_{i=1}^{N} \alpha_i y_i k(x_i, x_2) - \alpha_1^{old} y_1 k(x_1, x_2) - \alpha_2^{old} y_2 k(x_2, x_2)
$$</p>
<p>$$
= f_{\phi}(x_2) - b - \alpha_1^{old} y_1 K_{1, 2} - \alpha_2^{old} y_2 K_{2, 2}
$$</p>
<p>where $\alpha_1^{old}$ and $\alpha_2^{old}$ are $\alpha_1$ and $\alpha_2$ of the previous iteration. Since we see $\alpha_i, i \geq 3$ as constant, $\alpha_i$ shouldn&rsquo;t depends on update variables $\alpha_1, \alpha_2$.</p>
<p><strong>2.2.2 Rewrite The Complementary Slackness</strong></p>
<p>The constraint can be represented as</p>
<p>$$
\sum_{i=1}^{N} \alpha_i y_i = \alpha_1 y_1 + \alpha_2 y_2 + \sum_{i=3}^{N} \alpha_i y_i = 0
$$</p>
<p>$$
\alpha_1 y_1 + \alpha_2 y_2 = - \sum_{i=3}^{N} \alpha_i y_i = \zeta
$$</p>
<p>$$
\alpha_1 = \frac{\zeta - \alpha_2 y_2}{y_1}
$$</p>
<p>Since $y_1$ is either 1 or -1, thus</p>
<p>$$
\alpha_1 = \zeta y_1 - \alpha_2 y_1 y_2
$$</p>
<p>The old ones are the same.</p>
<p>$$
\alpha_1^{old} = \zeta y_1 - \alpha_2^{old} y_1 y_2
$$</p>
<p>Replace the symbol $\alpha_1, v_1, v_2$</p>
<p>$$
\mathcal{L}(\alpha) =
(\zeta y_1 - \alpha_2 y_1 y_2) + \alpha_2
\newline
-\frac{1}{2} (\zeta y_1 - \alpha_2 y_1 y_2)^2 K_{1,1} - \frac{1}{2} \alpha_2^2 K_{2,2} - (\zeta y_1 - \alpha_2 y_1 y_2) \alpha_2 y_1 y_2 K_{1, 2}
\newline
-(\zeta y_1 - \alpha_2 y_1 y_2) y_1 v_1 - \alpha_2 y_2 v_2
$$</p>
<p>$$
= (\zeta y_1 - \alpha_2 y_1 y_2) + \alpha_2
\newline
-\frac{1}{2} (\zeta^2 + \alpha_2^2 - 2 \zeta \alpha_2 y_2) K_{1,1} - \frac{1}{2} \alpha_2^2 K_{2,2} - (\zeta \alpha_2 y_2 - \alpha_2^2) K_{1, 2}
\newline
-(\zeta - \alpha_2 y_2) v_1 - \alpha_2 y_2 v_2
$$</p>
<p><strong>2.2.3 Combine the $v_1$, $v_2$ and $\zeta$</strong></p>
<p>$$
v_1 - v_2 = [ \ f_{\phi}(x_1) - b - \alpha_1^{old} y_1 K_{1, 1} - \alpha_2^{old} y_2 K_{2, 1} \ ] - [ \ f_{\phi}(x_2) - b - \alpha_1^{old} y_1 K_{1, 2} - \alpha_2^{old} y_2 K_{2, 2} \ ]
$$</p>
<p>$$
= [ \ f_{\phi}(x_1) - b - ( \zeta y_1 - \alpha_2^{old} y_1 y_2) y_1 K_{1, 1} - \alpha_2^{old} y_2 K_{2, 1} \ ] - [ \ f_{\phi}(x_2) - b - ( \zeta y_1 - \alpha_2^{old} y_1 y_2) y_1 K_{1, 2} - \alpha_2^{old} y_2 K_{2, 2} \ ]
$$</p>
<p>$$
= [ \ f_{\phi}(x_1) - f_{\phi}(x_2) \ ] + [ \ - ( \zeta - \alpha_2^{old} y_2) K_{1, 1} - \alpha_2^{old} y_2 K_{2, 1} \ ] - [ \ - ( \zeta - \alpha_2^{old} y_2) K_{1, 2} - \alpha_2^{old} y_2 K_{2, 2} \ ]
$$</p>
<p>$$
= [ \ f_{\phi}(x_1) - f_{\phi}(x_2) \ ] + [ \ - \zeta K_{1, 1} + \alpha_2^{old} y_2 K_{1, 1} - \alpha_2^{old} y_2 K_{2, 1} \ ] - [ \ - \zeta K_{1, 2} + \alpha_2^{old} y_2 K_{1, 2} - \alpha_2^{old} y_2 K_{2, 2} \ ]
$$</p>
<p>$$
= f_{\phi}(x_1) - f_{\phi}(x_2) - \zeta K_{1, 1} + \zeta K_{1, 2} + ( K_{1, 1} + K_{2, 2} - 2 K_{1, 2} ) \alpha_2^{old} y_2
$$</p>
<p><strong>2.2.4 Derive Gradient of $\alpha_2$</strong></p>
<p>$$
\frac{\partial \mathcal{L}(\alpha)}{\partial \alpha_2}
= -y_1 y_2 + 1 - \frac{1}{2} (2 \alpha_2 - 2 \zeta y_2) K_{1,1} - \alpha_2 K_{2, 2} - (\zeta y_2 - 2 \alpha_2) K_{1, 2} - (- y_2) v_1 - y_2 v_2
$$</p>
<p>$$
= (- \alpha_2 K_{1, 1} - \alpha_2 K_{2, 2} + 2 \alpha_2 K_{1, 2}) + \zeta y_2 K_{1, 1}- \zeta y_2 K_{1, 2} - y_1 y_2 + y_2 v_1 - y_2 v_2 + 1
$$</p>
<p>$$
= -\alpha_2 (K_{1, 1} + K_{2, 2} - 2 K_{1, 2}) + \zeta y_2 K_{1, 1}- \zeta y_2 K_{1, 2} - y_1 y_2 + y_2(v_1 - v_2) + 1
$$</p>
<p>Replace $v_1 - v_2$ containing old $\alpha_1^{old}, \alpha_2^{old}$ (derived in 2.2.3)</p>
<p>$$
\frac{\partial \mathcal{L}(\alpha)}{\partial \alpha_2} = -\alpha_2 (K_{1, 1} + K_{2, 2} - 2 K_{1, 2}) + \zeta y_2 K_{1, 1}- \zeta y_2 K_{1, 2} - y_1 y_2 + y_2 [ \ f_{\phi}(x_1) - f_{\phi}(x_2) - \zeta K_{1, 1} + \zeta K_{1, 2} + ( K_{1, 1} + K_{2, 2} - 2 K_{1, 2}) \alpha_2^{old} y_2 \ ] + 1
$$</p>
<p>$$
= -(K_{1, 1} + K_{2, 2} - 2 K_{1, 2}) \alpha_2 + ( K_{1, 1} + K_{2, 2} - 2 K_{1, 2} ) \alpha_2^{old} + y_2 (f_{\phi}(x_1) - f_{\phi}(x_2) + y_2 - y_1)
$$</p>
<p>Let $\eta$ and $E_i$ be</p>
<p>$$
\eta = K_{1, 1} + K_{2, 2} - 2 K_{1, 2}, \quad E_i = f_{\phi}(x_i) - y_i
$$</p>
<p>$$
\frac{\partial \mathcal{L}_d(\alpha)}{\partial \alpha_2} = - \eta \alpha_2 + \eta \alpha_2^{old} + y_2 (E_1 - E_2)
$$</p>
<p>Since we want to minimize the gradient, let the gradient be 0.</p>
<p>$$
-\eta \alpha_2 + \eta \alpha_2^{old} + y_2 (E_1 - E_2) = 0
$$</p>
<p>Then we can find the relation between new and old $\alpha_2$ as following</p>
<p>$$
\alpha_2 = \alpha_2^{old} + \frac{y_2 (E_1 - E_2)}{\eta}
$$</p>
<p>To make the notation more clear to identify, we denote $\alpha_2^{new}$ as the new value of the update.</p>
<p>$$
\alpha_2^{new} = \alpha_2^{old} + \frac{y_2 (E_1 - E_2)}{\eta}
$$</p>
<p>The corresponding code:</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__E</span>(self, i):
    <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>__f(i) <span style=color:#f92672>-</span> self<span style=color:#f92672>.</span>y[i]

<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__eta</span>(self, i, j):
    <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>K[i, i] <span style=color:#f92672>+</span> self<span style=color:#f92672>.</span>K[j, j] <span style=color:#f92672>-</span> <span style=color:#ae81ff>2</span> <span style=color:#f92672>*</span> self<span style=color:#f92672>.</span>K[i, j]

<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__alpha_j_new</span>(self, i, j):
    E_i <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>__E(i)
    E_j <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>__E(j)
    eta <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>__eta(i, j)
    <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>alpha[j] <span style=color:#f92672>+</span> (self<span style=color:#f92672>.</span>y[j] <span style=color:#f92672>*</span> (E_i <span style=color:#f92672>-</span> E_j) <span style=color:#f92672>/</span> eta), E_i, E_j, eta
</code></pre></div><h3 id=23-step-2-clip-with-bosk-constraint>2.3 Step 2. Clip with Bosk Constraint<a hidden class=anchor aria-hidden=true href=#23-step-2-clip-with-bosk-constraint>#</a></h3>
<p>The new values should satisfy the complementary slackness as</p>
<p>$$
\alpha_1 y_1 + \alpha_2 y_2 = \zeta, \quad 0 \leq \alpha_i \leq C
$$</p>
<p>Since $y_1, y_2$ may have different labels, thus we consider 2 cases. The first case is $y_1 \neq y_2$ as the left part of the figure 1 and another case is $y_1 = y_2$ which corresponds to he right part of the figure.</p>
<p>Note that there is another line in quadrant 3 in the case 2 but it doesn&rsquo;t show in the figure due to the limit of the size.</p>
<p><img src=/blog/img/smo/bosk.png alt="bosk constraint"></p>
<p><em><strong>Figure 1</strong></em></p>
<p><strong>2.3.1 Case 1: Inequality</strong></p>
<p>When $y_1 \neq y_2$, the equation is either $\alpha_1 - \alpha_2 = k$ or $\alpha_1 - \alpha_2 = -k$ where $k = |\zeta|$ is a positive constant.</p>
<p>First, we consider the blue area $\alpha_1 - \alpha_2 = -k$. We can see $\alpha_1 \in [C, k] = [C, \alpha_2 - \alpha_1]$. The upper bound should be $C$ and the lower bound should be $\alpha_2 - \alpha_1$.</p>
<p>$$
B_U = C, \ B_L = \alpha_2 - \alpha_1
$$</p>
<p>Next, we consider the grey area $\alpha_1 - \alpha_2 = k$. We can see $\alpha_1 \in [0, C-k] = [0, C + \alpha_2 - \alpha_1]$. The upper bound should be $C + \alpha_2 - \alpha_1$ and the lower bound should be 0.</p>
<p>$$
B_U = C + \alpha_2 - \alpha_1, \ B_L = 0
$$</p>
<p>Combine 2 cases, both new and old values should satisfy the bosk constraint. The upper bound of $\alpha_2^{new}$ can be written as</p>
<p>$$
B_U = \min(C, C + \alpha_2^{old} - \alpha_1^{old})
$$</p>
<p>and the lower bound is</p>
<p>$$
B_L = \max(0, \alpha_2^{old} - \alpha_1^{old})
$$</p>
<p><strong>2.3.2 Case 2: Equality</strong></p>
<p>When $y_1 = y_2$, the equation is either $\alpha_1 + \alpha_2 = k$ or $\alpha_1 + \alpha_2 = -k$ where $k$ is a positive constant.</p>
<p>In similar way, we can derive the case of equality. The upper bound can be written as</p>
<p>$$
B_U = \min(C, \alpha_2^{old} + \alpha_1^{old})
$$</p>
<p>and the lower bound is</p>
<p>$$
B_L = \max(0, \alpha_2^{old} + \alpha_1^{old} - C)
$$</p>
<p><strong>2.3.3 Clip The Value</strong></p>
<p>According the bound we&rsquo;ve derived, we need <strong>clip</strong> the updated variable $\alpha_2^{new}$ to satisfy the constraint. In addition, we denote the new value after clipping as $\alpha_2^*$.</p>
<p>$$
\alpha_2^* = CLIP(\alpha_2^{new}, B_L, B_U)
$$</p>
<p>The corresponding code:</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__bound</span>(self, i, j):
        <span style=color:#66d9ef>if</span> self<span style=color:#f92672>.</span>y[i] <span style=color:#f92672>==</span> self<span style=color:#f92672>.</span>y[j]:
            B_U <span style=color:#f92672>=</span> min(self<span style=color:#f92672>.</span>C, self<span style=color:#f92672>.</span>alpha[j] <span style=color:#f92672>+</span> self<span style=color:#f92672>.</span>alpha[i])
            B_L <span style=color:#f92672>=</span> max(<span style=color:#ae81ff>0</span>, self<span style=color:#f92672>.</span>alpha[j] <span style=color:#f92672>+</span> self<span style=color:#f92672>.</span>alpha[i] <span style=color:#f92672>-</span> self<span style=color:#f92672>.</span>C)
        <span style=color:#66d9ef>else</span>:
            B_U <span style=color:#f92672>=</span> min(self<span style=color:#f92672>.</span>C, self<span style=color:#f92672>.</span>C <span style=color:#f92672>+</span> self<span style=color:#f92672>.</span>alpha[j] <span style=color:#f92672>-</span> self<span style=color:#f92672>.</span>alpha[i])
            B_L <span style=color:#f92672>=</span> max(<span style=color:#ae81ff>0</span>, self<span style=color:#f92672>.</span>alpha[j] <span style=color:#f92672>-</span> self<span style=color:#f92672>.</span>alpha[i])
        <span style=color:#66d9ef>return</span> B_U, B_L
<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__update_alpha_j</span>(self, i, j):
        B_U, B_L <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>__bound(i, j)
        alpha_j_star, E_i, E_j, eta <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>__alpha_j_new(i, j)
        <span style=color:#66d9ef>return</span> np<span style=color:#f92672>.</span>clip(alpha_j_star, B_L, B_U), E_i, E_j, eta
</code></pre></div><p><strong>2.3.4 Update $\alpha_1$</strong></p>
<p>We&rsquo;ve know the complementary slackness.</p>
<p>$$
\alpha_1^* y_1 + \alpha_2^* y_2 = \alpha_1^{old} y_1 + \alpha_2^{old} y_2 = \zeta
$$</p>
<p>Move the updated value $\alpha_1^*$ to the left side and we can get</p>
<p>$$
\alpha_1^* = \frac{\alpha_1^{old} y_1 + \alpha_2^{old} y_2 - \alpha_2^* y_2}{y_1}
$$</p>
<p>$$
\alpha_1^* = \alpha_1^{old} + y_1 y_2(\alpha_2^{old} - \alpha_2^*)
$$</p>
<p>The corresponding code:</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__update_alpha_i</span>(self, i, j, alpha_j_star):
    <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>alpha[i] <span style=color:#f92672>+</span> self<span style=color:#f92672>.</span>y[i] <span style=color:#f92672>*</span> self<span style=color:#f92672>.</span>y[j] <span style=color:#f92672>*</span> (self<span style=color:#f92672>.</span>alpha[j] <span style=color:#f92672>-</span> alpha_j_star)
</code></pre></div><h3 id=24-step-3-update-bias>2.4 Step 3. Update Bias<a hidden class=anchor aria-hidden=true href=#24-step-3-update-bias>#</a></h3>
<p>The only equation that contains bias $b$ is the function $f_{\phi}(x) = b + \sum_{i=1}^N \alpha_i y_i k(x_i, x)$. When $0 \lt \alpha_i^* \lt C$, it means that the data point $x_i$ is right on the margin such that $f_{\phi}(x)=y_i$, $f_{\phi}^<em>(x_i) = y_i$ and the bias $b_1^</em>, b_2^<em>$ can be derived directly. Note that for convenience, $f_{\phi}^</em>(x_w) = \sum_{i=3}^N \alpha_i y_i K_{i, w} - \alpha_1^* y_1 K_{1, w} - \alpha_2^* y_2 K_{2, w} + b^* = y_w$ contains updated variables $\alpha_2^<em>, \alpha_2^</em>, b^*$.</p>
<p>If $0 &lt; \alpha_1^* &lt; C$, the data point $x_1$ should right on the margin and $f_{\phi}^*(x_1) = y_1$. The bias derived from $\alpha_1$.</p>
<p>$$
b_1^* = y_1 - \sum_{i=3}^N \alpha_i y_i K_{i, 1} - \alpha_1^* y_1 K_{1, 1} - \alpha_2^* y_2 K_{2, 1}
$$</p>
<p>$$
= (y_1 - f_{\phi}(x_1) + \alpha_1^{old} y_1 K_{1, 1} + \alpha_2^{old} y_2 K_{2, 1} + b) - \alpha_1^* y_1 K_{1, 1} - \alpha_2^* y_2 K_{2, 1}
$$</p>
<p>$$
= -E_1 - y_1 K_{1, 1} (\alpha_1^* - \alpha_1^{old}) - y_2 K_{2, 1} (\alpha_2^* - \alpha_2^{old}) + b
$$</p>
<p>If $0 &lt; \alpha_2^* &lt; C$, the data point $x_2$ should right on the margin and $f_{\phi}^*(x_2) = y_2$. The bias derived from $\alpha_2$.</p>
<p>$$
b_2^* = y_2 - \sum_{i=3}^N \alpha_i y_i K_{i, 2} - \alpha_1^* y_1 K_{1, 2} - \alpha_2^* y_2 K_{2, 2}
$$</p>
<p>$$
= (y_2 - f_{\phi}(x_2) + \alpha_1^{old} y_1 K_{1, 2} + \alpha_2^{old} y_2 K_{2, 2} + b) - \alpha_1^* y_1 K_{1, 2} - \alpha_2^* y_2 K_{2, 2}
$$</p>
<p>$$
= -E_2 - y_1 K_{1, 2} (\alpha_1^* - \alpha_1^{old}) - y_2 K_{2, 2} (\alpha_2^* - \alpha_2^{old}) + b
$$</p>
<p>When the data point $x_i, x_j$ are both not on the margin, we choose the average of $b_1^* \ and \ b_2^*$ as the updated value.</p>
<p>$$
b^* = \frac{b_1^* + b_2^*}{2}
$$</p>
<p>The code of updating bias.</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__update_b</span>(self, i, j, alpha_i_star, alpha_j_star, E_i, E_j):
        b_star <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
        b_i_star <span style=color:#f92672>=</span> <span style=color:#f92672>-</span>E_i <span style=color:#f92672>-</span> self<span style=color:#f92672>.</span>y[i] <span style=color:#f92672>*</span> self<span style=color:#f92672>.</span>K[i, i] <span style=color:#f92672>*</span> (alpha_i_star <span style=color:#f92672>-</span> self<span style=color:#f92672>.</span>alpha[i]) <span style=color:#f92672>-</span> self<span style=color:#f92672>.</span>y[j] <span style=color:#f92672>*</span> self<span style=color:#f92672>.</span>K[j, i] <span style=color:#f92672>*</span> (alpha_j_star <span style=color:#f92672>-</span> self<span style=color:#f92672>.</span>alpha[j]) <span style=color:#f92672>+</span> self<span style=color:#f92672>.</span>b
        b_j_star <span style=color:#f92672>=</span> <span style=color:#f92672>-</span>E_j <span style=color:#f92672>-</span> self<span style=color:#f92672>.</span>y[i] <span style=color:#f92672>*</span> self<span style=color:#f92672>.</span>K[i, j] <span style=color:#f92672>*</span> (alpha_i_star <span style=color:#f92672>-</span> self<span style=color:#f92672>.</span>alpha[i]) <span style=color:#f92672>-</span> self<span style=color:#f92672>.</span>y[j] <span style=color:#f92672>*</span> self<span style=color:#f92672>.</span>K[j, j] <span style=color:#f92672>*</span> (alpha_j_star <span style=color:#f92672>-</span> self<span style=color:#f92672>.</span>alpha[j]) <span style=color:#f92672>+</span> self<span style=color:#f92672>.</span>b

        <span style=color:#66d9ef>if</span> alpha_i_star <span style=color:#f92672>&lt;=</span> self<span style=color:#f92672>.</span>C <span style=color:#f92672>and</span> alpha_i_star <span style=color:#f92672>&gt;=</span> <span style=color:#ae81ff>0</span>:
            b_star <span style=color:#f92672>=</span> b_i_star
        <span style=color:#66d9ef>elif</span> alpha_j_star <span style=color:#f92672>&lt;=</span> self<span style=color:#f92672>.</span>C <span style=color:#f92672>and</span> alpha_j_star <span style=color:#f92672>&gt;=</span> <span style=color:#ae81ff>0</span>:
            b_star <span style=color:#f92672>=</span> b_j_star
        <span style=color:#66d9ef>else</span>:
            b_star <span style=color:#f92672>=</span> (b_i_star <span style=color:#f92672>+</span> b_j_star) <span style=color:#f92672>/</span> <span style=color:#ae81ff>2</span>
        
        <span style=color:#66d9ef>return</span> b_star
</code></pre></div><p>For more detail, please see the pseudo code.</p>
<h3 id=25-pseudo-code>2.5 Pseudo Code<a hidden class=anchor aria-hidden=true href=#25-pseudo-code>#</a></h3>
<hr>
<p>Given $C$, otherwise the default value is $C = 5$</p>
<p>Given $\epsilon$, otherwise the default value is $\epsilon = 10^{-6}$</p>
<p>Given $\text{max-iter}$, otherwise the default value is $\text{max-iter} = 10^{3}$</p>
<p>For all $\alpha_i = 0, 1 \leq i \leq N$</p>
<p>$b = 0$</p>
<p>$move = \infty$</p>
<p>while($move > \epsilon$ and $iter \leq \text{max-iter}$):</p>
<ul>
<li>
<p>$\alpha_1^* = \alpha_2^* = b^* = move = 0$</p>
</li>
<li>
<p>for($n$ in $N/2$):</p>
<ul>
<li>
<p>Choose the index $i, j$ from 1 to $N$</p>
</li>
<li>
<p>$E_i = f(x_i) - y_i$</p>
</li>
<li>
<p>$E_j = f(x_j) - y_j$</p>
</li>
<li>
<p>$\eta = K_{i, i} + K_{j, j} - 2 K_{i, j}$</p>
</li>
<li>
<p>$\alpha_j^{new} = \alpha_j + \frac{y_j (E_i - E_j)}{\eta}$</p>
<p><strong>Bosk Constraint</strong></p>
</li>
<li>
<p>if($y_i = y_j$):</p>
<ul>
<li>$B_U = \min(C, \alpha_j + \alpha_i)$</li>
<li>$B_L = \max(0, \alpha_j + \alpha_i - C)$</li>
</ul>
</li>
<li>
<p>else:</p>
<ul>
<li>$B_U = \min(C, C + \alpha_j - \alpha_i)$</li>
<li>$B_L = \max(0, \alpha_j - \alpha_i)$</li>
</ul>
</li>
<li>
<p>$\alpha_j^* = CLIP(\alpha_j^{new}, B_L, B_U)$</p>
</li>
<li>
<p>$\alpha_i^* = \alpha_i + y_i y_j(\alpha_j - \alpha_j^*)$</p>
<p><strong>Update Bias</strong></p>
</li>
<li>
<p>$b_i^* = - E_i - y_i K_{i, i} (\alpha_i^* - \alpha_i) - y_j K_{j, i} (\alpha_j^* - \alpha_j) + b$</p>
</li>
<li>
<p>$b_j^* = - E_j - y_i K_{i, j} (\alpha_i^* - \alpha_i) - y_j K_{j, j} (\alpha_j^* - \alpha_j) + b$</p>
</li>
<li>
<p>if($0 \leq \alpha_i \leq C$):</p>
<ul>
<li>$b^* = b_i^*$</li>
</ul>
</li>
<li>
<p>else if($0 \leq \alpha_j \leq C$):</p>
<ul>
<li>$b^* = b_j^*$</li>
</ul>
</li>
<li>
<p>else:</p>
<ul>
<li>$b^* = \frac{b_i^* + b_j^*}{2}$</li>
</ul>
</li>
<li>
<p>$move = move + |\alpha_1^* - \alpha_1| + |\alpha_2^* - \alpha_2| + |b^* - b|$</p>
</li>
<li>
<p>Let $\alpha_i = \alpha_i^* \quad \alpha_j = \alpha_j^* \quad b = b^*$</p>
</li>
</ul>
</li>
<li>
<p>$iter = iter + 1$</p>
</li>
</ul>
<hr>
<p>Here is the Python code:</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>fit</span>(self, X, y):
        self<span style=color:#f92672>.</span>X <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array(X)
        self<span style=color:#f92672>.</span>y <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>reshape(np<span style=color:#f92672>.</span>array(y), (<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, ))
        self<span style=color:#f92672>.</span>n, self<span style=color:#f92672>.</span>dim <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>X<span style=color:#f92672>.</span>shape
        
        self<span style=color:#f92672>.</span>K <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>cal_kernel(self<span style=color:#f92672>.</span>X)

        self<span style=color:#f92672>.</span>alpha <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>zeros(self<span style=color:#f92672>.</span>n)
        self<span style=color:#f92672>.</span>b <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>

        iter <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
        loss <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>inf
        move <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>inf

        <span style=color:#66d9ef>while</span> iter <span style=color:#f92672>&lt;</span> self<span style=color:#f92672>.</span>max_iter <span style=color:#f92672>and</span> move <span style=color:#f92672>&gt;</span> self<span style=color:#f92672>.</span>epsilon:
            loss <span style=color:#f92672>=</span> move <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>

            <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(self<span style=color:#f92672>.</span>n):
                j <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>__choose_j(i)

                alpha_j_star, E_i, E_j, eta <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>__update_alpha_j(i, j)
                <span style=color:#66d9ef>if</span> eta <span style=color:#f92672>&lt;=</span> <span style=color:#ae81ff>0</span>:
                    self<span style=color:#f92672>.</span>warning(<span style=color:#e6db74>&#39;Eta &lt;= 0&#39;</span>)
                    <span style=color:#66d9ef>continue</span>

                alpha_i_star <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>__update_alpha_i(i, j, alpha_j_star)
                <span style=color:#66d9ef>if</span> abs(alpha_j_star <span style=color:#f92672>-</span> self<span style=color:#f92672>.</span>alpha[j]) <span style=color:#f92672>&lt;</span> <span style=color:#ae81ff>0.00001</span>:
                    self<span style=color:#f92672>.</span>warning(<span style=color:#e6db74>&#39;alpha_j not moving enough&#39;</span>)
                    <span style=color:#66d9ef>continue</span>

                b_star <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>__update_b(i, j, alpha_i_star, alpha_j_star, E_i, E_j)

                <span style=color:#75715e># Calculate the movement of alpha and b</span>
                move <span style=color:#f92672>=</span> move <span style=color:#f92672>+</span> abs(alpha_i_star <span style=color:#f92672>-</span> self<span style=color:#f92672>.</span>alpha[i]) <span style=color:#f92672>+</span> abs(alpha_j_star <span style=color:#f92672>-</span> self<span style=color:#f92672>.</span>alpha[j]) <span style=color:#f92672>+</span> abs(b_star <span style=color:#f92672>-</span> self<span style=color:#f92672>.</span>b)

                <span style=color:#75715e># Update variables</span>
                self<span style=color:#f92672>.</span>alpha[i] <span style=color:#f92672>=</span> alpha_i_star
                self<span style=color:#f92672>.</span>alpha[j] <span style=color:#f92672>=</span> alpha_j_star
                self<span style=color:#f92672>.</span>b <span style=color:#f92672>=</span> b_star
            
            <span style=color:#75715e># Calculate the loss</span>
            loss <span style=color:#f92672>=</span> sum(map(<span style=color:#66d9ef>lambda</span> x: abs(self<span style=color:#f92672>.</span>__E(x)), np<span style=color:#f92672>.</span>arange(self<span style=color:#f92672>.</span>n)))
            <span style=color:#75715e># Calculate the accuracy</span>
            acc <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>acc()
            self<span style=color:#f92672>.</span>loss_history<span style=color:#f92672>.</span>append(loss)
            self<span style=color:#f92672>.</span>move_history<span style=color:#f92672>.</span>append(move)
            self<span style=color:#f92672>.</span>acc_history<span style=color:#f92672>.</span>append(acc)

            <span style=color:#75715e># if not skip:</span>
            iter <span style=color:#f92672>+=</span> <span style=color:#ae81ff>1</span>
            self<span style=color:#f92672>.</span>info(<span style=color:#e6db74>&#34;Iter: &#34;</span>, iter, <span style=color:#e6db74>&#34; | Loss: &#34;</span>, loss, <span style=color:#e6db74>&#34; | Move: &#34;</span>, move, <span style=color:#e6db74>&#34; | Acc: &#34;</span>, acc)
</code></pre></div><h2 id=3-fourier-kernel-approximation>3. Fourier Kernel Approximation<a hidden class=anchor aria-hidden=true href=#3-fourier-kernel-approximation>#</a></h2>
<p>The Fourier kernel approximation is proposed from the paper <strong>Random Features for Large-Scale Kernel Machines</strong> on NIPS'07. It&rsquo;s a widely-used approximation to accelerate the kernel computing especially for the high dimensional dataset. For a dataset with dimension $D$ and data points $N$, the time complexity of computing the exact kernel is $\mathcal{O}(DN^2)$ and the Fourier kernel approximation is $\mathcal{O}(SN^3)$ with $S$ samples. While the dimension goes up, the approximation remains the same computing time because it is regardless to the dimension of the dataset.</p>
<h3 id=31-bochners-theorem>3.1 Bochner&rsquo;s Theorem<a hidden class=anchor aria-hidden=true href=#31-bochners-theorem>#</a></h3>
<p>If $\phi: \mathbb{R}^n \to \mathbb{C}$ is a positive definite, continuous, and satisfies $\phi(0)=1$, then there is some Borel probability measure $\mu \in \mathbb{R}^n$ such that $\phi = \hat{\mu}$</p>
<p>Thus, we can extend the Bochner&rsquo;s theorem to kernel.</p>
<h3 id=32-theorem-1>3.2 Theorem 1<a hidden class=anchor aria-hidden=true href=#32-theorem-1>#</a></h3>
<p>According to Bochner&rsquo;s theorem, a continuous kernel $k(x, y) = k(x-y) \in \mathbb{R}^d$ is positive definite if and only if $k(\delta)$ is the Fourier transform of a non-negative measure.</p>
<p>If a shift-invariant kernel $k(\delta)$ is a properly scaled, Bochner&rsquo;s theorem guarantees that its Fourier transform $p(\omega)$ is a proper probability distribution. Defining $\zeta_{\omega}(x) = e^{j \omega' x}$, we have</p>
<p>$$
k(x-y) = \int_{\omega} p(\omega) e^{j \omega' (x - y)} d \omega = E_{\omega}[\zeta_{\omega}(x) \zeta_{\omega}(y)]
$$</p>
<p>where $\zeta_{\omega}(x) \zeta_{\omega}(y)$ is an unbiased estimate of $k(x, y)$ when $\omega$ is drawn from $p(\omega)$.</p>
<p>With Mote-Carlo simulation, we can approximate the integral with the summation over the probability $p(\omega)$.</p>
<p>$$
z(x)' z(y) = \frac{1}{D} \sum_{j=1}^D \mathbb{z}<em>{w_j}(x) \mathbb{z}</em>{w_j}(y)
$$</p>
<p>$$
\mathbb{z}_{\omega}(x) = \sqrt{2} cos(\omega x + b) \ \text{where} \ \omega \sim p(\omega)
$$</p>
<p>In order to approximate the RBF kernel $k(k, y) = e^{-\frac{||x - y||_2^2}{2}}$, we draw $\omega$ from Fourier transformed distribution $p(\omega) = \mathcal{N}(0, 1)$.</p>
<h2 id=4-experiments>4. Experiments<a hidden class=anchor aria-hidden=true href=#4-experiments>#</a></h2>
<h3 id=41-simulation-with-exact-kernel>4.1 Simulation With Exact Kernel<a hidden class=anchor aria-hidden=true href=#41-simulation-with-exact-kernel>#</a></h3>
<p><img src=/blog/img/smo/all.png alt></p>
<p>The parameters of SVM:</p>
<ul>
<li>C: 0.6</li>
<li>$\gamma$ of RBF: 2</li>
</ul>
<p>Here we generate 3 kinds of data. The first row is generated by a Gaussian mixture model. The second row is like a moon generated by Scikit-Learn package. The third one is also generated by Scikit-Learn package and the package generate 2 circles, one is in the inner side and the other one is in the outer side.</p>
<p>The SMO and kernel seem work properly even under noise and nonlinear dataset.</p>
<h3 id=42-simulation-with-approximated-kernel>4.2 Simulation With Approximated Kernel<a hidden class=anchor aria-hidden=true href=#42-simulation-with-approximated-kernel>#</a></h3>
<p><img src=/blog/img/smo/all_approx_200.png alt></p>
<p>We draw 200 samples from $p(\omega)$ to approximate the RBF kernel. As we can see, the testing accuracies are close to the ones of exact kernels in most of cases.</p>
<h3 id=43-real-dataset>4.3 Real Dataset<a hidden class=anchor aria-hidden=true href=#43-real-dataset>#</a></h3>
<p><strong>4.3.1 PCA Preprocess</strong></p>
<p>Apply SVM on the &ldquo;Women&rsquo;s Clothing E-Commerce Review Dataset&rdquo; with C = 0.6 and $\gamma$ of RBF kernel = 2, the <strong>training accuracy is 82.03%</strong> and the <strong>testing accuracy is 81.54%</strong>. The accuracy, loss and, the movement of variables are showed in the following graph.</p>
<p><img src=/blog/img/smo/pca5_train.png alt></p>
<p>As we can see, the movement of variable gets smaller during training and converge around 50 and the accuracy remains about 82%.</p>
<p><strong>4.3.2 LDA Preprocess</strong></p>
<p><img src=/blog/img/smo/lda5_train.png alt></p>
<p>The <strong>training accuracy is also 82.03%</strong> and the <strong>testing accuracy is 81.54%</strong>, but the curves are smoother than the ones of PCA.</p>
</div>
<footer class=post-footer>
<ul class=post-tags>
<li><a href=https://frankccccc.github.io/blog/tags/svm/>SVM</a></li>
<li><a href=https://frankccccc.github.io/blog/tags/machine-learning/>machine learning</a></li>
<li><a href=https://frankccccc.github.io/blog/tags/numerical-optimization/>numerical optimization</a></li>
</ul>
<nav class=paginav>
<a class=prev href=https://frankccccc.github.io/blog/posts/from_em_to_vbem/>
<span class=title>« Prev Page</span>
<br>
<span>From EM To VBEM</span>
</a>
<a class=next href=https://frankccccc.github.io/blog/posts/toward_nngp_and_ntk/>
<span class=title>Next Page »</span>
<br>
<span>Part II - Toward NNGP and NTK</span>
</a>
</nav>
<div class=share-buttons>
<a target=_blank rel="noopener noreferrer" aria-label="share A Review of SVM and SMO on twitter" href="https://twitter.com/intent/tweet/?text=A%20Review%20of%20SVM%20and%20SMO&url=https%3a%2f%2ffrankccccc.github.io%2fblog%2fposts%2fsmo%2f&hashtags=SVM%2cmachinelearning%2cnumericaloptimization"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share A Review of SVM and SMO on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2ffrankccccc.github.io%2fblog%2fposts%2fsmo%2f&title=A%20Review%20of%20SVM%20and%20SMO&summary=A%20Review%20of%20SVM%20and%20SMO&source=https%3a%2f%2ffrankccccc.github.io%2fblog%2fposts%2fsmo%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share A Review of SVM and SMO on reddit" href="https://reddit.com/submit?url=https%3a%2f%2ffrankccccc.github.io%2fblog%2fposts%2fsmo%2f&title=A%20Review%20of%20SVM%20and%20SMO"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share A Review of SVM and SMO on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2ffrankccccc.github.io%2fblog%2fposts%2fsmo%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share A Review of SVM and SMO on whatsapp" href="https://api.whatsapp.com/send?text=A%20Review%20of%20SVM%20and%20SMO%20-%20https%3a%2f%2ffrankccccc.github.io%2fblog%2fposts%2fsmo%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share A Review of SVM and SMO on telegram" href="https://telegram.me/share/url?text=A%20Review%20of%20SVM%20and%20SMO&url=https%3a%2f%2ffrankccccc.github.io%2fblog%2fposts%2fsmo%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg>
</a>
</div>
</footer>
<div style=padding-top:2.5rem;padding-bottom:2.5rem;text-align:left>
<h1 style=padding-bottom:.5rem>COMMENTS</h1>
<h5>Your comments will encouage me to share more~~</h5>
</div>
<div id=utter-container></div>
<script src=https://utteranc.es/client.js repo=frankccccc/blog issue-term=title theme=photon-dark crossorigin=anonymous async></script>
</article>
</main><footer class=footer>
<span>&copy; 2022 <a href=https://frankccccc.github.io/blog/>Golden Hat</a></span>
<span>&#183;</span>
<span>Powered by <a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span>
<span>&#183;</span>
<span>Theme <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span>
</footer>
<a href=#top aria-label="go to top" title="Go to Top (Alt + G)">
<button class=top-link id=top-link type=button accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</button>
</a>
<script defer src=/blog/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5+kdJvBz5iKbt6B5PJI=" onload=hljs.initHighlightingOnLoad()></script>
<script>window.onload=function(){localStorage.getItem("menu-scroll-position")&&(document.getElementById('menu').scrollLeft=localStorage.getItem("menu-scroll-position"))},document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})});var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")};function menu_on_scroll(){localStorage.setItem("menu-scroll-position",document.getElementById('menu').scrollLeft)}</script>
<script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script>
</body>
</html>